{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Holocron: a Deep Learning toolbox for PyTorch","text":"<p>Holocron is meant to bridge the gap between PyTorch and latest research papers. It brings training components that are not available yet in PyTorch with a similar interface.</p> <p>This project is meant for:</p> <ul> <li> speed: architectures in this repo are picked for both pure performances and minimal latency</li> <li> research: train your models easily to SOTA standards</li> </ul>"},{"location":"#model-zoo","title":"Model zoo","text":""},{"location":"#image-classification","title":"Image classification","text":"<ul> <li>TridentNet from \"Scale-Aware Trident Networks for Object Detection\"</li> <li>SKNet from \"Selective Kernel Networks\"</li> <li>PyConvResNet from \"Pyramidal Convolution: Rethinking Convolutional Neural Networks for Visual Recognition\"</li> <li>ReXNet from \"ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network\"</li> <li>RepVGG from \"RepVGG: Making VGG-style ConvNets Great Again\"</li> </ul>"},{"location":"#semantic-segmentation","title":"Semantic segmentation","text":"<ul> <li>U-Net from \"U-Net: Convolutional Networks for Biomedical Image Segmentation\"</li> <li>U-Net++ from \"UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation\"</li> <li>UNet3+ from \"UNet 3+: A Full-Scale Connected UNet For Medical Image Segmentation\"</li> </ul>"},{"location":"#object-detection","title":"Object detection","text":"<ul> <li>YOLO from \"You Only Look Once: Unified, Real-Time Object Detection\"</li> <li>YOLOv2 from \"YOLO9000: Better, Faster, Stronger\"</li> <li>YOLOv4 from \"YOLOv4: Optimal Speed and Accuracy of Object Detection\"</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This library requires Python 3.11 or higher.</p>"},{"location":"getting-started/installation/#via-python-package","title":"Via Python Package","text":"<p>Install the last stable release of the package using uv:</p> <pre><code>uv pip install pylocron\n</code></pre>"},{"location":"getting-started/installation/#via-git","title":"Via Git","text":"<p>Install the library in developer mode:</p> <pre><code>git clone https://github.com/frgfm/Holocron.git\npip install -e Holocron/.\n</code></pre>"},{"location":"getting-started/notebooks/","title":"Holocron Notebooks","text":"<p>Here are some notebooks compiled for users to better leverage the library capabilities:</p> Notebook Description Quicktour A presentation of the main features of Holocron HuggingFace Hub integration Use HuggingFace model hub with Holocron Image classification How to train your own image classifier"},{"location":"notes/changelog/","title":"Changelog","text":""},{"location":"notes/changelog/#v021-2022-07-16","title":"v0.2.1 (2022-07-16)","text":"<p>Release note: v0.2.1</p>"},{"location":"notes/changelog/#v020-2022-02-05","title":"v0.2.0 (2022-02-05)","text":"<p>Release note: v0.2.0</p>"},{"location":"notes/changelog/#v013-2020-10-27","title":"v0.1.3 (2020-10-27)","text":"<p>Release note: v0.1.3</p>"},{"location":"notes/changelog/#v012-2020-06-21","title":"v0.1.2 (2020-06-21)","text":"<p>Release note: v0.1.2</p>"},{"location":"notes/changelog/#v011-2020-05-12","title":"v0.1.1 (2020-05-12)","text":"<p>Release note: v0.1.1</p>"},{"location":"notes/changelog/#v010-2020-05-11","title":"v0.1.0 (2020-05-11)","text":"<p>Release note: v0.1.0</p>"},{"location":"reference/nn/","title":"holocron.nn","text":"<p>An addition to the <code>torch.nn</code> module of Pytorch to extend the range of neural networks building blocks.</p>"},{"location":"reference/nn/#non-linear-activations","title":"Non-linear activations","text":""},{"location":"reference/nn/#holocron.nn.HardMish","title":"HardMish","text":"<pre><code>HardMish(inplace: bool = False)\n</code></pre> <p>               Bases: <code>_Activation</code></p> <p>Implements the Had Mish activation module from \"H-Mish\".</p> <p>This activation is computed as follows:</p> \\[ f(x) = \\frac{x}{2} \\cdot \\min(2, \\max(0, x + 2)) \\] PARAMETER DESCRIPTION <code>inplace</code> <p>should the operation be performed inplace</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>holocron/nn/modules/activation.py</code> <pre><code>def __init__(self, inplace: bool = False) -&gt; None:\n    super().__init__()\n    self.inplace: bool = inplace\n</code></pre>"},{"location":"reference/nn/#holocron.nn.NLReLU","title":"NLReLU","text":"<pre><code>NLReLU(beta: float = 1.0, inplace: bool = False)\n</code></pre> <p>               Bases: <code>_Activation</code></p> <p>Implements the Natural-Logarithm ReLU activation module from \"Natural-Logarithm-Rectified Activation Function in Convolutional Neural Networks\".</p> <p>This activation is computed as follows:</p> \\[ f(x) = ln(1 + \\beta \\cdot max(0, x)) \\] PARAMETER DESCRIPTION <code>beta</code> <p>beta used for NReLU</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>inplace</code> <p>should the operation be performed inplace</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>holocron/nn/modules/activation.py</code> <pre><code>def __init__(self, beta: float = 1.0, inplace: bool = False) -&gt; None:\n    super().__init__(inplace)\n    self.beta: float = beta\n</code></pre>"},{"location":"reference/nn/#holocron.nn.FReLU","title":"FReLU","text":"<pre><code>FReLU(in_channels: int, kernel_size: int = 3)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Implements the Funnel activation module from \"Funnel Activation for Visual Recognition\".</p> <p>This activation is computed as follows:</p> \\[ f(x) = max(\\mathbb{T}(x), x) \\] <p>where the \\(\\mathbb{T}\\) is the spatial contextual feature extraction. It is a convolution filter of size <code>kernel_size</code>, same padding and groups equal to the number of input channels, followed by a batch normalization.</p> PARAMETER DESCRIPTION <code>in_channels</code> <p>number of input channels</p> <p> TYPE: <code>int</code> </p> <code>kernel_size</code> <p>size of the convolution filter</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Source code in <code>holocron/nn/modules/activation.py</code> <pre><code>def __init__(self, in_channels: int, kernel_size: int = 3) -&gt; None:\n    super().__init__()\n    self.conv = nn.Conv2d(in_channels, in_channels, kernel_size, padding=kernel_size // 2, groups=in_channels)\n    self.bn = nn.BatchNorm2d(in_channels)\n</code></pre>"},{"location":"reference/nn/#loss-functions","title":"Loss functions","text":""},{"location":"reference/nn/#holocron.nn.Loss","title":"Loss","text":"<pre><code>Loss(weight: float | list[float] | Tensor | None = None, ignore_index: int = -100, reduction: str = 'mean')\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Base loss class.</p> PARAMETER DESCRIPTION <code>weight</code> <p>class weight for loss computation</p> <p> TYPE: <code>float | list[float] | Tensor | None</code> DEFAULT: <code>None</code> </p> <code>ignore_index</code> <p>specifies target value that is ignored and do not contribute to gradient</p> <p> TYPE: <code>int</code> DEFAULT: <code>-100</code> </p> <code>reduction</code> <p>type of reduction to apply to the final loss</p> <p> TYPE: <code>str</code> DEFAULT: <code>'mean'</code> </p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>if the reduction method is not supported</p> Source code in <code>holocron/nn/modules/loss.py</code> <pre><code>def __init__(\n    self,\n    weight: float | list[float] | Tensor | None = None,\n    ignore_index: int = -100,\n    reduction: str = \"mean\",\n) -&gt; None:\n    super().__init__()\n    # Cast class weights if possible\n    self.weight: Tensor | None\n    if isinstance(weight, (float, int)):\n        self.register_buffer(\"weight\", torch.Tensor([weight, 1 - weight]))\n    elif isinstance(weight, list):\n        self.register_buffer(\"weight\", torch.Tensor(weight))\n    elif isinstance(weight, Tensor):\n        self.register_buffer(\"weight\", weight)\n    else:\n        self.weight: Tensor | None = None\n    self.ignore_index: int = ignore_index\n    # Set the reduction method\n    if reduction not in {\"none\", \"mean\", \"sum\"}:\n        raise NotImplementedError(\"argument reduction received an incorrect input\")\n    self.reduction: str = reduction\n</code></pre>"},{"location":"reference/nn/#holocron.nn.FocalLoss","title":"FocalLoss","text":"<pre><code>FocalLoss(gamma: float = 2.0, **kwargs: Any)\n</code></pre> <p>               Bases: <code>Loss</code></p> <p>Implementation of Focal Loss as described in \"Focal Loss for Dense Object Detection\".</p> <p>While the weighted cross-entropy is described by:</p> \\[ CE(p_t) = -\\alpha_t log(p_t) \\] <p>where \\(\\alpha_t\\) is the loss weight of class \\(t\\), and \\(p_t\\) is the predicted probability of class \\(t\\).</p> <p>the focal loss introduces a modulating factor</p> \\[ FL(p_t) = -\\alpha_t (1 - p_t)^\\gamma log(p_t) \\] <p>where \\(\\gamma\\) is a positive focusing parameter.</p> PARAMETER DESCRIPTION <code>gamma</code> <p>exponent parameter of the focal loss</p> <p> TYPE: <code>float</code> DEFAULT: <code>2.0</code> </p> <code>**kwargs</code> <p>keyword args of <code>Loss</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>holocron/nn/modules/loss.py</code> <pre><code>def __init__(self, gamma: float = 2.0, **kwargs: Any) -&gt; None:\n    super().__init__(**kwargs)\n    self.gamma: float = gamma\n</code></pre>"},{"location":"reference/nn/#holocron.nn.MultiLabelCrossEntropy","title":"MultiLabelCrossEntropy","text":"<pre><code>MultiLabelCrossEntropy(*args: Any, **kwargs: Any)\n</code></pre> <p>               Bases: <code>Loss</code></p> <p>Implementation of the cross-entropy loss for multi-label targets</p> PARAMETER DESCRIPTION <code>*args</code> <p>args of <code>Loss</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>keyword args of <code>Loss</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>holocron/nn/modules/loss.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/nn/#holocron.nn.ComplementCrossEntropy","title":"ComplementCrossEntropy","text":"<pre><code>ComplementCrossEntropy(gamma: float = -1, **kwargs: Any)\n</code></pre> <p>               Bases: <code>Loss</code></p> <p>Implements the complement cross entropy loss from \"Imbalanced Image Classification with Complement Cross Entropy\"</p> PARAMETER DESCRIPTION <code>gamma</code> <p>smoothing factor</p> <p> TYPE: <code>float</code> DEFAULT: <code>-1</code> </p> <code>**kwargs</code> <p>keyword args of <code>Loss</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>holocron/nn/modules/loss.py</code> <pre><code>def __init__(self, gamma: float = -1, **kwargs: Any) -&gt; None:\n    super().__init__(**kwargs)\n    self.gamma: float = gamma\n</code></pre>"},{"location":"reference/nn/#holocron.nn.MutualChannelLoss","title":"MutualChannelLoss","text":"<pre><code>MutualChannelLoss(weight: float | list[float] | Tensor | None = None, ignore_index: int = -100, reduction: str = 'mean', xi: int = 2, alpha: float = 1)\n</code></pre> <p>               Bases: <code>Loss</code></p> <p>Implements the mutual channel loss from \"The Devil is in the Channels: Mutual-Channel Loss for Fine-Grained Image Classification\".</p> PARAMETER DESCRIPTION <code>weight</code> <p>class weight for loss computation</p> <p> TYPE: <code>float | list[float] | Tensor | None</code> DEFAULT: <code>None</code> </p> <code>ignore_index</code> <p>specifies target value that is ignored and do not contribute to gradient</p> <p> TYPE: <code>int</code> DEFAULT: <code>-100</code> </p> <code>reduction</code> <p>type of reduction to apply to the final loss</p> <p> TYPE: <code>str</code> DEFAULT: <code>'mean'</code> </p> <code>xi</code> <p>num of features per class</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>alpha</code> <p>diversity factor</p> <p> TYPE: <code>float</code> DEFAULT: <code>1</code> </p> Source code in <code>holocron/nn/modules/loss.py</code> <pre><code>def __init__(\n    self,\n    weight: float | list[float] | Tensor | None = None,\n    ignore_index: int = -100,\n    reduction: str = \"mean\",\n    xi: int = 2,\n    alpha: float = 1,\n) -&gt; None:\n    super().__init__(weight, ignore_index, reduction)\n    self.xi: int = xi\n    self.alpha: float = alpha\n</code></pre>"},{"location":"reference/nn/#holocron.nn.DiceLoss","title":"DiceLoss","text":"<pre><code>DiceLoss(weight: float | list[float] | Tensor | None = None, gamma: float = 1.0, eps: float = 1e-08)\n</code></pre> <p>               Bases: <code>Loss</code></p> <p>Implements the dice loss from \"V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation\".</p> PARAMETER DESCRIPTION <code>weight</code> <p>class weight for loss computation</p> <p> TYPE: <code>float | list[float] | Tensor | None</code> DEFAULT: <code>None</code> </p> <code>gamma</code> <p>recall/precision control param</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>eps</code> <p>small value added to avoid division by zero</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-08</code> </p> Source code in <code>holocron/nn/modules/loss.py</code> <pre><code>def __init__(\n    self,\n    weight: float | list[float] | Tensor | None = None,\n    gamma: float = 1.0,\n    eps: float = 1e-8,\n) -&gt; None:\n    super().__init__(weight)\n    self.gamma: float = gamma\n    self.eps: float = eps\n</code></pre>"},{"location":"reference/nn/#holocron.nn.PolyLoss","title":"PolyLoss","text":"<pre><code>PolyLoss(*args: Any, eps: float = 2.0, **kwargs: Any)\n</code></pre> <p>               Bases: <code>Loss</code></p> <p>Implements the Poly1 loss from \"PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions\".</p> PARAMETER DESCRIPTION <code>*args</code> <p>args of <code>Loss</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>()</code> </p> <code>eps</code> <p>epsilon 1 from the paper</p> <p> TYPE: <code>float</code> DEFAULT: <code>2.0</code> </p> <code>**kwargs</code> <p>keyword args of <code>Loss</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>holocron/nn/modules/loss.py</code> <pre><code>def __init__(\n    self,\n    *args: Any,\n    eps: float = 2.0,\n    **kwargs: Any,\n) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.eps: float = eps\n</code></pre>"},{"location":"reference/nn/#loss-wrappers","title":"Loss wrappers","text":""},{"location":"reference/nn/#holocron.nn.ClassBalancedWrapper","title":"ClassBalancedWrapper","text":"<pre><code>ClassBalancedWrapper(criterion: Module, num_samples: Tensor, beta: float = 0.99)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Implementation of the class-balanced loss as described in \"Class-Balanced Loss Based on Effective Number of Samples\".</p> <p>Given a loss function \\(\\mathcal{L}\\), the class-balanced loss is described by:</p> \\[ CB(p, y) = \\frac{1 - \\beta}{1 - \\beta^{n_y}} \\mathcal{L}(p, y) \\] <p>where \\(p\\) is the predicted probability for class \\(y\\), \\(n_y\\) is the number of training samples for class \\(y\\), and \\(\\beta\\) is exponential factor.</p> PARAMETER DESCRIPTION <code>criterion</code> <p>loss module</p> <p> TYPE: <code>Module</code> </p> <code>num_samples</code> <p>number of samples for each class</p> <p> TYPE: <code>Tensor</code> </p> <code>beta</code> <p>rebalancing exponent</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.99</code> </p> Source code in <code>holocron/nn/modules/loss.py</code> <pre><code>def __init__(self, criterion: nn.Module, num_samples: Tensor, beta: float = 0.99) -&gt; None:\n    super().__init__()\n    self.criterion = criterion\n    self.beta: float = beta\n    cb_weights = (1 - beta) / (1 - beta**num_samples)\n    if self.criterion.weight is None:\n        self.criterion.weight: Tensor | None = cb_weights\n    else:\n        self.criterion.weight *= cb_weights.to(device=self.criterion.weight.device)  # ty: ignore[invalid-argument-type,possibly-missing-attribute]\n</code></pre>"},{"location":"reference/nn/#convolution-layers","title":"Convolution layers","text":""},{"location":"reference/nn/#holocron.nn.NormConv2d","title":"NormConv2d","text":"<pre><code>NormConv2d(in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = True, padding_mode: Literal['zeros', 'reflect', 'replicate', 'circular'] = 'zeros', eps: float = 1e-14)\n</code></pre> <p>               Bases: <code>_NormConvNd</code></p> <p>Implements the normalized convolution module from \"Normalized Convolutional Neural Network\".</p> <p>In the simplest case, the output value of the layer with input size \\((N, C_{in}, H, W)\\) and output \\((N, C_{out}, H_{out}, W_{out})\\) can be precisely described as:</p> \\[ out(N_i, C_{out_j}) = bias(C_{out_j}) + \\sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \\star \\frac{input(N_i, k) - \\mu(N_i, k)}{\\sqrt{\\sigma^2(N_i, k) + \\epsilon}} \\] <p>where \\(\\star\\) is the valid 2D cross-correlation operator, \\(\\mu(N_i, k)\\) and \\(\\sigma\u00b2(N_i, k)\\) are the mean and variance of \\(input(N_i, k)\\) over all slices, \\(N\\) is a batch size, \\(C\\) denotes a number of channels, \\(H\\) is a height of input planes in pixels, and \\(W\\) is width in pixels.</p> PARAMETER DESCRIPTION <code>in_channels</code> <p>Number of channels in the input image</p> <p> TYPE: <code>int</code> </p> <code>out_channels</code> <p>Number of channels produced by the convolution</p> <p> TYPE: <code>int</code> </p> <code>kernel_size</code> <p>Size of the convolving kernel</p> <p> TYPE: <code>int</code> </p> <code>stride</code> <p>Stride of the convolution.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>padding</code> <p>Zero-padding added to both sides of the input.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>dilation</code> <p>Spacing between kernel elements.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>groups</code> <p>Number of blocked connections from input channels to output channels.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>bias</code> <p>If <code>True</code>, adds a learnable bias to the output.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>padding_mode</code> <p><code>'zeros'</code>, <code>'reflect'</code>, <code>'replicate'</code> or <code>'circular'</code>.</p> <p> TYPE: <code>Literal['zeros', 'reflect', 'replicate', 'circular']</code> DEFAULT: <code>'zeros'</code> </p> <code>eps</code> <p>a value added to the denominator for numerical stability.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-14</code> </p> Source code in <code>holocron/nn/modules/conv.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: int,\n    stride: int = 1,\n    padding: int = 0,\n    dilation: int = 1,\n    groups: int = 1,\n    bias: bool = True,\n    padding_mode: Literal[\"zeros\", \"reflect\", \"replicate\", \"circular\"] = \"zeros\",\n    eps: float = 1e-14,\n) -&gt; None:\n    kernel_size = _pair(kernel_size)\n    stride = _pair(stride)\n    padding = _pair(padding)\n    dilation = _pair(dilation)\n    super().__init__(\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        dilation,\n        False,\n        _pair(0),\n        groups,\n        bias,\n        padding_mode,\n        False,\n        eps,\n    )\n</code></pre>"},{"location":"reference/nn/#holocron.nn.Add2d","title":"Add2d","text":"<pre><code>Add2d(in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = True, padding_mode: Literal['zeros', 'reflect', 'replicate', 'circular'] = 'zeros', normalize_slices: bool = False, eps: float = 1e-14)\n</code></pre> <p>               Bases: <code>_NormConvNd</code></p> <p>Implements the adder module from \"AdderNet: Do We Really Need Multiplications in Deep Learning?\".</p> <p>In the simplest case, the output value of the layer at position \\((m, n)\\) in channel \\(c\\) with filter F of spatial size \\((d, d)\\), intput size \\((C_{in}, H, W)\\) and output \\((C_{out}, H, W)\\) can be precisely described as:</p> \\[ out(m, n, c) = - \\sum\\limits_{i=0}^d \\sum\\limits_{j=0}^d \\sum\\limits_{k=0}^{C_{in}} |X(m + i, n + j, k) - F(i, j, k, c)| \\] <p>where \\(C\\) denotes a number of channels, \\(H\\) is a height of input planes in pixels, and \\(W\\) is width in pixels.</p> <p></p> PARAMETER DESCRIPTION <code>in_channels</code> <p>Number of channels in the input image</p> <p> TYPE: <code>int</code> </p> <code>out_channels</code> <p>Number of channels produced by the convolution</p> <p> TYPE: <code>int</code> </p> <code>kernel_size</code> <p>Size of the convolving kernel</p> <p> TYPE: <code>int</code> </p> <code>stride</code> <p>Stride of the convolution.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>padding</code> <p>Zero-padding added to both sides of the input.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>dilation</code> <p>Spacing between kernel elements.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>groups</code> <p>Number of blocked connections from input channels to output channels.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>bias</code> <p>If <code>True</code>, adds a learnable bias to the output.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>padding_mode</code> <p><code>'zeros'</code>, <code>'reflect'</code>, <code>'replicate'</code> or <code>'circular'</code>.</p> <p> TYPE: <code>Literal['zeros', 'reflect', 'replicate', 'circular']</code> DEFAULT: <code>'zeros'</code> </p> <code>normalize_slices</code> <p>whether slices should be normalized before performing cross-correlation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>eps</code> <p>a value added to the denominator for numerical stability.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-14</code> </p> Source code in <code>holocron/nn/modules/conv.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: int,\n    stride: int = 1,\n    padding: int = 0,\n    dilation: int = 1,\n    groups: int = 1,\n    bias: bool = True,\n    padding_mode: Literal[\"zeros\", \"reflect\", \"replicate\", \"circular\"] = \"zeros\",\n    normalize_slices: bool = False,\n    eps: float = 1e-14,\n) -&gt; None:\n    kernel_size = _pair(kernel_size)\n    stride = _pair(stride)\n    padding = _pair(padding)\n    dilation = _pair(dilation)\n    super().__init__(\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        dilation,\n        False,\n        _pair(0),\n        groups,\n        bias,\n        padding_mode,\n        normalize_slices,\n        eps,\n    )\n</code></pre>"},{"location":"reference/nn/#holocron.nn.SlimConv2d","title":"SlimConv2d","text":"<pre><code>SlimConv2d(in_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = True, padding_mode: Literal['zeros', 'reflect', 'replicate', 'circular'] = 'zeros', r: int = 32, L: int = 2)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Implements the convolution module from \"SlimConv: Reducing Channel Redundancy in Convolutional Neural Networks by Weights Flipping\".</p> <p>First, we compute channel-wise weights as follows:</p> \\[ z(c) = \\frac{1}{H \\cdot W} \\sum\\limits_{i=1}^H \\sum\\limits_{j=1}^W X_{c,i,j} \\] <p>where \\(X \\in \\mathbb{R}^{C \\times H \\times W}\\) is the input tensor, \\(H\\) is height in pixels, and \\(W\\) is width in pixels.</p> \\[ w = \\sigma(F_{fc2}(\\delta(F_{fc1}(z)))) \\] <p>where \\(z \\in \\mathbb{R}^{C}\\) contains channel-wise statistics, \\(\\sigma\\) refers to the sigmoid function, \\(\\delta\\) refers to the ReLU function, \\(F_{fc1}\\) is a convolution operation with kernel of size \\((1, 1)\\) with \\(max(C/r, L)\\) output channels followed by batch normalization, and \\(F_{fc2}\\) is a plain convolution operation with kernel of size \\((1, 1)\\) with \\(C\\) output channels.</p> <p>We then proceed with reconstructing and transforming both pathways:</p> \\[ X_{top} = X \\odot w X_{bot} = X \\odot \\check{w} \\] <p>where \\(\\odot\\) refers to the element-wise multiplication and \\(\\check{w}\\) is the channel-wise reverse-flip of \\(w\\).</p> \\[ T_{top} = F_{top}(X_{top}^{(1)} + X_{top}^{(2)}) T_{bot} = F_{bot}(X_{bot}^{(1)} + X_{bot}^{(2)}) \\] <p>where \\(X^{(1)}\\) and \\(X^{(2)}\\) are the channel-wise first and second halves of \\(X\\), \\(F_{top}\\) is a convolution of kernel size \\((3, 3)\\), and \\(F_{bot}\\) is a convolution of kernel size \\((1, 1)\\) reducing channels by half, followed by a convolution of kernel size \\((3, 3)\\).</p> <p>Finally we fuse both pathways to yield the output:</p> \\[ Y = T_{top} \\oplus T_{bot} \\] <p>where \\(\\oplus\\) is the channel-wise concatenation.</p> <p></p> PARAMETER DESCRIPTION <code>in_channels</code> <p>Number of channels in the input image</p> <p> TYPE: <code>int</code> </p> <code>kernel_size</code> <p>Size of the convolving kernel</p> <p> TYPE: <code>int</code> </p> <code>stride</code> <p>Stride of the convolution.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>padding</code> <p>Zero-padding added to both sides of the input.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>dilation</code> <p>Spacing between kernel elements.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>groups</code> <p>Number of blocked connections from input channels to output channels.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>bias</code> <p>If <code>True</code>, adds a learnable bias to the output.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>padding_mode</code> <p><code>'zeros'</code>, <code>'reflect'</code>, <code>'replicate'</code> or <code>'circular'</code>.</p> <p> TYPE: <code>Literal['zeros', 'reflect', 'replicate', 'circular']</code> DEFAULT: <code>'zeros'</code> </p> <code>r</code> <p>squeezing divider.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>L</code> <p>minimum squeezed channels.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> Source code in <code>holocron/nn/modules/conv.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    kernel_size: int,\n    stride: int = 1,\n    padding: int = 0,\n    dilation: int = 1,\n    groups: int = 1,\n    bias: bool = True,\n    padding_mode: Literal[\"zeros\", \"reflect\", \"replicate\", \"circular\"] = \"zeros\",\n    r: int = 32,\n    L: int = 2,  # noqa: N803\n) -&gt; None:\n    super().__init__()\n    self.fc1 = nn.Conv2d(in_channels, max(in_channels // r, L), 1)\n    self.bn = nn.BatchNorm2d(max(in_channels // r, L))\n    self.fc2 = nn.Conv2d(max(in_channels // r, L), in_channels, 1)\n    self.conv_top = nn.Conv2d(\n        in_channels // 2, in_channels // 2, kernel_size, stride, padding, dilation, groups, bias, padding_mode\n    )\n    self.conv_bot1 = nn.Conv2d(in_channels // 2, in_channels // 4, 1)\n    self.conv_bot2 = nn.Conv2d(\n        in_channels // 4, in_channels // 4, kernel_size, stride, padding, dilation, groups, bias, padding_mode\n    )\n</code></pre>"},{"location":"reference/nn/#holocron.nn.PyConv2d","title":"PyConv2d","text":"<pre><code>PyConv2d(in_channels: int, out_channels: int, kernel_size: int, num_levels: int = 2, padding: int = 0, groups: list[int] | None = None, **kwargs: Any)\n</code></pre> <p>               Bases: <code>ModuleList</code></p> <p>Implements the convolution module from \"Pyramidal Convolution: Rethinking Convolutional Neural Networks for Visual Recognition\".</p> <p></p> PARAMETER DESCRIPTION <code>in_channels</code> <p>Number of channels in the input image</p> <p> TYPE: <code>int</code> </p> <code>out_channels</code> <p>Number of channels produced by the convolution</p> <p> TYPE: <code>int</code> </p> <code>kernel_size</code> <p>Size of the convolving kernel</p> <p> TYPE: <code>int</code> </p> <code>num_levels</code> <p>number of stacks in the pyramid.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>padding</code> <p>Zero-padding added to both sides of the input.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>groups</code> <p>Number of blocked connections from input channels to output channels.</p> <p> TYPE: <code>list[int] | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>keyword args of <code>torch.nn.Conv2d</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>holocron/nn/modules/conv.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: int,\n    num_levels: int = 2,\n    padding: int = 0,\n    groups: list[int] | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    if num_levels == 1:\n        super().__init__([\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size,\n                padding=padding,\n                groups=groups[0] if isinstance(groups, list) else 1,\n                **kwargs,\n            )\n        ])\n    else:\n        exp2 = int(math.log2(num_levels))\n        reminder = num_levels - 2**exp2\n        out_chans = [out_channels // 2 ** (exp2 + 1)] * (2 * reminder) + [out_channels // 2**exp2] * (\n            num_levels - 2 * reminder\n        )\n\n        k_sizes = [kernel_size + 2 * idx for idx in range(num_levels)]\n        if groups is None:\n            groups = [1] + [\n                min(2 ** (2 + idx), out_chan)\n                for idx, out_chan in zip(range(num_levels - 1), out_chans[1:], strict=True)\n            ]\n        elif not isinstance(groups, list) or len(groups) != num_levels:\n            raise ValueError(\"The argument `group` is expected to be a list of integer of size `num_levels`.\")\n        paddings = [padding + idx for idx in range(num_levels)]\n\n        super().__init__([\n            nn.Conv2d(in_channels, out_chan, k_size, padding=padding, groups=group, **kwargs)\n            for out_chan, k_size, padding, group in zip(out_chans, k_sizes, paddings, groups, strict=True)\n        ])\n    self.num_levels: int = num_levels\n</code></pre>"},{"location":"reference/nn/#holocron.nn.Involution2d","title":"Involution2d","text":"<pre><code>Involution2d(in_channels: int, kernel_size: int, padding: int = 0, stride: int = 1, groups: int = 1, dilation: int = 1, reduction_ratio: float = 1)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Implements the convolution module from \"Involution: Inverting the Inherence of Convolution for Visual Recognition\", adapted from the proposed PyTorch implementation in the paper.</p> <p></p> PARAMETER DESCRIPTION <code>in_channels</code> <p>Number of channels in the input image</p> <p> TYPE: <code>int</code> </p> <code>kernel_size</code> <p>Size of the convolving kernel</p> <p> TYPE: <code>int</code> </p> <code>padding</code> <p>Zero-padding added to both sides of the input.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>stride</code> <p>Stride of the convolution.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>groups</code> <p>Number of blocked connections from input channels to output channels.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>dilation</code> <p>Spacing between kernel elements.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>reduction_ratio</code> <p>reduction ratio of the channels to generate the kernel</p> <p> TYPE: <code>float</code> DEFAULT: <code>1</code> </p> Source code in <code>holocron/nn/modules/conv.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    kernel_size: int,\n    padding: int = 0,\n    stride: int = 1,\n    groups: int = 1,\n    dilation: int = 1,\n    reduction_ratio: float = 1,\n) -&gt; None:\n    super().__init__()\n\n    self.groups: int = groups\n    self.k_size: int = kernel_size\n\n    self.pool: nn.AvgPool2d | None = nn.AvgPool2d(stride, stride) if stride &gt; 1 else None\n    self.reduce = nn.Conv2d(in_channels, int(in_channels // reduction_ratio), 1)\n    self.span = nn.Conv2d(int(in_channels // reduction_ratio), kernel_size**2 * groups, 1)\n    self.unfold = nn.Unfold(kernel_size, dilation, padding, stride)\n</code></pre>"},{"location":"reference/nn/#regularization-layers","title":"Regularization layers","text":""},{"location":"reference/nn/#holocron.nn.DropBlock2d","title":"DropBlock2d","text":"<pre><code>DropBlock2d(p: float = 0.1, block_size: int = 7, inplace: bool = False)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Implements the DropBlock module from \"DropBlock: A regularization method for convolutional networks\"</p> <p></p> PARAMETER DESCRIPTION <code>p</code> <p>probability of dropping activation value</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>block_size</code> <p>size of each block that is expended from the sampled mask</p> <p> TYPE: <code>int</code> DEFAULT: <code>7</code> </p> <code>inplace</code> <p>whether the operation should be done inplace</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>holocron/nn/modules/dropblock.py</code> <pre><code>def __init__(self, p: float = 0.1, block_size: int = 7, inplace: bool = False) -&gt; None:\n    super().__init__()\n    self.p: float = p\n    self.block_size: int = block_size\n    self.inplace: bool = inplace\n</code></pre>"},{"location":"reference/nn/#downsampling","title":"Downsampling","text":""},{"location":"reference/nn/#holocron.nn.ConcatDownsample2d","title":"ConcatDownsample2d","text":"<pre><code>ConcatDownsample2d(scale_factor: int)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Implements a loss-less downsampling operation described in \"YOLO9000: Better, Faster, Stronger\" by stacking adjacent information on the channel dimension.</p> PARAMETER DESCRIPTION <code>scale_factor</code> <p>spatial scaling factor</p> <p> TYPE: <code>int</code> </p> Source code in <code>holocron/nn/modules/downsample.py</code> <pre><code>def __init__(self, scale_factor: int) -&gt; None:\n    super().__init__()\n    self.scale_factor: int = scale_factor\n</code></pre>"},{"location":"reference/nn/#holocron.nn.GlobalAvgPool2d","title":"GlobalAvgPool2d","text":"<pre><code>GlobalAvgPool2d(flatten: bool = False)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Fast implementation of global average pooling from \"TResNet: High Performance GPU-Dedicated Architecture\"</p> PARAMETER DESCRIPTION <code>flatten</code> <p>whether spatial dimensions should be squeezed</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>holocron/nn/modules/downsample.py</code> <pre><code>def __init__(self, flatten: bool = False) -&gt; None:\n    super().__init__()\n    self.flatten: bool = flatten\n</code></pre>"},{"location":"reference/nn/#holocron.nn.GlobalMaxPool2d","title":"GlobalMaxPool2d","text":"<pre><code>GlobalMaxPool2d(flatten: bool = False)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Fast implementation of global max pooling from \"TResNet: High Performance GPU-Dedicated Architecture\"</p> PARAMETER DESCRIPTION <code>flatten</code> <p>whether spatial dimensions should be squeezed</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>holocron/nn/modules/downsample.py</code> <pre><code>def __init__(self, flatten: bool = False) -&gt; None:\n    super().__init__()\n    self.flatten: bool = flatten\n</code></pre>"},{"location":"reference/nn/#holocron.nn.BlurPool2d","title":"BlurPool2d","text":"<pre><code>BlurPool2d(channels: int, kernel_size: int = 3, stride: int = 2)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Ross Wightman's implementation of blur pooling module as described in \"Making Convolutional Networks Shift-Invariant Again\".</p> <p></p> PARAMETER DESCRIPTION <code>channels</code> <p>Number of input channels</p> <p> TYPE: <code>int</code> </p> <code>kernel_size</code> <p>binomial filter size for blurring. currently supports 3 (default) and 5.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>stride</code> <p>downsampling filter stride</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> Source code in <code>holocron/nn/modules/downsample.py</code> <pre><code>def __init__(self, channels: int, kernel_size: int = 3, stride: int = 2) -&gt; None:\n    super().__init__()\n    self.channels: int = channels\n    if kernel_size &lt;= 1:\n        raise AssertionError\n    self.kernel_size: int = kernel_size\n    self.stride: int = stride\n    pad_size = [get_padding(kernel_size, stride, dilation=1)] * 4\n    self.padding = nn.ReflectionPad2d(pad_size)  # type: ignore[arg-type]\n    self._coeffs = torch.tensor((np.poly1d((0.5, 0.5)) ** (self.kernel_size - 1)).coeffs)  # for torchscript compat\n    self.kernel: dict[str, Tensor] = {}  # lazy init by device for DataParallel compat\n</code></pre>"},{"location":"reference/nn/#holocron.nn.SPP","title":"SPP","text":"<pre><code>SPP(kernel_sizes: list[int])\n</code></pre> <p>               Bases: <code>ModuleList</code></p> <p>SPP layer from \"Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition\".</p> PARAMETER DESCRIPTION <code>kernel_sizes</code> <p>kernel sizes of each pooling</p> <p> TYPE: <code>list[int]</code> </p> Source code in <code>holocron/nn/modules/downsample.py</code> <pre><code>def __init__(self, kernel_sizes: list[int]) -&gt; None:\n    super().__init__([nn.MaxPool2d(k_size, stride=1, padding=k_size // 2) for k_size in kernel_sizes])\n</code></pre>"},{"location":"reference/nn/#holocron.nn.ZPool","title":"ZPool","text":"<pre><code>ZPool(dim: int = 1)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Z-pool layer from \"Rotate to Attend: Convolutional Triplet Attention Module\".</p> PARAMETER DESCRIPTION <code>dim</code> <p>dimension to pool across</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Source code in <code>holocron/nn/modules/downsample.py</code> <pre><code>def __init__(self, dim: int = 1) -&gt; None:\n    super().__init__()\n    self.dim: int = dim\n</code></pre>"},{"location":"reference/nn/#attention","title":"Attention","text":""},{"location":"reference/nn/#holocron.nn.SAM","title":"SAM","text":"<pre><code>SAM(in_channels: int)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>SAM layer from \"CBAM: Convolutional Block Attention Module\" modified in \"YOLOv4: Optimal Speed and Accuracy of Object Detection\".</p> PARAMETER DESCRIPTION <code>in_channels</code> <p>input channels</p> <p> TYPE: <code>int</code> </p> Source code in <code>holocron/nn/modules/attention.py</code> <pre><code>def __init__(self, in_channels: int) -&gt; None:\n    super().__init__()\n    self.conv = nn.Conv2d(in_channels, 1, 1)\n</code></pre>"},{"location":"reference/nn/#holocron.nn.LambdaLayer","title":"LambdaLayer","text":"<pre><code>LambdaLayer(in_channels: int, out_channels: int, dim_k: int, n: int | None = None, r: int | None = None, num_heads: int = 4, dim_u: int = 1)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Lambda layer from \"LambdaNetworks: Modeling long-range interactions without attention\". The implementation was adapted from lucidrains.</p> <p></p> PARAMETER DESCRIPTION <code>in_channels</code> <p>input channels</p> <p> TYPE: <code>int</code> </p> <code>out_channels</code> <p>output channels</p> <p> TYPE: <code>int</code> </p> <code>dim_k</code> <p>key dimension</p> <p> TYPE: <code>int</code> </p> <code>n</code> <p>number of input pixels</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>r</code> <p>receptive field for relative positional encoding</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>num_heads</code> <p>number of attention heads</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>dim_u</code> <p>intra-depth dimension</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Source code in <code>holocron/nn/modules/lambda_layer.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    dim_k: int,\n    n: int | None = None,\n    r: int | None = None,\n    num_heads: int = 4,\n    dim_u: int = 1,\n) -&gt; None:\n    super().__init__()\n    self.u: int = dim_u\n    self.num_heads: int = num_heads\n\n    if out_channels % num_heads != 0:\n        raise AssertionError(\"values dimension must be divisible by number of heads for multi-head query\")\n    dim_v = out_channels // num_heads\n\n    # Project input and context to get queries, keys &amp; values\n    self.to_q = nn.Conv2d(in_channels, dim_k * num_heads, 1, bias=False)\n    self.to_k = nn.Conv2d(in_channels, dim_k * dim_u, 1, bias=False)\n    self.to_v = nn.Conv2d(in_channels, dim_v * dim_u, 1, bias=False)\n\n    self.norm_q = nn.BatchNorm2d(dim_k * num_heads)\n    self.norm_v = nn.BatchNorm2d(dim_v * dim_u)\n\n    self.local_contexts: bool = r is not None\n    if r is not None:\n        if r % 2 != 1:\n            raise AssertionError(\"Receptive kernel size should be odd\")\n        self.padding: int = r // 2\n        self.R = nn.Parameter(torch.randn(dim_k, dim_u, 1, r, r))\n    else:\n        if n is None:\n            raise AssertionError(\"You must specify the total sequence length (h x w)\")\n        self.pos_emb = nn.Parameter(torch.randn(n, n, dim_k, dim_u))\n</code></pre>"},{"location":"reference/nn/#holocron.nn.TripletAttention","title":"TripletAttention","text":"<pre><code>TripletAttention()\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Triplet attention layer from \"Rotate to Attend: Convolutional Triplet Attention Module\". This implementation is based on the one from the paper's authors.</p> Source code in <code>holocron/nn/modules/attention.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__()\n    self.c_branch = DimAttention(dim=1)\n    self.h_branch = DimAttention(dim=2)\n    self.w_branch = DimAttention(dim=3)\n</code></pre>"},{"location":"reference/ops/","title":"holocron.ops","text":"<p><code>holocron.ops</code> implements operators that are specific for Computer Vision.</p> <p>!!! note     Those operators currently do not support TorchScript.</p>"},{"location":"reference/ops/#boxes","title":"Boxes","text":""},{"location":"reference/ops/#holocron.ops.box_giou","title":"box_giou","text":"<pre><code>box_giou(boxes1: Tensor, boxes2: Tensor) -&gt; Tensor\n</code></pre> <p>Computes the Generalized-IoU as described in \"Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression\". This implementation was adapted from https://github.com/facebookresearch/detr/blob/master/util/box_ops.py</p> <p>The generalized IoU is defined as follows:</p> \\[ GIoU = IoU - \\frac{|C - A \\cup B|}{|C|} \\] <p>where \\(\\IoU\\) is the Intersection over Union, \\(A \\cup B\\) is the area of the boxes' union, and \\(C\\) is the area of the smallest enclosing box covering the two boxes.</p> PARAMETER DESCRIPTION <code>boxes1</code> <p>bounding boxes of shape [M, 4]</p> <p> TYPE: <code>Tensor</code> </p> <code>boxes2</code> <p>bounding boxes of shape [N, 4]</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Generalized-IoU of shape [M, N]</p> RAISES DESCRIPTION <code>AssertionError</code> <p>if the boxes are in incorrect coordinate format</p> Source code in <code>holocron/ops/boxes.py</code> <pre><code>def box_giou(boxes1: Tensor, boxes2: Tensor) -&gt; Tensor:\n    r\"\"\"Computes the Generalized-IoU as described in [\"Generalized Intersection over Union: A Metric and A Loss\n    for Bounding Box Regression\"](https://arxiv.org/pdf/1902.09630.pdf). This implementation was adapted\n    from https://github.com/facebookresearch/detr/blob/master/util/box_ops.py\n\n    The generalized IoU is defined as follows:\n\n    $$\n    GIoU = IoU - \\frac{|C - A \\cup B|}{|C|}\n    $$\n\n    where $\\IoU$ is the Intersection over Union,\n    $A \\cup B$ is the area of the boxes' union,\n    and $C$ is the area of the smallest enclosing box covering the two boxes.\n\n    Args:\n        boxes1: bounding boxes of shape [M, 4]\n        boxes2: bounding boxes of shape [N, 4]\n\n    Returns:\n        Generalized-IoU of shape [M, N]\n\n    Raises:\n        AssertionError: if the boxes are in incorrect coordinate format\n    \"\"\"\n    # degenerate boxes gives inf / nan results\n    # so do an early check\n    if torch.any(boxes1[:, 2:] &lt; boxes1[:, :2]) or torch.any(boxes2[:, 2:] &lt; boxes2[:, :2]):\n        raise AssertionError(\"Incorrect coordinate format\")\n    iou, union = _box_iou(boxes1, boxes2)\n\n    lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n    rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n\n    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n    area = wh[:, :, 0] * wh[:, :, 1]\n\n    return iou - (area - union) / area\n</code></pre>"},{"location":"reference/ops/#holocron.ops.diou_loss","title":"diou_loss","text":"<pre><code>diou_loss(boxes1: Tensor, boxes2: Tensor) -&gt; Tensor\n</code></pre> <p>Computes the Distance-IoU loss as described in \"Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression\".</p> <p>The loss is defined as follows:</p> \\[ \\mathcal{L}_{DIoU} = 1 - IoU + \\frac{\\rho^2(b, b^{GT})}{c^2} \\] <p>where \\(\\IoU\\) is the Intersection over Union, \\(b\\) and \\(b^{GT}\\) are the centers of the box and the ground truth box respectively, \\(c\\) c is the diagonal length of the smallest enclosing box covering the two boxes, and \\(\\rho(.)\\) is the Euclidean distance.</p> <p></p> PARAMETER DESCRIPTION <code>boxes1</code> <p>bounding boxes of shape [M, 4]</p> <p> TYPE: <code>Tensor</code> </p> <code>boxes2</code> <p>bounding boxes of shape [N, 4]</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Distance-IoU loss of shape [M, N]</p> Source code in <code>holocron/ops/boxes.py</code> <pre><code>def diou_loss(boxes1: Tensor, boxes2: Tensor) -&gt; Tensor:\n    r\"\"\"Computes the Distance-IoU loss as described in [\"Distance-IoU Loss: Faster and Better Learning for\n    Bounding Box Regression\"](https://arxiv.org/pdf/1911.08287.pdf).\n\n    The loss is defined as follows:\n\n    $$\n    \\mathcal{L}_{DIoU} = 1 - IoU + \\frac{\\rho^2(b, b^{GT})}{c^2}\n    $$\n\n    where $\\IoU$ is the Intersection over Union,\n    $b$ and $b^{GT}$ are the centers of the box and the ground truth box respectively,\n    $c$ c is the diagonal length of the smallest enclosing box covering the two boxes,\n    and $\\rho(.)$ is the Euclidean distance.\n\n    ![Distance-IoU loss](https://github.com/frgfm/Holocron/releases/download/v0.1.3/diou_loss.png)\n\n    Args:\n        boxes1: bounding boxes of shape [M, 4]\n        boxes2: bounding boxes of shape [N, 4]\n\n    Returns:\n        Distance-IoU loss of shape [M, N]\n    \"\"\"\n    return 1 - box_iou(boxes1, boxes2) + iou_penalty(boxes1, boxes2)\n</code></pre>"},{"location":"reference/ops/#holocron.ops.ciou_loss","title":"ciou_loss","text":"<pre><code>ciou_loss(boxes1: Tensor, boxes2: Tensor) -&gt; Tensor\n</code></pre> <p>Computes the Complete IoU loss as described in \"Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression\".</p> <p>The loss is defined as follows:</p> \\[ \\mathcal{L}_{CIoU} = 1 - IoU + \\frac{\\rho^2(b, b^{GT})}{c^2} + \\alpha v \\] <p>where \\(\\IoU\\) is the Intersection over Union, \\(b\\) and \\(b^{GT}\\) are the centers of the box and the ground truth box respectively, \\(c\\) c is the diagonal length of the smallest enclosing box covering the two boxes, \\(\\rho(.)\\) is the Euclidean distance, \\(\\alpha\\) is a positive trade-off parameter, and \\(v\\) is the aspect ratio consistency.</p> <p>More specifically:</p> \\[ v = \\frac{4}{\\pi^2} \\Big(\\arctan{\\frac{w^{GT}}{h^{GT}}} - \\arctan{\\frac{w}{h}}\\Big)^2 \\] <p>and</p> \\[ \\alpha = \\frac{v}{(1 - IoU) + v} \\] PARAMETER DESCRIPTION <code>boxes1</code> <p>bounding boxes of shape [M, 4]</p> <p> TYPE: <code>Tensor</code> </p> <code>boxes2</code> <p>bounding boxes of shape [N, 4]</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Complete IoU loss of shape [M, N]</p> Example <p>import torch from holocron.ops.boxes import box_ciou boxes1 = torch.tensor([[0, 0, 100, 100], [100, 100, 200, 200]], dtype=torch.float32) boxes2 = torch.tensor([[50, 50, 150, 150]], dtype=torch.float32) box_ciou(boxes1, boxes2)</p> Source code in <code>holocron/ops/boxes.py</code> <pre><code>def ciou_loss(boxes1: Tensor, boxes2: Tensor) -&gt; Tensor:\n    r\"\"\"Computes the Complete IoU loss as described in [\"Distance-IoU Loss: Faster and Better Learning for\n    Bounding Box Regression\"](https://arxiv.org/pdf/1911.08287.pdf).\n\n    The loss is defined as follows:\n\n    $$\n    \\mathcal{L}_{CIoU} = 1 - IoU + \\frac{\\rho^2(b, b^{GT})}{c^2} + \\alpha v\n    $$\n\n    where $\\IoU$ is the Intersection over Union,\n    $b$ and $b^{GT}$ are the centers of the box and the ground truth box respectively,\n    $c$ c is the diagonal length of the smallest enclosing box covering the two boxes,\n    $\\rho(.)$ is the Euclidean distance,\n    $\\alpha$ is a positive trade-off parameter,\n    and $v$ is the aspect ratio consistency.\n\n    More specifically:\n\n    $$\n    v = \\frac{4}{\\pi^2} \\Big(\\arctan{\\frac{w^{GT}}{h^{GT}}} - \\arctan{\\frac{w}{h}}\\Big)^2\n    $$\n\n    and\n\n    $$\n    \\alpha = \\frac{v}{(1 - IoU) + v}\n    $$\n\n    Args:\n        boxes1: bounding boxes of shape [M, 4]\n        boxes2: bounding boxes of shape [N, 4]\n\n    Returns:\n        Complete IoU loss of shape [M, N]\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from holocron.ops.boxes import box_ciou\n        &gt;&gt;&gt; boxes1 = torch.tensor([[0, 0, 100, 100], [100, 100, 200, 200]], dtype=torch.float32)\n        &gt;&gt;&gt; boxes2 = torch.tensor([[50, 50, 150, 150]], dtype=torch.float32)\n        &gt;&gt;&gt; box_ciou(boxes1, boxes2)\n    \"\"\"\n    iou = box_iou(boxes1, boxes2)\n    v = aspect_ratio_consistency(boxes1, boxes2)\n\n    ciou_loss = 1 - iou + iou_penalty(boxes1, boxes2)\n\n    # Check\n    filter_ = (v != 0) &amp; (iou != 0)\n    ciou_loss[filter_].addcdiv_(v[filter_], 1 - iou[filter_] + v[filter_])\n\n    return ciou_loss\n</code></pre>"},{"location":"reference/optim/","title":"holocron.optim","text":"<p>To use <code>holocron.optim</code> you have to construct an optimizer object, that will hold the current state and will update the parameters based on the computed gradients.</p>"},{"location":"reference/optim/#optimizers","title":"Optimizers","text":"<p>Implementations of recent parameter optimizer for Pytorch modules.</p>"},{"location":"reference/optim/#holocron.optim.LARS","title":"LARS","text":"<pre><code>LARS(params: Iterable[Parameter], lr: float = 0.001, momentum: float = 0.0, dampening: float = 0.0, weight_decay: float = 0.0, nesterov: bool = False, scale_clip: tuple[float, float] | None = None)\n</code></pre> <p>               Bases: <code>Optimizer</code></p> <p>Implements the LARS optimizer from \"Large batch training of convolutional networks\".</p> <p>The estimation of global and local learning rates is described as follows, \\(\\forall t \\geq 1\\):</p> \\[ \\alpha_t \\leftarrow \\alpha (1 - t / T)^2 \\\\ \\gamma_t \\leftarrow \\frac{\\lVert \\theta_t \\rVert}{\\lVert g_t \\rVert  + \\lambda \\lVert \\theta_t \\rVert} \\] <p>where \\(\\theta_t\\) is the parameter value at step \\(t\\) (\\(\\theta_0\\) being the initialization value), \\(g_t\\) is the gradient of \\(\\theta_t\\), \\(T\\) is the total number of steps, \\(\\alpha\\) is the learning rate \\(\\lambda \\geq 0\\) is the weight decay.</p> <p>Then we estimate the momentum using:</p> \\[ v_t \\leftarrow m v_{t-1} + \\alpha_t \\gamma_t (g_t + \\lambda \\theta_t) \\] <p>where \\(m\\) is the momentum and \\(v_0 = 0\\).</p> <p>And finally the update step is performed using the following rule:</p> \\[ \\theta_t \\leftarrow \\theta_{t-1} - v_t \\] PARAMETER DESCRIPTION <code>params</code> <p>iterable of parameters to optimize or dicts defining parameter groups</p> <p> TYPE: <code>Iterable[Parameter]</code> </p> <code>lr</code> <p>learning rate</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>momentum</code> <p>momentum factor</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>weight_decay</code> <p>weight decay (L2 penalty)</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>dampening</code> <p>dampening for momentum</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>nesterov</code> <p>enables Nesterov momentum</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>scale_clip</code> <p>the lower and upper bounds for the weight norm in local LR of LARS</p> <p> TYPE: <code>tuple[float, float] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>holocron/optim/lars.py</code> <pre><code>def __init__(\n    self,\n    params: Iterable[torch.nn.Parameter],\n    lr: float = 1e-3,\n    momentum: float = 0.0,\n    dampening: float = 0.0,\n    weight_decay: float = 0.0,\n    nesterov: bool = False,\n    scale_clip: tuple[float, float] | None = None,\n) -&gt; None:\n    if not isinstance(lr, float) or lr &lt; 0.0:\n        raise ValueError(f\"Invalid learning rate: {lr}\")\n    if momentum &lt; 0.0:\n        raise ValueError(f\"Invalid momentum value: {momentum}\")\n    if weight_decay &lt; 0.0:\n        raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n\n    defaults = {\n        \"lr\": lr,\n        \"momentum\": momentum,\n        \"dampening\": dampening,\n        \"weight_decay\": weight_decay,\n        \"nesterov\": nesterov,\n    }\n    if nesterov and (momentum &lt;= 0 or dampening != 0):\n        raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n    super().__init__(params, defaults)\n    # LARS arguments\n    self.scale_clip = scale_clip\n    if self.scale_clip is None:\n        self.scale_clip = (0.0, 10.0)\n</code></pre>"},{"location":"reference/optim/#holocron.optim.LARS.step","title":"step","text":"<pre><code>step(closure: Callable[[], float] | None = None) -&gt; float | None\n</code></pre> <p>Performs a single optimization step.</p> PARAMETER DESCRIPTION <code>closure</code> <p>A closure that reevaluates the model and returns the loss.</p> <p> TYPE: <code>Callable[[], float] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float | None</code> <p>loss value</p> Source code in <code>holocron/optim/lars.py</code> <pre><code>@torch.no_grad()\ndef step(self, closure: Callable[[], float] | None = None) -&gt; float | None:  # type: ignore[override]\n    \"\"\"Performs a single optimization step.\n\n    Arguments:\n        closure: A closure that reevaluates the model and returns the loss.\n\n    Returns:\n        loss value\n    \"\"\"\n    loss = None\n    if closure is not None:\n        with torch.enable_grad():\n            loss = closure()\n\n    for group in self.param_groups:\n        weight_decay = group[\"weight_decay\"]\n        momentum = group[\"momentum\"]\n        dampening = group[\"dampening\"]\n        nesterov = group[\"nesterov\"]\n\n        for p in group[\"params\"]:\n            if p.grad is None:\n                continue\n            d_p = p.grad.data\n\n            # LARS\n            p_norm = torch.norm(p.data)\n            denom = torch.norm(d_p)\n            if weight_decay != 0:\n                d_p.add_(p.data, alpha=weight_decay)\n                denom.add_(p_norm, alpha=weight_decay)\n            # Compute the local LR\n            local_lr = 1 if p_norm == 0 or denom == 0 else p_norm / denom\n\n            if momentum == 0:\n                p.data.add_(d_p, alpha=-group[\"lr\"] * local_lr)\n            else:\n                param_state = self.state[p]\n                if \"momentum_buffer\" not in param_state:\n                    momentum_buffer = param_state[\"momentum_buffer\"] = torch.clone(d_p).detach()\n                else:\n                    momentum_buffer = param_state[\"momentum_buffer\"]\n                    momentum_buffer.mul_(momentum).add_(d_p, alpha=1 - dampening)\n                d_p = d_p.add(momentum_buffer, alpha=momentum) if nesterov else momentum_buffer\n                p.data.add_(d_p, alpha=-group[\"lr\"] * local_lr)\n                self.state[p][\"momentum_buffer\"] = momentum_buffer\n\n    return loss\n</code></pre>"},{"location":"reference/optim/#holocron.optim.LAMB","title":"LAMB","text":"<pre><code>LAMB(params: Iterable[Parameter], lr: float = 0.001, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-08, weight_decay: float = 0.0, scale_clip: tuple[float, float] | None = None)\n</code></pre> <p>               Bases: <code>Optimizer</code></p> <p>Implements the Lamb optimizer from \"Large batch optimization for deep learning: training BERT in 76 minutes\".</p> <p>The estimation of momentums is described as follows, \\(\\forall t \\geq 1\\):</p> \\[ m_t \\leftarrow \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\ v_t \\leftarrow \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\] <p>where \\(g_t\\) is the gradient of \\(\\theta_t\\), \\(\\beta_1, \\beta_2 \\in [0, 1]^2\\) are the exponential average smoothing coefficients, \\(m_0 = 0,\\ v_0 = 0\\).</p> <p>Then we correct their biases using:</p> \\[ \\hat{m_t} \\leftarrow \\frac{m_t}{1 - \\beta_1^t} \\\\ \\hat{v_t} \\leftarrow \\frac{v_t}{1 - \\beta_2^t} \\] <p>And finally the update step is performed using the following rule:</p> \\[ r_t \\leftarrow \\frac{\\hat{m_t}}{\\sqrt{\\hat{v_t}} + \\epsilon} \\\\ \\theta_t \\leftarrow \\theta_{t-1} - \\alpha \\phi(\\lVert \\theta_t \\rVert) \\frac{r_t + \\lambda \\theta_t}{\\lVert r_t + \\theta_t \\rVert} \\] <p>where \\(\\theta_t\\) is the parameter value at step \\(t\\) (\\(\\theta_0\\) being the initialization value), \\(\\phi\\) is a clipping function, \\(\\alpha\\) is the learning rate, \\(\\lambda \\geq 0\\) is the weight decay, \\(\\epsilon &gt; 0\\).</p> PARAMETER DESCRIPTION <code>params</code> <p>iterable of parameters to optimize or dicts defining parameter groups</p> <p> TYPE: <code>Iterable[Parameter]</code> </p> <code>lr</code> <p>learning rate</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>betas</code> <p>beta coefficients used for running averages</p> <p> TYPE: <code>tuple[float, float]</code> DEFAULT: <code>(0.9, 0.999)</code> </p> <code>eps</code> <p>term added to the denominator to improve numerical stability</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-08</code> </p> <code>weight_decay</code> <p>weight decay (L2 penalty)</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>scale_clip</code> <p>the lower and upper bounds for the weight norm in local LR of LARS</p> <p> TYPE: <code>tuple[float, float] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>holocron/optim/lamb.py</code> <pre><code>def __init__(\n    self,\n    params: Iterable[torch.nn.Parameter],\n    lr: float = 1e-3,\n    betas: tuple[float, float] = (0.9, 0.999),\n    eps: float = 1e-8,\n    weight_decay: float = 0.0,\n    scale_clip: tuple[float, float] | None = None,\n) -&gt; None:\n    if lr &lt; 0.0:\n        raise ValueError(f\"Invalid learning rate: {lr}\")\n    if eps &lt; 0.0:\n        raise ValueError(f\"Invalid epsilon value: {eps}\")\n    if not 0.0 &lt;= betas[0] &lt; 1.0:\n        raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n    if not 0.0 &lt;= betas[1] &lt; 1.0:\n        raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n    defaults = {\"lr\": lr, \"betas\": betas, \"eps\": eps, \"weight_decay\": weight_decay}\n    super().__init__(params, defaults)\n    # LARS arguments\n    self.scale_clip = scale_clip\n    if self.scale_clip is None:\n        self.scale_clip = (0.0, 10.0)\n</code></pre>"},{"location":"reference/optim/#holocron.optim.LAMB.step","title":"step","text":"<pre><code>step(closure: Callable[[], float] | None = None) -&gt; float | None\n</code></pre> <p>Performs a single optimization step.</p> PARAMETER DESCRIPTION <code>closure</code> <p>A closure that reevaluates the model and returns the loss.</p> <p> TYPE: <code>Callable[[], float] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float | None</code> <p>loss value</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>if the optimizer does not support sparse gradients</p> Source code in <code>holocron/optim/lamb.py</code> <pre><code>@torch.no_grad()\ndef step(self, closure: Callable[[], float] | None = None) -&gt; float | None:  # type: ignore[override]\n    \"\"\"Performs a single optimization step.\n\n    Arguments:\n        closure: A closure that reevaluates the model and returns the loss.\n\n    Returns:\n        loss value\n\n    Raises:\n        RuntimeError: if the optimizer does not support sparse gradients\n    \"\"\"\n    loss = None\n    if closure is not None:\n        with torch.enable_grad():\n            loss = closure()\n\n    for group in self.param_groups:\n        for p in group[\"params\"]:\n            if p.grad is None:\n                continue\n            grad = p.grad.data\n            if grad.is_sparse:\n                raise RuntimeError(f\"{self.__class__.__name__} does not support sparse gradients\")\n\n            state = self.state[p]\n\n            # State initialization\n            if len(state) == 0:\n                state[\"step\"] = 0\n                # Exponential moving average of gradient values\n                state[\"exp_avg\"] = torch.zeros_like(p.data)\n                # Exponential moving average of squared gradient values\n                state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n\n            exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n            beta1, beta2 = group[\"betas\"]\n\n            state[\"step\"] += 1\n\n            # Decay the first and second moment running average coefficient\n            exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n            exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n\n            # Gradient term correction\n            update = torch.zeros_like(p.data)\n            denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n            update.addcdiv_(exp_avg, denom)\n\n            # Weight decay\n            if group[\"weight_decay\"] != 0:\n                update.add_(p.data, alpha=group[\"weight_decay\"])\n\n            # LARS\n            p_norm = p.data.pow(2).sum().sqrt()\n            update_norm = update.pow(2).sum().sqrt()\n            phi_p = p_norm.clamp(*self.scale_clip)\n            # Compute the local LR\n            local_lr = 1 if phi_p == 0 or update_norm == 0 else phi_p / update_norm\n\n            state[\"local_lr\"] = local_lr\n\n            p.data.add_(update, alpha=-group[\"lr\"] * local_lr)\n\n    return loss\n</code></pre>"},{"location":"reference/optim/#holocron.optim.RaLars","title":"RaLars","text":"<pre><code>RaLars(params: Iterable[Parameter], lr: float = 0.001, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-08, weight_decay: float = 0.0, force_adaptive_momentum: bool = False, scale_clip: tuple[float, float] | None = None)\n</code></pre> <p>               Bases: <code>Optimizer</code></p> <p>Implements the RAdam optimizer from \"On the variance of the Adaptive Learning Rate and Beyond\" with optional Layer-wise adaptive Scaling from \"Large Batch Training of Convolutional Networks\"</p> PARAMETER DESCRIPTION <code>params</code> <p>iterable of parameters to optimize or dicts defining parameter groups</p> <p> TYPE: <code>Iterable[Parameter]</code> </p> <code>lr</code> <p>learning rate</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>betas</code> <p>coefficients used for running averages</p> <p> TYPE: <code>tuple[float, float]</code> DEFAULT: <code>(0.9, 0.999)</code> </p> <code>eps</code> <p>term added to the denominator to improve numerical stability</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-08</code> </p> <code>weight_decay</code> <p>weight decay (L2 penalty)</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>force_adaptive_momentum</code> <p>use adaptive momentum if variance is not tractable</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>scale_clip</code> <p>the maximal upper bound for the scale factor of LARS</p> <p> TYPE: <code>tuple[float, float] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>holocron/optim/ralars.py</code> <pre><code>def __init__(\n    self,\n    params: Iterable[torch.nn.Parameter],\n    lr: float = 1e-3,\n    betas: tuple[float, float] = (0.9, 0.999),\n    eps: float = 1e-8,\n    weight_decay: float = 0.0,\n    force_adaptive_momentum: bool = False,\n    scale_clip: tuple[float, float] | None = None,\n) -&gt; None:\n    if lr &lt; 0.0:\n        raise ValueError(f\"Invalid learning rate: {lr}\")\n    if eps &lt; 0.0:\n        raise ValueError(f\"Invalid epsilon value: {eps}\")\n    if not 0.0 &lt;= betas[0] &lt; 1.0:\n        raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n    if not 0.0 &lt;= betas[1] &lt; 1.0:\n        raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n    defaults = {\"lr\": lr, \"betas\": betas, \"eps\": eps, \"weight_decay\": weight_decay}\n    super().__init__(params, defaults)\n    # RAdam tweaks\n    self.force_adaptive_momentum = force_adaptive_momentum\n    # LARS arguments\n    self.scale_clip = scale_clip\n    if self.scale_clip is None:\n        self.scale_clip = (0, 10)\n</code></pre>"},{"location":"reference/optim/#holocron.optim.RaLars.step","title":"step","text":"<pre><code>step(closure: Callable[[], float] | None = None) -&gt; float | None\n</code></pre> <p>Performs a single optimization step.</p> PARAMETER DESCRIPTION <code>closure</code> <p>A closure that reevaluates the model and returns the loss.</p> <p> TYPE: <code>Callable[[], float] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float | None</code> <p>loss value</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>if the optimizer does not support sparse gradients</p> Source code in <code>holocron/optim/ralars.py</code> <pre><code>@torch.no_grad()\ndef step(self, closure: Callable[[], float] | None = None) -&gt; float | None:  # type: ignore[override]\n    \"\"\"Performs a single optimization step.\n\n    Arguments:\n        closure: A closure that reevaluates the model and returns the loss.\n\n    Returns:\n        loss value\n\n    Raises:\n        RuntimeError: if the optimizer does not support sparse gradients\n    \"\"\"\n    loss = None\n    if closure is not None:\n        with torch.enable_grad():\n            loss = closure()\n\n    for group in self.param_groups:\n        # Get group-shared variables\n        beta1, beta2 = group[\"betas\"]\n        # Compute max length of SMA on first step\n        if not isinstance(group.get(\"sma_inf\"), float):\n            group[\"sma_inf\"] = 2 / (1 - beta2) - 1\n        sma_inf = group[\"sma_inf\"]\n\n        for p in group[\"params\"]:\n            if p.grad is None:\n                continue\n            grad = p.grad.data\n            if grad.is_sparse:\n                raise RuntimeError(f\"{self.__class__.__name__} does not support sparse gradients\")\n\n            state = self.state[p]\n\n            # State initialization\n            if len(state) == 0:\n                state[\"step\"] = 0\n                # Exponential moving average of gradient values\n                state[\"exp_avg\"] = torch.zeros_like(p.data)\n                # Exponential moving average of squared gradient values\n                state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n\n            exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n\n            state[\"step\"] += 1\n\n            # Decay the first and second moment running average coefficient\n            exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n            exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n\n            # Bias correction\n            bias_correction1 = 1 - beta1 ** state[\"step\"]\n            bias_correction2 = 1 - beta2 ** state[\"step\"]\n\n            # Compute length of SMA\n            sma_t = sma_inf - 2 * state[\"step\"] * (1 - bias_correction2) / bias_correction2\n\n            update = torch.zeros_like(p.data)\n            if sma_t &gt; 4:\n                # Variance rectification term\n                r_t = math.sqrt((sma_t - 4) * (sma_t - 2) * sma_inf / ((sma_inf - 4) * (sma_inf - 2) * sma_t))\n                # Adaptive momentum\n                update.addcdiv_(\n                    exp_avg / bias_correction1, (exp_avg_sq / bias_correction2).sqrt().add_(group[\"eps\"]), value=r_t\n                )\n            elif self.force_adaptive_momentum:\n                # Adaptive momentum without variance rectification (Adam)\n                update.addcdiv_(\n                    exp_avg / bias_correction1, (exp_avg_sq / bias_correction2).sqrt().add_(group[\"eps\"])\n                )\n            else:\n                # Unadapted momentum\n                update.add_(exp_avg / bias_correction1)\n\n            # Weight decay\n            if group[\"weight_decay\"] != 0:\n                update.add_(p.data, alpha=group[\"weight_decay\"])\n\n            # LARS\n            p_norm = p.data.pow(2).sum().sqrt()\n            update_norm = update.pow(2).sum().sqrt()\n            phi_p = p_norm.clamp(*self.scale_clip)\n            # Compute the local LR\n            local_lr = 1 if phi_p == 0 or update_norm == 0 else phi_p / update_norm\n\n            state[\"local_lr\"] = local_lr\n\n            p.data.add_(update, alpha=-group[\"lr\"] * local_lr)\n\n    return loss\n</code></pre>"},{"location":"reference/optim/#holocron.optim.TAdam","title":"TAdam","text":"<pre><code>TAdam(params: Iterable[Parameter], lr: float = 0.001, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-08, weight_decay: float = 0.0, amsgrad: bool = False, dof: float | None = None)\n</code></pre> <p>               Bases: <code>Optimizer</code></p> <p>Implements the TAdam optimizer from \"TAdam: A Robust Stochastic Gradient Optimizer\".</p> <p>The estimation of momentums is described as follows, \\(\\forall t \\geq 1\\):</p> \\[ w_t \\leftarrow (\\nu + d) \\Big(\\nu + \\sum\\limits_{j} \\frac{(g_t^j - m_{t-1}^j)^2}{v_{t-1} + \\epsilon} \\Big)^{-1} \\\\ m_t \\leftarrow \\frac{W_{t-1}}{W_{t-1} + w_t} m_{t-1} + \\frac{w_t}{W_{t-1} + w_t} g_t \\\\ v_t \\leftarrow \\beta_2 v_{t-1} + (1 - \\beta_2) (g_t - g_{t-1}) \\] <p>where \\(g_t\\) is the gradient of \\(\\theta_t\\), \\(\\beta_1, \\beta_2 \\in [0, 1]^2\\) are the exponential average smoothing coefficients, \\(m_0 = 0,\\ v_0 = 0,\\ W_0 = \\frac{\\beta_1}{1 - \\beta_1}\\); \\(\\nu\\) is the degrees of freedom and \\(d\\) if the number of dimensions of the parameter gradient.</p> <p>Then we correct their biases using:</p> \\[ \\hat{m_t} \\leftarrow \\frac{m_t}{1 - \\beta_1^t} \\\\ \\hat{v_t} \\leftarrow \\frac{v_t}{1 - \\beta_2^t} \\] <p>And finally the update step is performed using the following rule:</p> \\[ \\theta_t \\leftarrow \\theta_{t-1} - \\alpha \\frac{\\hat{m_t}}{\\sqrt{\\hat{v_t}} + \\epsilon} \\] <p>where \\(\\theta_t\\) is the parameter value at step \\(t\\) (\\(\\theta_0\\) being the initialization value), \\(\\alpha\\) is the learning rate, \\(\\epsilon &gt; 0\\).</p> PARAMETER DESCRIPTION <code>params</code> <p>iterable of parameters to optimize or dicts defining parameter groups</p> <p> TYPE: <code>Iterable[Parameter]</code> </p> <code>lr</code> <p>learning rate</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>betas</code> <p>coefficients used for running averages</p> <p> TYPE: <code>tuple[float, float]</code> DEFAULT: <code>(0.9, 0.999)</code> </p> <code>eps</code> <p>term added to the denominator to improve numerical stability</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-08</code> </p> <code>weight_decay</code> <p>weight decay (L2 penalty)</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>dof</code> <p>degrees of freedom</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> Source code in <code>holocron/optim/tadam.py</code> <pre><code>def __init__(\n    self,\n    params: Iterable[torch.nn.Parameter],\n    lr: float = 1e-3,\n    betas: tuple[float, float] = (0.9, 0.999),\n    eps: float = 1e-8,\n    weight_decay: float = 0.0,\n    amsgrad: bool = False,\n    dof: float | None = None,\n) -&gt; None:\n    if lr &lt; 0.0:\n        raise ValueError(f\"Invalid learning rate: {lr}\")\n    if eps &lt; 0.0:\n        raise ValueError(f\"Invalid epsilon value: {eps}\")\n    if not 0.0 &lt;= betas[0] &lt; 1.0:\n        raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n    if not 0.0 &lt;= betas[1] &lt; 1.0:\n        raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n    if not weight_decay &gt;= 0.0:\n        raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n    defaults = {\"lr\": lr, \"betas\": betas, \"eps\": eps, \"weight_decay\": weight_decay, \"amsgrad\": amsgrad, \"dof\": dof}\n    super().__init__(params, defaults)\n</code></pre>"},{"location":"reference/optim/#holocron.optim.TAdam.step","title":"step","text":"<pre><code>step(closure: Callable[[], float] | None = None) -&gt; float | None\n</code></pre> <p>Performs a single optimization step.</p> PARAMETER DESCRIPTION <code>closure</code> <p>A closure that reevaluates the model and returns the loss.</p> <p> TYPE: <code>Callable[[], float] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float | None</code> <p>loss value</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>if the optimizer does not support sparse gradients</p> Source code in <code>holocron/optim/tadam.py</code> <pre><code>@torch.no_grad()\ndef step(self, closure: Callable[[], float] | None = None) -&gt; float | None:  # type: ignore[override]\n    \"\"\"Performs a single optimization step.\n\n    Arguments:\n        closure: A closure that reevaluates the model and returns the loss.\n\n    Returns:\n        loss value\n\n    Raises:\n        RuntimeError: if the optimizer does not support sparse gradients\n    \"\"\"\n    loss = None\n    if closure is not None:\n        with torch.enable_grad():\n            loss = closure()\n\n    for group in self.param_groups:\n        params_with_grad = []\n        grads = []\n        exp_avgs = []\n        exp_avg_sqs = []\n        W_ts = []  # noqa: N806\n        max_exp_avg_sqs = []\n        state_steps = []\n\n        beta1, beta2 = group[\"betas\"]\n\n        for p in group[\"params\"]:\n            if p.grad is not None:\n                params_with_grad.append(p)\n                if p.grad.is_sparse:\n                    raise RuntimeError(f\"{self.__class__.__name__} does not support sparse gradients\")\n                grads.append(p.grad)\n\n                state = self.state[p]\n                # Lazy state initialization\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    # Exponential moving average of gradient values\n                    state[\"exp_avg\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    # Exponential moving average of squared gradient values\n                    state[\"exp_avg_sq\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    if group[\"amsgrad\"]:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state[\"max_exp_avg_sq\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    # Tadam specific\n                    state[\"W_t\"] = beta1 / (1 - beta1) * torch.ones(1, dtype=p.data.dtype, device=p.data.device)\n\n                exp_avgs.append(state[\"exp_avg\"])\n                exp_avg_sqs.append(state[\"exp_avg_sq\"])\n                W_ts.append(state[\"W_t\"])\n\n                if group[\"amsgrad\"]:\n                    max_exp_avg_sqs.append(state[\"max_exp_avg_sq\"])\n\n                # update the steps for each param group update\n                state[\"step\"] += 1\n                # record the step after step update\n                state_steps.append(state[\"step\"])\n\n        tadam(\n            params_with_grad,\n            grads,\n            exp_avgs,\n            exp_avg_sqs,\n            max_exp_avg_sqs,\n            W_ts,\n            state_steps,\n            group[\"amsgrad\"],\n            beta1,\n            beta2,\n            group[\"lr\"],\n            group[\"weight_decay\"],\n            group[\"eps\"],\n            group[\"dof\"],\n        )\n\n    return loss\n</code></pre>"},{"location":"reference/optim/#holocron.optim.AdaBelief","title":"AdaBelief","text":"<p>               Bases: <code>Adam</code></p> <p>Implements the AdaBelief optimizer from \"AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients\".</p> <p>The estimation of momentums is described as follows, \\(\\forall t \\geq 1\\):</p> \\[ m_t \\leftarrow \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\ s_t \\leftarrow \\beta_2 s_{t-1} + (1 - \\beta_2) (g_t - m_t)^2 + \\epsilon \\] <p>where \\(g_t\\) is the gradient of \\(\\theta_t\\), \\(\\beta_1, \\beta_2 \\in [0, 1]^2\\) are the exponential average smoothing coefficients, \\(m_0 = 0,\\ s_0 = 0\\), \\(\\epsilon &gt; 0\\).</p> <p>Then we correct their biases using:</p> \\[ \\hat{m_t} \\leftarrow \\frac{m_t}{1 - \\beta_1^t} \\\\ \\hat{s_t} \\leftarrow \\frac{s_t}{1 - \\beta_2^t} \\] <p>And finally the update step is performed using the following rule:</p> \\[ \\theta_t \\leftarrow \\theta_{t-1} - \\alpha \\frac{\\hat{m_t}}{\\sqrt{\\hat{s_t}} + \\epsilon} \\] <p>where \\(\\theta_t\\) is the parameter value at step \\(t\\) (\\(\\theta_0\\) being the initialization value), \\(\\alpha\\) is the learning rate, \\(\\epsilon &gt; 0\\).</p> PARAMETER DESCRIPTION <code>params</code> <p>iterable of parameters to optimize or dicts defining parameter groups</p> <p> </p> <code>lr</code> <p>learning rate</p> <p> </p> <code>betas</code> <p>coefficients used for running averages</p> <p> </p> <code>eps</code> <p>term added to the denominator to improve numerical stability</p> <p> </p> <code>weight_decay</code> <p>weight decay (L2 penalty)</p> <p> </p> <code>amsgrad</code> <p>whether to use the AMSGrad variant</p> <p> </p>"},{"location":"reference/optim/#holocron.optim.AdaBelief.step","title":"step","text":"<pre><code>step(closure: Callable[[], float] | None = None) -&gt; float | None\n</code></pre> <p>Performs a single optimization step.</p> PARAMETER DESCRIPTION <code>closure</code> <p>A closure that reevaluates the model and returns the loss.</p> <p> TYPE: <code>Callable[[], float] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float | None</code> <p>loss value</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>if the optimizer does not support sparse gradients</p> Source code in <code>holocron/optim/adabelief.py</code> <pre><code>@torch.no_grad()\ndef step(self, closure: Callable[[], float] | None = None) -&gt; float | None:  # type: ignore[override]\n    \"\"\"Performs a single optimization step.\n\n    Arguments:\n        closure: A closure that reevaluates the model\n            and returns the loss.\n\n    Returns:\n        loss value\n\n    Raises:\n        RuntimeError: if the optimizer does not support sparse gradients\n    \"\"\"\n    loss = None\n    if closure is not None:\n        with torch.enable_grad():\n            loss = closure()\n\n    for group in self.param_groups:\n        params_with_grad = []\n        grads = []\n        exp_avgs = []\n        exp_avg_sqs = []\n        max_exp_avg_sqs = []\n        state_steps = []\n\n        for p in group[\"params\"]:\n            if p.grad is not None:\n                params_with_grad.append(p)\n                if p.grad.is_sparse:\n                    raise RuntimeError(f\"{self.__class__.__name__} does not support sparse gradients\")\n                grads.append(p.grad)\n\n                state = self.state[p]\n                # Lazy state initialization\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    # Exponential moving average of gradient values\n                    state[\"exp_avg\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    # Exponential moving average of squared gradient values\n                    state[\"exp_avg_sq\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    if group[\"amsgrad\"]:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state[\"max_exp_avg_sq\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n\n                exp_avgs.append(state[\"exp_avg\"])\n                exp_avg_sqs.append(state[\"exp_avg_sq\"])\n\n                if group[\"amsgrad\"]:\n                    max_exp_avg_sqs.append(state[\"max_exp_avg_sq\"])\n\n                # update the steps for each param group update\n                state[\"step\"] += 1\n                # record the step after step update\n                state_steps.append(state[\"step\"])\n\n        beta1, beta2 = group[\"betas\"]\n        adabelief(\n            params_with_grad,\n            grads,\n            exp_avgs,\n            exp_avg_sqs,\n            max_exp_avg_sqs,\n            state_steps,\n            group[\"amsgrad\"],\n            beta1,\n            beta2,\n            group[\"lr\"],\n            group[\"weight_decay\"],\n            group[\"eps\"],\n        )\n    return loss\n</code></pre>"},{"location":"reference/optim/#holocron.optim.AdamP","title":"AdamP","text":"<pre><code>AdamP(params: Iterable[Parameter], lr: float = 0.001, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-08, weight_decay: float = 0.0, amsgrad: bool = False, delta: float = 0.1)\n</code></pre> <p>               Bases: <code>Adam</code></p> <p>Implements the AdamP optimizer from \"AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights\".</p> <p>The estimation of momentums is described as follows, \\(\\forall t \\geq 1\\):</p> \\[ m_t \\leftarrow \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\ v_t \\leftarrow \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\] <p>where \\(g_t\\) is the gradient of \\(\\theta_t\\), \\(\\beta_1, \\beta_2 \\in [0, 1]^2\\) are the exponential average smoothing coefficients, \\(m_0 = g_0,\\ v_0 = 0\\).</p> <p>Then we correct their biases using:</p> \\[ \\hat{m_t} \\leftarrow \\frac{m_t}{1 - \\beta_1^t} \\\\ \\hat{v_t} \\leftarrow \\frac{v_t}{1 - \\beta_2^t} \\] <p>And finally the update step is performed using the following rule:</p> \\[ p_t \\leftarrow \\frac{\\hat{m_t}}{\\sqrt{\\hat{n_t} + \\epsilon}} \\\\ q_t \\leftarrow \\begin{cases}   \\prod_{\\theta_t}(p_t) &amp; if\\ cos(\\theta_t, g_t) &lt; \\delta / \\sqrt{dim(\\theta)}\\\\   p_t &amp; \\text{otherwise}\\\\ \\end{cases} \\\\ \\theta_t \\leftarrow \\theta_{t-1} - \\alpha q_t \\] <p>where \\(\\theta_t\\) is the parameter value at step \\(t\\) (\\(\\theta_0\\) being the initialization value), \\(\\prod_{\\theta_t}(p_t)\\) is the projection of \\(p_t\\) onto the tangent space of \\(\\theta_t\\), \\(cos(\\theta_t, g_t)\\) is the cosine similarity between \\(\\theta_t\\) and \\(g_t\\), \\(\\alpha\\) is the learning rate, \\(\\delta &gt; 0\\), \\(\\epsilon &gt; 0\\).</p> PARAMETER DESCRIPTION <code>params</code> <p>iterable of parameters to optimize or dicts defining parameter groups</p> <p> TYPE: <code>Iterable[Parameter]</code> </p> <code>lr</code> <p>learning rate</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>betas</code> <p>coefficients used for running averages</p> <p> TYPE: <code>tuple[float, float]</code> DEFAULT: <code>(0.9, 0.999)</code> </p> <code>eps</code> <p>term added to the denominator to improve numerical stability</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-08</code> </p> <code>weight_decay</code> <p>weight decay (L2 penalty)</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>amsgrad</code> <p>whether to use the AMSGrad variant</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>delta</code> <p>delta threshold for projection</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> Source code in <code>holocron/optim/adamp.py</code> <pre><code>def __init__(\n    self,\n    params: Iterable[torch.nn.Parameter],\n    lr: float = 1e-3,\n    betas: tuple[float, float] = (0.9, 0.999),\n    eps: float = 1e-8,\n    weight_decay: float = 0.0,\n    amsgrad: bool = False,\n    delta: float = 0.1,\n) -&gt; None:\n    super().__init__(params, lr, betas, eps, weight_decay, amsgrad)\n    self.delta = delta\n</code></pre>"},{"location":"reference/optim/#holocron.optim.AdamP.step","title":"step","text":"<pre><code>step(closure: Callable[[], float] | None = None) -&gt; float | None\n</code></pre> <p>Performs a single optimization step.</p> PARAMETER DESCRIPTION <code>closure</code> <p>A closure that reevaluates the model and returns the loss.</p> <p> TYPE: <code>Callable[[], float] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float | None</code> <p>loss value</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>if the optimizer does not support sparse gradients</p> Source code in <code>holocron/optim/adamp.py</code> <pre><code>@torch.no_grad()\ndef step(self, closure: Callable[[], float] | None = None) -&gt; float | None:  # type: ignore[override]\n    \"\"\"Performs a single optimization step.\n\n    Arguments:\n        closure: A closure that reevaluates the model and returns the loss.\n\n    Returns:\n        loss value\n\n    Raises:\n        RuntimeError: if the optimizer does not support sparse gradients\n    \"\"\"\n    loss = None\n    if closure is not None:\n        with torch.enable_grad():\n            loss = closure()\n\n    for group in self.param_groups:\n        params_with_grad = []\n        grads = []\n        exp_avgs = []\n        exp_avg_sqs = []\n        max_exp_avg_sqs = []\n        state_steps = []\n\n        for p in group[\"params\"]:\n            if p.grad is not None:\n                params_with_grad.append(p)\n                if p.grad.is_sparse:\n                    raise RuntimeError(f\"{self.__class__.__name__} does not support sparse gradients\")\n                grads.append(p.grad)\n\n                state = self.state[p]\n                # Lazy state initialization\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    # Exponential moving average of gradient values\n                    state[\"exp_avg\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    # Exponential moving average of squared gradient values\n                    state[\"exp_avg_sq\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    if group[\"amsgrad\"]:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state[\"max_exp_avg_sq\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n\n                exp_avgs.append(state[\"exp_avg\"])\n                exp_avg_sqs.append(state[\"exp_avg_sq\"])\n                if group[\"amsgrad\"]:\n                    max_exp_avg_sqs.append(state[\"max_exp_avg_sq\"])\n\n                # update the steps for each param group update\n                state[\"step\"] += 1\n                # record the step after step update\n                state_steps.append(state[\"step\"])\n\n        beta1, beta2 = group[\"betas\"]\n        adamp(\n            params_with_grad,\n            grads,\n            exp_avgs,\n            exp_avg_sqs,\n            max_exp_avg_sqs,\n            state_steps,\n            group[\"amsgrad\"],\n            beta1,\n            beta2,\n            group[\"lr\"],\n            group[\"weight_decay\"],\n            group[\"eps\"],\n            self.delta,\n        )\n\n    return loss\n</code></pre>"},{"location":"reference/optim/#holocron.optim.Adan","title":"Adan","text":"<pre><code>Adan(params: Iterable[Parameter], lr: float = 0.001, betas: tuple[float, float, float] = (0.98, 0.92, 0.99), eps: float = 1e-08, weight_decay: float = 0.0, amsgrad: bool = False)\n</code></pre> <p>               Bases: <code>Adam</code></p> <p>Implements the Adan optimizer from \"Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models\".</p> <p>The estimation of momentums is described as follows, \\(\\forall t \\geq 1\\):</p> \\[ m_t \\leftarrow \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\ v_t \\leftarrow \\beta_2 v_{t-1} + (1 - \\beta_2) (g_t - g_{t-1}) \\\\ n_t \\leftarrow \\beta_3 n_{t-1} + (1 - \\beta_3) [g_t + \\beta_2 (g_t - g_{t - 1})]^2 \\] <p>where \\(g_t\\) is the gradient of \\(\\theta_t\\), \\(\\beta_1, \\beta_2, \\beta_3 \\in [0, 1]^3\\) are the exponential average smoothing coefficients, \\(m_0 = g_0,\\ v_0 = 0,\\ n_0 = g_0^2\\).</p> <p>Then we correct their biases using:</p> \\[ \\hat{m_t} \\leftarrow \\frac{m_t}{1 - \\beta_1^t} \\\\ \\hat{v_t} \\leftarrow \\frac{v_t}{1 - \\beta_2^t} \\\\ \\hat{n_t} \\leftarrow \\frac{n_t}{1 - \\beta_3^t} \\] <p>And finally the update step is performed using the following rule:</p> \\[ p_t \\leftarrow \\frac{\\hat{m_t} + (1 - \\beta_2) \\hat{v_t}}{\\sqrt{\\hat{n_t} + \\epsilon}} \\\\ \\theta_t \\leftarrow \\frac{\\theta_{t-1} - \\alpha p_t}{1 + \\lambda \\alpha} \\] <p>where \\(\\theta_t\\) is the parameter value at step \\(t\\) (\\(\\theta_0\\) being the initialization value), \\(\\alpha\\) is the learning rate, \\(\\lambda \\geq 0\\) is the weight decay, \\(\\epsilon &gt; 0\\).</p> PARAMETER DESCRIPTION <code>params</code> <p>iterable of parameters to optimize or dicts defining parameter groups</p> <p> TYPE: <code>Iterable[Parameter]</code> </p> <code>lr</code> <p>learning rate</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>betas</code> <p>coefficients used for running averages</p> <p> TYPE: <code>tuple[float, float, float]</code> DEFAULT: <code>(0.98, 0.92, 0.99)</code> </p> <code>eps</code> <p>term added to the denominator to improve numerical stability</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-08</code> </p> <code>weight_decay</code> <p>weight decay (L2 penalty)</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>amsgrad</code> <p>whether to use the AMSGrad variant</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>holocron/optim/adan.py</code> <pre><code>def __init__(\n    self,\n    params: Iterable[torch.nn.Parameter],\n    lr: float = 1e-3,\n    betas: tuple[float, float, float] = (0.98, 0.92, 0.99),\n    eps: float = 1e-8,\n    weight_decay: float = 0.0,\n    amsgrad: bool = False,\n) -&gt; None:\n    super().__init__(params, lr, betas, eps, weight_decay, amsgrad)  # type: ignore[arg-type]\n</code></pre>"},{"location":"reference/optim/#holocron.optim.Adan.step","title":"step","text":"<pre><code>step(closure: Callable[[], float] | None = None) -&gt; float | None\n</code></pre> <p>Performs a single optimization step.</p> PARAMETER DESCRIPTION <code>closure</code> <p>A closure that reevaluates the model and returns the loss.</p> <p> TYPE: <code>Callable[[], float] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float | None</code> <p>loss value</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>if the optimizer does not support sparse gradients</p> Source code in <code>holocron/optim/adan.py</code> <pre><code>@torch.no_grad()\ndef step(self, closure: Callable[[], float] | None = None) -&gt; float | None:  # type: ignore[override]\n    \"\"\"Performs a single optimization step.\n\n    Arguments:\n        closure: A closure that reevaluates the model and returns the loss.\n\n    Returns:\n        loss value\n\n    Raises:\n        RuntimeError: if the optimizer does not support sparse gradients\n    \"\"\"\n    loss = None\n    if closure is not None:\n        with torch.enable_grad():\n            loss = closure()\n\n    for group in self.param_groups:\n        params_with_grad = []\n        grads = []\n        prev_grads = []\n        exp_avgs = []\n        exp_avg_sqs = []\n        exp_avg_deltas = []\n        max_exp_avg_deltas = []\n        state_steps = []\n\n        for p in group[\"params\"]:\n            if p.grad is not None:\n                params_with_grad.append(p)\n                if p.grad.is_sparse:\n                    raise RuntimeError(f\"{self.__class__.__name__} does not support sparse gradients\")\n                grads.append(p.grad)\n\n                state = self.state[p]\n                # Lazy state initialization\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    # Exponential moving average of gradient values\n                    state[\"exp_avg\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    # Exponential moving average of squared gradient values\n                    state[\"exp_avg_sq\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    # Exponential moving average of gradient delta values\n                    state[\"exp_avg_delta\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    if group[\"amsgrad\"]:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state[\"max_exp_avg_delta\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    state[\"prev_grad\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n\n                prev_grads.append(state[\"prev_grad\"])\n                exp_avgs.append(state[\"exp_avg\"])\n                exp_avg_sqs.append(state[\"exp_avg_sq\"])\n                exp_avg_deltas.append(state[\"exp_avg_delta\"])\n                if group[\"amsgrad\"]:\n                    max_exp_avg_deltas.append(state[\"max_exp_avg_delta\"])\n\n                # update the steps for each param group update\n                state[\"step\"] += 1\n                # record the step after step update\n                state_steps.append(state[\"step\"])\n\n        beta1, beta2, beta3 = group[\"betas\"]\n        adan(\n            params_with_grad,\n            grads,\n            prev_grads,\n            exp_avgs,\n            exp_avg_sqs,\n            exp_avg_deltas,\n            max_exp_avg_deltas,\n            state_steps,\n            group[\"amsgrad\"],\n            beta1,\n            beta2,\n            beta3,\n            group[\"lr\"],\n            group[\"weight_decay\"],\n            group[\"eps\"],\n        )\n\n    return loss\n</code></pre>"},{"location":"reference/optim/#holocron.optim.AdEMAMix","title":"AdEMAMix","text":"<pre><code>AdEMAMix(params: Iterable[Parameter], lr: float = 0.001, betas: tuple[float, float, float] = (0.9, 0.999, 0.9999), alpha: float = 5.0, eps: float = 1e-08, weight_decay: float = 0.0)\n</code></pre> <p>               Bases: <code>Optimizer</code></p> <p>Implements the AdEMAMix optimizer from \"The AdEMAMix Optimizer: Better, Faster, Older\".</p> <p>The estimation of momentums is described as follows, \\(\\forall t \\geq 1\\):</p> \\[ m_{1,t} \\leftarrow \\beta_1 m_{1, t-1} + (1 - \\beta_1) g_t \\\\ m_{2,t} \\leftarrow \\beta_3 m_{2, t-1} + (1 - \\beta_3) g_t \\\\ s_t \\leftarrow \\beta_2 s_{t-1} + (1 - \\beta_2) (g_t - m_t)^2 + \\epsilon \\] <p>where \\(g_t\\) is the gradient of \\(\\theta_t\\), \\(\\beta_1, \\beta_2, \\beta_3 \\in [0, 1]^3\\) are the exponential average smoothing coefficients, \\(m_{1,0} = 0,\\ m_{2,0} = 0,\\ s_0 = 0\\), \\(\\epsilon &gt; 0\\).</p> <p>Then we correct their biases using:</p> \\[ \\hat{m_{1,t}} \\leftarrow \\frac{m_{1,t}}{1 - \\beta_1^t} \\\\ \\hat{s_t} \\leftarrow \\frac{s_t}{1 - \\beta_2^t} \\] <p>And finally the update step is performed using the following rule:</p> \\[ \\theta_t \\leftarrow \\theta_{t-1} - \\eta \\frac{\\hat{m_{1,t}} + \\alpha m_{2,t}}{\\sqrt{\\hat{s_t}} + \\epsilon} \\] <p>where \\(\\theta_t\\) is the parameter value at step \\(t\\) (\\(\\theta_0\\) being the initialization value), \\(\\eta\\) is the learning rate, \\(\\alpha &gt; 0\\) \\(\\epsilon &gt; 0\\).</p> PARAMETER DESCRIPTION <code>params</code> <p>iterable of parameters to optimize or dicts defining parameter groups</p> <p> TYPE: <code>Iterable[Parameter]</code> </p> <code>lr</code> <p>learning rate</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>betas</code> <p>coefficients used for running averages</p> <p> TYPE: <code>tuple[float, float, float]</code> DEFAULT: <code>(0.9, 0.999, 0.9999)</code> </p> <code>alpha</code> <p>the exponential decay rate of the second moment estimates</p> <p> TYPE: <code>float</code> DEFAULT: <code>5.0</code> </p> <code>eps</code> <p>term added to the denominator to improve numerical stability</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-08</code> </p> <code>weight_decay</code> <p>weight decay (L2 penalty)</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> Source code in <code>holocron/optim/ademamix.py</code> <pre><code>def __init__(\n    self,\n    params: Iterable[torch.nn.Parameter],\n    lr: float = 1e-3,\n    betas: tuple[float, float, float] = (0.9, 0.999, 0.9999),\n    alpha: float = 5.0,\n    eps: float = 1e-8,\n    weight_decay: float = 0.0,\n) -&gt; None:\n    if lr &lt; 0.0:\n        raise ValueError(f\"Invalid learning rate: {lr}\")\n    if eps &lt; 0.0:\n        raise ValueError(f\"Invalid epsilon value: {eps}\")\n    for idx, beta in enumerate(betas):\n        if not 0.0 &lt;= beta &lt; 1.0:\n            raise ValueError(f\"Invalid beta parameter at index {idx}: {beta}\")\n    defaults = {\"lr\": lr, \"betas\": betas, \"alpha\": alpha, \"eps\": eps, \"weight_decay\": weight_decay}\n    super().__init__(params, defaults)\n</code></pre>"},{"location":"reference/optim/#holocron.optim.AdEMAMix.step","title":"step","text":"<pre><code>step(closure: Callable[[], float] | None = None) -&gt; float | None\n</code></pre> <p>Performs a single optimization step.</p> PARAMETER DESCRIPTION <code>closure</code> <p>A closure that reevaluates the model and returns the loss.</p> <p> TYPE: <code>callable</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float | None</code> <p>float | None: loss value</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>if the optimizer does not support sparse gradients</p> Source code in <code>holocron/optim/ademamix.py</code> <pre><code>@torch.no_grad()\ndef step(self, closure: Callable[[], float] | None = None) -&gt; float | None:  # type: ignore[override]\n    \"\"\"Performs a single optimization step.\n\n    Arguments:\n        closure (callable, optional): A closure that reevaluates the model\n            and returns the loss.\n\n    Returns:\n        float | None: loss value\n\n    Raises:\n        RuntimeError: if the optimizer does not support sparse gradients\n    \"\"\"\n    loss = None\n    if closure is not None:\n        with torch.enable_grad():\n            loss = closure()\n\n    for group in self.param_groups:\n        params_with_grad = []\n        grads = []\n        exp_avgs = []\n        exp_avgs_slow = []\n        exp_avg_sqs = []\n        state_steps = []\n\n        for p in group[\"params\"]:\n            if p.grad is not None:\n                params_with_grad.append(p)\n                if p.grad.is_sparse:\n                    raise RuntimeError(f\"{self.__class__.__name__} does not support sparse gradients\")\n                grads.append(p.grad)\n\n                state = self.state[p]\n                # Lazy state initialization\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    # Exponential moving average of gradient values\n                    state[\"exp_avg\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    state[\"exp_avg_slow\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n                    # Exponential moving average of squared gradient values\n                    state[\"exp_avg_sq\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n\n                exp_avgs.append(state[\"exp_avg\"])\n                exp_avgs_slow.append(state[\"exp_avg_slow\"])\n                exp_avg_sqs.append(state[\"exp_avg_sq\"])\n\n                # update the steps for each param group update\n                state[\"step\"] += 1\n                # record the step after step update\n                state_steps.append(state[\"step\"])\n\n        beta1, beta2, beta3 = group[\"betas\"]\n        ademamix(\n            params_with_grad,\n            grads,\n            exp_avgs,\n            exp_avgs_slow,\n            exp_avg_sqs,\n            state_steps,\n            beta1,\n            beta2,\n            beta3,\n            group[\"alpha\"],\n            group[\"lr\"],\n            group[\"weight_decay\"],\n            group[\"eps\"],\n        )\n    return loss\n</code></pre>"},{"location":"reference/optim/#optimizer-wrappers","title":"Optimizer wrappers","text":"<p><code>holocron.optim</code> also implements optimizer wrappers.</p> <p>A base optimizer should always be passed to the wrapper; e.g., you should write your code this way:</p> <pre><code>&gt;&gt;&gt; optimizer = ...\n&gt;&gt;&gt; optimizer = wrapper(optimizer)\n</code></pre>"},{"location":"reference/optim/#holocron.optim.Lookahead","title":"Lookahead","text":"<pre><code>Lookahead(base_optimizer: Optimizer, sync_rate: float = 0.5, sync_period: int = 6)\n</code></pre> <p>               Bases: <code>Optimizer</code></p> <p>Implements the Lookahead optimizer wrapper from <code>\"Lookahead Optimizer: k steps forward, 1 step back\" &lt;https://arxiv.org/pdf/1907.08610.pdf&gt;</code>_.</p> <p>from torch.optim import AdamW from holocron.optim.wrapper import Lookahead model = ... opt = AdamW(model.parameters(), lr=3e-4) opt_wrapper = Lookahead(opt)</p> PARAMETER DESCRIPTION <code>base_optimizer</code> <p>base parameter optimizer</p> <p> TYPE: <code>Optimizer</code> </p> <code>sync_rate</code> <p>rate of weight synchronization</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> <code>sync_period</code> <p>number of step performed on fast weights before weight synchronization</p> <p> TYPE: <code>int</code> DEFAULT: <code>6</code> </p> Source code in <code>holocron/optim/wrapper.py</code> <pre><code>def __init__(\n    self,\n    base_optimizer: torch.optim.Optimizer,\n    sync_rate: float = 0.5,\n    sync_period: int = 6,\n) -&gt; None:\n    if sync_rate &lt; 0 or sync_rate &gt; 1:\n        raise ValueError(f\"expected positive float lower than 1 as sync_rate, received: {sync_rate}\")\n    if not isinstance(sync_period, int) or sync_period &lt; 1:\n        raise ValueError(f\"expected positive integer as sync_period, received: {sync_period}\")\n    # Optimizer attributes\n    self.defaults = {\"sync_rate\": sync_rate, \"sync_period\": sync_period}\n    self.state = defaultdict(dict)\n    # Base optimizer attributes\n    self.base_optimizer = base_optimizer\n    # Wrapper attributes\n    self.fast_steps = 0\n    self.param_groups = []\n    for group in self.base_optimizer.param_groups:\n        self._add_param_group(group)\n</code></pre>"},{"location":"reference/optim/#holocron.optim.Lookahead.step","title":"step","text":"<pre><code>step(closure: Callable[[], float] | None = None) -&gt; float | None\n</code></pre> <p>Performs a single optimization step.</p> PARAMETER DESCRIPTION <code>closure</code> <p>A closure that reevaluates the model and returns the loss.</p> <p> TYPE: <code>Callable[[], float] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float | None</code> <p>loss value</p> Source code in <code>holocron/optim/wrapper.py</code> <pre><code>def step(self, closure: Callable[[], float] | None = None) -&gt; float | None:  # type: ignore[override]\n    \"\"\"Performs a single optimization step.\n\n    Arguments:\n        closure: A closure that reevaluates the model and returns the loss.\n\n    Returns:\n        loss value\n    \"\"\"\n    # Update fast params\n    loss = self.base_optimizer.step(closure)\n    self.fast_steps += 1\n    # Synchronization every sync_period steps on fast params\n    if self.fast_steps % self.defaults[\"sync_period\"] == 0:\n        self.sync_params(self.defaults[\"sync_rate\"])\n\n    return loss\n</code></pre>"},{"location":"reference/optim/#holocron.optim.Lookahead.add_param_group","title":"add_param_group","text":"<pre><code>add_param_group(param_group: dict[str, Any]) -&gt; None\n</code></pre> <p>Adds a parameter group to base optimizer (fast weights) and its corresponding slow version</p> PARAMETER DESCRIPTION <code>param_group</code> <p>parameter group</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>holocron/optim/wrapper.py</code> <pre><code>def add_param_group(self, param_group: dict[str, Any]) -&gt; None:\n    \"\"\"Adds a parameter group to base optimizer (fast weights) and its corresponding slow version\n\n    Args:\n        param_group: parameter group\n    \"\"\"\n    # Add param group to base optimizer\n    self.base_optimizer.add_param_group(param_group)\n\n    # Add the corresponding slow param group\n    self._add_param_group(self.base_optimizer.param_groups[-1])\n</code></pre>"},{"location":"reference/optim/#holocron.optim.Lookahead.sync_params","title":"sync_params","text":"<pre><code>sync_params(sync_rate: float = 0.0) -&gt; None\n</code></pre> <p>Synchronize parameters as follows: slow_param &lt;- slow_param + sync_rate * (fast_param - slow_param)</p> PARAMETER DESCRIPTION <code>sync_rate</code> <p>synchronization rate of parameters</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> Source code in <code>holocron/optim/wrapper.py</code> <pre><code>def sync_params(self, sync_rate: float = 0.0) -&gt; None:\n    \"\"\"Synchronize parameters as follows:\n    slow_param &lt;- slow_param + sync_rate * (fast_param - slow_param)\n\n    Args:\n        sync_rate: synchronization rate of parameters\n    \"\"\"\n    for fast_group, slow_group in zip(self.base_optimizer.param_groups, self.param_groups, strict=True):\n        for fast_p, slow_p in zip(fast_group[\"params\"], slow_group[\"params\"], strict=True):\n            # Outer update\n            if sync_rate &gt; 0:\n                slow_p.data.add_(fast_p.data - slow_p.data, alpha=sync_rate)\n            # Synchronize fast and slow params\n            fast_p.data.copy_(slow_p.data)\n</code></pre>"},{"location":"reference/optim/#holocron.optim.Scout","title":"Scout","text":"<pre><code>Scout(base_optimizer: Optimizer, sync_rate: float = 0.5, sync_period: int = 6)\n</code></pre> <p>               Bases: <code>Optimizer</code></p> <p>Implements a new optimizer wrapper based on \"Lookahead Optimizer: k steps forward, 1 step back\".</p> Example <p>from torch.optim import AdamW from holocron.optim.wrapper import Scout model = ... opt = AdamW(model.parameters(), lr=3e-4) opt_wrapper = Scout(opt)</p> PARAMETER DESCRIPTION <code>base_optimizer</code> <p>base parameter optimizer</p> <p> TYPE: <code>Optimizer</code> </p> <code>sync_rate</code> <p>rate of weight synchronization</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> <code>sync_period</code> <p>number of step performed on fast weights before weight synchronization</p> <p> TYPE: <code>int</code> DEFAULT: <code>6</code> </p> Source code in <code>holocron/optim/wrapper.py</code> <pre><code>def __init__(\n    self,\n    base_optimizer: torch.optim.Optimizer,\n    sync_rate: float = 0.5,\n    sync_period: int = 6,\n) -&gt; None:\n    if sync_rate &lt; 0 or sync_rate &gt; 1:\n        raise ValueError(f\"expected positive float lower than 1 as sync_rate, received: {sync_rate}\")\n    if not isinstance(sync_period, int) or sync_period &lt; 1:\n        raise ValueError(f\"expected positive integer as sync_period, received: {sync_period}\")\n    # Optimizer attributes\n    self.defaults = {\"sync_rate\": sync_rate, \"sync_period\": sync_period}\n    self.state = defaultdict(dict)\n    # Base optimizer attributes\n    self.base_optimizer = base_optimizer\n    # Wrapper attributes\n    self.fast_steps = 0\n    self.param_groups = []\n    for group in self.base_optimizer.param_groups:\n        self._add_param_group(group)\n    # Buffer for scouting\n    self.buffer = [p.data.unsqueeze(0) for group in self.param_groups for p in group[\"params\"]]\n</code></pre>"},{"location":"reference/optim/#holocron.optim.Scout.step","title":"step","text":"<pre><code>step(closure: Callable[[], float] | None = None) -&gt; float | None\n</code></pre> <p>Performs a single optimization step.</p> PARAMETER DESCRIPTION <code>closure</code> <p>A closure that reevaluates the model and returns the loss.</p> <p> TYPE: <code>Callable[[], float] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float | None</code> <p>loss value</p> Source code in <code>holocron/optim/wrapper.py</code> <pre><code>def step(self, closure: Callable[[], float] | None = None) -&gt; float | None:  # type: ignore[override]\n    \"\"\"Performs a single optimization step.\n\n    Arguments:\n        closure: A closure that reevaluates the model and returns the loss.\n\n    Returns:\n        loss value\n    \"\"\"\n    # Update fast params\n    loss = self.base_optimizer.step(closure)\n    self.fast_steps += 1\n    # Add it to buffer\n    idx = 0\n    for group in self.base_optimizer.param_groups:\n        for p in group[\"params\"]:\n            self.buffer[idx] = torch.cat((self.buffer[idx], p.data.clone().detach().unsqueeze(0)))\n            idx += 1\n    # Synchronization every sync_period steps on fast params\n    if self.fast_steps % self.defaults[\"sync_period\"] == 0:\n        # Compute STD of updates\n        update_similarity = []\n        for _ in range(len(self.buffer)):\n            p = self.buffer.pop()\n            update = p[1:] - p[:-1]\n            max_dev = (update - torch.mean(update, dim=0)).abs().max(dim=0).values\n            update_similarity.append((torch.std(update, dim=0) / max_dev).mean().item())\n        update_coherence = sum(update_similarity) / len(update_similarity)\n\n        sync_rate = max(1 - update_coherence, self.defaults[\"sync_rate\"])\n        # sync_rate = self.defaults['sync_rate']\n        self.sync_params(sync_rate)\n        # Reset buffer\n        self.buffer = []\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                self.buffer.append(p.data.unsqueeze(0))\n\n    return loss\n</code></pre>"},{"location":"reference/optim/#holocron.optim.Scout.add_param_group","title":"add_param_group","text":"<pre><code>add_param_group(param_group: dict[str, Any]) -&gt; None\n</code></pre> <p>Adds a parameter group to base optimizer (fast weights) and its corresponding slow version</p> PARAMETER DESCRIPTION <code>param_group</code> <p>parameter group</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>holocron/optim/wrapper.py</code> <pre><code>def add_param_group(self, param_group: dict[str, Any]) -&gt; None:\n    \"\"\"Adds a parameter group to base optimizer (fast weights) and its corresponding slow version\n\n    Args:\n        param_group: parameter group\n    \"\"\"\n    # Add param group to base optimizer\n    self.base_optimizer.add_param_group(param_group)\n\n    # Add the corresponding slow param group\n    self._add_param_group(self.base_optimizer.param_groups[-1])\n</code></pre>"},{"location":"reference/optim/#holocron.optim.Scout.sync_params","title":"sync_params","text":"<pre><code>sync_params(sync_rate: float = 0.0) -&gt; None\n</code></pre> <p>Synchronize parameters as follows: slow_param &lt;- slow_param + sync_rate * (fast_param - slow_param)</p> PARAMETER DESCRIPTION <code>sync_rate</code> <p>synchronization rate of parameters</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> Source code in <code>holocron/optim/wrapper.py</code> <pre><code>def sync_params(self, sync_rate: float = 0.0) -&gt; None:\n    \"\"\"Synchronize parameters as follows:\n    slow_param &lt;- slow_param + sync_rate * (fast_param - slow_param)\n\n    Args:\n        sync_rate: synchronization rate of parameters\n    \"\"\"\n    for fast_group, slow_group in zip(self.base_optimizer.param_groups, self.param_groups, strict=True):\n        for fast_p, slow_p in zip(fast_group[\"params\"], slow_group[\"params\"], strict=True):\n            # Outer update\n            if sync_rate &gt; 0:\n                slow_p.data.add_(fast_p.data - slow_p.data, alpha=sync_rate)\n            # Synchronize fast and slow params\n            fast_p.data.copy_(slow_p.data)\n</code></pre>"},{"location":"reference/trainer/","title":"holocron.trainer","text":"<p><code>holocron.trainer</code> provides some basic objects for training purposes.</p>"},{"location":"reference/trainer/#holocron.trainer.Trainer","title":"Trainer","text":"<pre><code>Trainer(model: Module, train_loader: DataLoader, val_loader: DataLoader, criterion: Module, optimizer: Optimizer, gpu: int | None = None, output_file: str = './checkpoint.pth', amp: bool = False, skip_nan_loss: bool = False, nan_tolerance: int = 5, gradient_acc: int = 1, gradient_clip: float | None = None, on_epoch_end: Callable[[dict[str, float]], Any] | None = None)\n</code></pre> <p>Baseline trainer class.</p> PARAMETER DESCRIPTION <code>model</code> <p>model to train</p> <p> TYPE: <code>Module</code> </p> <code>train_loader</code> <p>training loader</p> <p> TYPE: <code>DataLoader</code> </p> <code>val_loader</code> <p>validation loader</p> <p> TYPE: <code>DataLoader</code> </p> <code>criterion</code> <p>loss criterion</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>parameter optimizer</p> <p> TYPE: <code>Optimizer</code> </p> <code>gpu</code> <p>index of the GPU to use</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>output_file</code> <p>path where checkpoints will be saved</p> <p> TYPE: <code>str</code> DEFAULT: <code>'./checkpoint.pth'</code> </p> <code>amp</code> <p>whether to use automatic mixed precision</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>skip_nan_loss</code> <p>whether the optimizer step should be skipped when the loss is NaN</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>nan_tolerance</code> <p>number of consecutive batches with NaN loss before stopping the training</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>gradient_acc</code> <p>number of batches to accumulate the gradient of before performing the update step</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>gradient_clip</code> <p>the gradient clip value</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>on_epoch_end</code> <p>callback triggered at the end of an epoch</p> <p> TYPE: <code>Callable[[dict[str, float]], Any] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    train_loader: DataLoader,\n    val_loader: DataLoader,\n    criterion: nn.Module,\n    optimizer: torch.optim.Optimizer,\n    gpu: int | None = None,\n    output_file: str = \"./checkpoint.pth\",\n    amp: bool = False,\n    skip_nan_loss: bool = False,\n    nan_tolerance: int = 5,\n    gradient_acc: int = 1,\n    gradient_clip: float | None = None,\n    on_epoch_end: Callable[[dict[str, float]], Any] | None = None,\n) -&gt; None:\n    self.model = model\n    self.train_loader = train_loader\n    self.val_loader = val_loader\n    self.criterion = criterion\n    self.optimizer = optimizer\n    self.amp = amp\n    self.scaler: GradScaler\n    self.on_epoch_end = on_epoch_end\n    self.skip_nan_loss = skip_nan_loss\n    self.nan_tolerance = nan_tolerance\n    self.gradient_acc = gradient_acc\n    self.grad_clip = gradient_clip\n\n    # Output file\n    self.output_file = output_file\n\n    # Initialize\n    self.step = 0\n    self.start_epoch = 0\n    self.epoch = 0\n    self._grad_count = 0\n    self.min_loss = math.inf\n    self.gpu = gpu\n    self._params: tuple[ParamSeq, ParamSeq] = ([], [])\n    self.lr_recorder: list[float] = []\n    self.loss_recorder: list[float] = []\n    self.set_device(gpu)\n    self._reset_opt(self.optimizer.defaults[\"lr\"])\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.Trainer.set_device","title":"set_device","text":"<pre><code>set_device(gpu: int | None = None) -&gt; None\n</code></pre> <p>Move tensor objects to the target GPU</p> PARAMETER DESCRIPTION <code>gpu</code> <p>index of the target GPU device</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>AssertionError</code> <p>if PyTorch cannot access the GPU</p> <code>ValueError</code> <p>if the device index is invalid</p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def set_device(self, gpu: int | None = None) -&gt; None:\n    \"\"\"Move tensor objects to the target GPU\n\n    Args:\n        gpu: index of the target GPU device\n\n    Raises:\n        AssertionError: if PyTorch cannot access the GPU\n        ValueError: if the device index is invalid\n    \"\"\"\n    if isinstance(gpu, int):\n        if not torch.cuda.is_available():\n            raise AssertionError(\"PyTorch cannot access your GPU. Please investigate!\")\n        if gpu &gt;= torch.cuda.device_count():\n            raise ValueError(\"Invalid device index\")\n        torch.cuda.set_device(gpu)\n        self.model = self.model.cuda()\n        if isinstance(self.criterion, torch.nn.Module):\n            self.criterion = self.criterion.cuda()\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.Trainer.save","title":"save","text":"<pre><code>save(output_file: str) -&gt; None\n</code></pre> <p>Save a trainer checkpoint</p> PARAMETER DESCRIPTION <code>output_file</code> <p>destination file path</p> <p> TYPE: <code>str</code> </p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def save(self, output_file: str) -&gt; None:\n    \"\"\"Save a trainer checkpoint\n\n    Args:\n        output_file: destination file path\n    \"\"\"\n    torch.save(\n        {\n            \"epoch\": self.epoch,\n            \"step\": self.step,\n            \"min_loss\": self.min_loss,\n            \"model\": self.model.state_dict(),\n        },\n        output_file,\n        _use_new_zipfile_serialization=False,\n    )\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.Trainer.load","title":"load","text":"<pre><code>load(state: dict[str, Any]) -&gt; None\n</code></pre> <p>Resume from a trainer state</p> PARAMETER DESCRIPTION <code>state</code> <p>checkpoint dictionary</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def load(self, state: dict[str, Any]) -&gt; None:\n    \"\"\"Resume from a trainer state\n\n    Args:\n        state: checkpoint dictionary\n    \"\"\"\n    self.start_epoch = state[\"epoch\"]\n    self.epoch = self.start_epoch\n    self.step = state[\"step\"]\n    self.min_loss = state[\"min_loss\"]\n    self.model.load_state_dict(state[\"model\"])\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.Trainer.to_cuda","title":"to_cuda","text":"<pre><code>to_cuda(x: Tensor, target: Tensor | list[dict[str, Tensor]]) -&gt; tuple[Tensor, Tensor | list[dict[str, Tensor]]]\n</code></pre> <p>Move input and target to GPU</p> PARAMETER DESCRIPTION <code>x</code> <p>input tensor</p> <p> TYPE: <code>Tensor</code> </p> <code>target</code> <p>target tensor or list of target dictionaries</p> <p> TYPE: <code>Tensor | list[dict[str, Tensor]]</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor, Tensor | list[dict[str, Tensor]]]</code> <p>tuple of input and target tensors</p> RAISES DESCRIPTION <code>ValueError</code> <p>if the device index is invalid</p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def to_cuda(\n    self, x: Tensor, target: Tensor | list[dict[str, Tensor]]\n) -&gt; tuple[Tensor, Tensor | list[dict[str, Tensor]]]:\n    \"\"\"Move input and target to GPU\n\n    Args:\n        x: input tensor\n        target: target tensor or list of target dictionaries\n\n    Returns:\n        tuple of input and target tensors\n\n    Raises:\n        ValueError: if the device index is invalid\n    \"\"\"\n    if isinstance(self.gpu, int):\n        if self.gpu &gt;= torch.cuda.device_count():\n            raise ValueError(\"Invalid device index\")\n        return self._to_cuda(x, target)  # type: ignore[arg-type]\n    return x, target\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.Trainer.fit_n_epochs","title":"fit_n_epochs","text":"<pre><code>fit_n_epochs(num_epochs: int, lr: float, freeze_until: str | None = None, sched_type: str = 'onecycle', norm_weight_decay: float | None = None, **kwargs: Any) -&gt; None\n</code></pre> <p>Train the model for a given number of epochs.</p> PARAMETER DESCRIPTION <code>num_epochs</code> <p>number of epochs to train</p> <p> TYPE: <code>int</code> </p> <code>lr</code> <p>learning rate to be used by the scheduler</p> <p> TYPE: <code>float</code> </p> <code>freeze_until</code> <p>last layer to freeze</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>sched_type</code> <p>type of scheduler to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>'onecycle'</code> </p> <code>norm_weight_decay</code> <p>weight decay to apply to normalization parameters</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>keyword args passed to the <code>LRScheduler</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def fit_n_epochs(\n    self,\n    num_epochs: int,\n    lr: float,\n    freeze_until: str | None = None,\n    sched_type: str = \"onecycle\",\n    norm_weight_decay: float | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Train the model for a given number of epochs.\n\n    Args:\n        num_epochs: number of epochs to train\n        lr: learning rate to be used by the scheduler\n        freeze_until: last layer to freeze\n        sched_type: type of scheduler to use\n        norm_weight_decay: weight decay to apply to normalization parameters\n        **kwargs: keyword args passed to the [`LRScheduler`][torch.optim.lr_scheduler.LRScheduler]\n    \"\"\"\n    freeze_model(self.model.train(), freeze_until)\n    # Update param groups &amp; LR\n    self._reset_opt(lr, norm_weight_decay)\n    # Scheduler\n    self._reset_scheduler(lr, num_epochs, sched_type, **kwargs)\n\n    if self.amp:\n        self.scaler = GradScaler(\"cuda\")\n\n    mb = master_bar(range(num_epochs))\n    for _ in mb:\n        self._fit_epoch(mb)\n        eval_metrics = self.evaluate()\n\n        # master bar\n        mb.main_bar.comment = f\"Epoch {self.epoch}/{self.start_epoch + num_epochs}\"\n        mb.write(f\"Epoch {self.epoch}/{self.start_epoch + num_epochs} - {self._eval_metrics_str(eval_metrics)}\")\n\n        if eval_metrics[\"val_loss\"] &lt; self.min_loss:\n            print(  # noqa: T201\n                f\"Validation loss decreased {self.min_loss:.4} --&gt; {eval_metrics['val_loss']:.4}: saving state...\"\n            )\n            self.min_loss = eval_metrics[\"val_loss\"]\n            self.save(self.output_file)\n\n        if self.on_epoch_end is not None:\n            self.on_epoch_end(eval_metrics)\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.Trainer.find_lr","title":"find_lr","text":"<pre><code>find_lr(freeze_until: str | None = None, start_lr: float = 1e-07, end_lr: float = 1, norm_weight_decay: float | None = None, num_it: int = 100) -&gt; None\n</code></pre> <p>Gridsearch the optimal learning rate for the training as described in \"Cyclical Learning Rates for Training Neural Networks\".</p> PARAMETER DESCRIPTION <code>freeze_until</code> <p>last layer to freeze</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>start_lr</code> <p>initial learning rate</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-07</code> </p> <code>end_lr</code> <p>final learning rate</p> <p> TYPE: <code>float</code> DEFAULT: <code>1</code> </p> <code>norm_weight_decay</code> <p>weight decay to apply to normalization parameters</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>num_it</code> <p>number of iterations to perform</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the number of iterations is greater than the number of available batches</p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def find_lr(\n    self,\n    freeze_until: str | None = None,\n    start_lr: float = 1e-7,\n    end_lr: float = 1,\n    norm_weight_decay: float | None = None,\n    num_it: int = 100,\n) -&gt; None:\n    \"\"\"Gridsearch the optimal learning rate for the training as described in\n    [\"Cyclical Learning Rates for Training Neural Networks\"](https://arxiv.org/pdf/1506.01186.pdf).\n\n    Args:\n       freeze_until: last layer to freeze\n       start_lr: initial learning rate\n       end_lr: final learning rate\n       norm_weight_decay: weight decay to apply to normalization parameters\n       num_it: number of iterations to perform\n\n    Raises:\n        ValueError: if the number of iterations is greater than the number of available batches\n    \"\"\"\n    if num_it &gt; len(self.train_loader):\n        raise ValueError(\"the value of `num_it` needs to be lower than the number of available batches\")\n\n    freeze_model(self.model.train(), freeze_until)\n    # Update param groups &amp; LR\n    self._reset_opt(start_lr, norm_weight_decay)\n    gamma = (end_lr / start_lr) ** (1 / (num_it - 1))\n    scheduler = MultiplicativeLR(self.optimizer, lambda step: gamma)\n\n    self.lr_recorder = [start_lr * gamma**idx for idx in range(num_it)]\n    self.loss_recorder = []\n\n    if self.amp:\n        self.scaler = GradScaler(\"cuda\")\n\n    for batch_idx, (x, target) in enumerate(self.train_loader):\n        x, target = self.to_cuda(x, target)\n\n        # Forward\n        batch_loss: Tensor = self._get_loss(x, target)  # type: ignore[assignment]\n        self._backprop_step(batch_loss)\n        # Update LR\n        scheduler.step()\n\n        # Record\n        if torch.isnan(batch_loss) or torch.isinf(batch_loss):\n            if batch_idx == 0:\n                raise ValueError(\"loss value is NaN or inf.\")\n            break\n        self.loss_recorder.append(batch_loss.item())\n        # Stop after the number of iterations\n        if batch_idx + 1 == num_it:\n            break\n\n    self.lr_recorder = self.lr_recorder[: len(self.loss_recorder)]\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.Trainer.plot_recorder","title":"plot_recorder","text":"<pre><code>plot_recorder(beta: float = 0.95, **kwargs: Any) -&gt; None\n</code></pre> <p>Display the results of the LR grid search</p> PARAMETER DESCRIPTION <code>beta</code> <p>smoothing factor</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.95</code> </p> <code>**kwargs</code> <p>keyword args of <code>matplotlib.pyplot.show</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>AssertionError</code> <p>if the number of learning rate recorder and loss recorder are not the same or if the number of learning rate recorder is 0</p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def plot_recorder(self, beta: float = 0.95, **kwargs: Any) -&gt; None:\n    \"\"\"Display the results of the LR grid search\n\n    Args:\n        beta: smoothing factor\n        **kwargs: keyword args of [`matplotlib.pyplot.show`][matplotlib.pyplot.show]\n\n    Raises:\n        AssertionError: if the number of learning rate recorder and loss recorder are not the same or if the number of learning rate recorder is 0\n    \"\"\"\n    if len(self.lr_recorder) != len(self.loss_recorder) or len(self.lr_recorder) == 0:\n        raise AssertionError(\"Please run the `lr_find` method first\")\n\n    # Exp moving average of loss\n    smoothed_losses = []\n    avg_loss = 0.0\n    for idx, loss in enumerate(self.loss_recorder):\n        avg_loss = beta * avg_loss + (1 - beta) * loss\n        smoothed_losses.append(avg_loss / (1 - beta ** (idx + 1)))\n\n    # Properly rescale Y-axis\n    data_slice = slice(\n        min(len(self.loss_recorder) // 10, 10),\n        -min(len(self.loss_recorder) // 20, 5) if len(self.loss_recorder) &gt;= 20 else len(self.loss_recorder),\n    )\n    vals: np.ndarray = np.array(smoothed_losses[data_slice])\n    min_idx = vals.argmin()\n    max_val = vals.max() if min_idx is None else vals[: min_idx + 1].max()\n    delta = max_val - vals[min_idx]\n\n    plt.plot(self.lr_recorder[data_slice], smoothed_losses[data_slice])\n    plt.xscale(\"log\")\n    plt.xlabel(\"Learning Rate\")\n    plt.ylabel(\"Training loss\")\n    plt.ylim(vals[min_idx] - 0.1 * delta, max_val + 0.2 * delta)\n    plt.grid(True, linestyle=\"--\", axis=\"x\")\n    plt.show(**kwargs)\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.Trainer.check_setup","title":"check_setup","text":"<pre><code>check_setup(freeze_until: str | None = None, lr: float = 0.0003, norm_weight_decay: float | None = None, num_it: int = 100, **kwargs: Any) -&gt; None\n</code></pre> <p>Check whether you can overfit one batch</p> PARAMETER DESCRIPTION <code>freeze_until</code> <p>last layer to freeze</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>lr</code> <p>learning rate to be used for training</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0003</code> </p> <code>norm_weight_decay</code> <p>weight decay to apply to normalization parameters</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>num_it</code> <p>number of iterations to perform</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>**kwargs</code> <p>keyword args of <code>matplotlib.pyplot.show</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the loss value is NaN or inf</p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def check_setup(\n    self,\n    freeze_until: str | None = None,\n    lr: float = 3e-4,\n    norm_weight_decay: float | None = None,\n    num_it: int = 100,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Check whether you can overfit one batch\n\n    Args:\n        freeze_until: last layer to freeze\n        lr: learning rate to be used for training\n        norm_weight_decay: weight decay to apply to normalization parameters\n        num_it: number of iterations to perform\n        **kwargs: keyword args of [`matplotlib.pyplot.show`][matplotlib.pyplot.show]\n\n    Raises:\n        ValueError: if the loss value is NaN or inf\n    \"\"\"\n    freeze_model(self.model.train(), freeze_until)\n    # Update param groups &amp; LR\n    self._reset_opt(lr, norm_weight_decay)\n\n    x, target = next(iter(self.train_loader))\n    x, target = self.to_cuda(x, target)\n\n    losses = []\n\n    if self.amp:\n        self.scaler = GradScaler(\"cuda\")\n\n    for _ in range(num_it):\n        # Forward\n        batch_loss: Tensor = self._get_loss(x, target)  # type: ignore[assignment]\n        # Backprop\n        self._backprop_step(batch_loss)\n\n        if torch.isnan(batch_loss) or torch.isinf(batch_loss):\n            raise ValueError(\"loss value is NaN or inf.\")\n\n        losses.append(batch_loss.item())\n\n    plt.plot(np.arange(len(losses)), losses)\n    plt.xlabel(\"Optimization steps\")\n    plt.ylabel(\"Training loss\")\n    plt.grid(True, linestyle=\"--\", axis=\"x\")\n    plt.show(**kwargs)\n</code></pre>"},{"location":"reference/trainer/#image-classification","title":"Image classification","text":""},{"location":"reference/trainer/#holocron.trainer.ClassificationTrainer","title":"ClassificationTrainer","text":"<pre><code>ClassificationTrainer(model: Module, train_loader: DataLoader, val_loader: DataLoader, criterion: Module, optimizer: Optimizer, gpu: int | None = None, output_file: str = './checkpoint.pth', amp: bool = False, skip_nan_loss: bool = False, nan_tolerance: int = 5, gradient_acc: int = 1, gradient_clip: float | None = None, on_epoch_end: Callable[[dict[str, float]], Any] | None = None)\n</code></pre> <p>               Bases: <code>Trainer</code></p> <p>Image classification trainer class.</p> PARAMETER DESCRIPTION <code>model</code> <p>model to train</p> <p> TYPE: <code>Module</code> </p> <code>train_loader</code> <p>training loader</p> <p> TYPE: <code>DataLoader</code> </p> <code>val_loader</code> <p>validation loader</p> <p> TYPE: <code>DataLoader</code> </p> <code>criterion</code> <p>loss criterion</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>parameter optimizer</p> <p> TYPE: <code>Optimizer</code> </p> <code>gpu</code> <p>index of the GPU to use</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>output_file</code> <p>path where checkpoints will be saved</p> <p> TYPE: <code>str</code> DEFAULT: <code>'./checkpoint.pth'</code> </p> <code>amp</code> <p>whether to use automatic mixed precision</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>skip_nan_loss</code> <p>whether the optimizer step should be skipped when the loss is NaN</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>nan_tolerance</code> <p>number of consecutive batches with NaN loss before stopping the training</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>gradient_acc</code> <p>number of batches to accumulate the gradient of before performing the update step</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>gradient_clip</code> <p>the gradient clip value</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>on_epoch_end</code> <p>callback triggered at the end of an epoch</p> <p> TYPE: <code>Callable[[dict[str, float]], Any] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    train_loader: DataLoader,\n    val_loader: DataLoader,\n    criterion: nn.Module,\n    optimizer: torch.optim.Optimizer,\n    gpu: int | None = None,\n    output_file: str = \"./checkpoint.pth\",\n    amp: bool = False,\n    skip_nan_loss: bool = False,\n    nan_tolerance: int = 5,\n    gradient_acc: int = 1,\n    gradient_clip: float | None = None,\n    on_epoch_end: Callable[[dict[str, float]], Any] | None = None,\n) -&gt; None:\n    self.model = model\n    self.train_loader = train_loader\n    self.val_loader = val_loader\n    self.criterion = criterion\n    self.optimizer = optimizer\n    self.amp = amp\n    self.scaler: GradScaler\n    self.on_epoch_end = on_epoch_end\n    self.skip_nan_loss = skip_nan_loss\n    self.nan_tolerance = nan_tolerance\n    self.gradient_acc = gradient_acc\n    self.grad_clip = gradient_clip\n\n    # Output file\n    self.output_file = output_file\n\n    # Initialize\n    self.step = 0\n    self.start_epoch = 0\n    self.epoch = 0\n    self._grad_count = 0\n    self.min_loss = math.inf\n    self.gpu = gpu\n    self._params: tuple[ParamSeq, ParamSeq] = ([], [])\n    self.lr_recorder: list[float] = []\n    self.loss_recorder: list[float] = []\n    self.set_device(gpu)\n    self._reset_opt(self.optimizer.defaults[\"lr\"])\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.ClassificationTrainer.set_device","title":"set_device","text":"<pre><code>set_device(gpu: int | None = None) -&gt; None\n</code></pre> <p>Move tensor objects to the target GPU</p> PARAMETER DESCRIPTION <code>gpu</code> <p>index of the target GPU device</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>AssertionError</code> <p>if PyTorch cannot access the GPU</p> <code>ValueError</code> <p>if the device index is invalid</p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def set_device(self, gpu: int | None = None) -&gt; None:\n    \"\"\"Move tensor objects to the target GPU\n\n    Args:\n        gpu: index of the target GPU device\n\n    Raises:\n        AssertionError: if PyTorch cannot access the GPU\n        ValueError: if the device index is invalid\n    \"\"\"\n    if isinstance(gpu, int):\n        if not torch.cuda.is_available():\n            raise AssertionError(\"PyTorch cannot access your GPU. Please investigate!\")\n        if gpu &gt;= torch.cuda.device_count():\n            raise ValueError(\"Invalid device index\")\n        torch.cuda.set_device(gpu)\n        self.model = self.model.cuda()\n        if isinstance(self.criterion, torch.nn.Module):\n            self.criterion = self.criterion.cuda()\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.ClassificationTrainer.save","title":"save","text":"<pre><code>save(output_file: str) -&gt; None\n</code></pre> <p>Save a trainer checkpoint</p> PARAMETER DESCRIPTION <code>output_file</code> <p>destination file path</p> <p> TYPE: <code>str</code> </p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def save(self, output_file: str) -&gt; None:\n    \"\"\"Save a trainer checkpoint\n\n    Args:\n        output_file: destination file path\n    \"\"\"\n    torch.save(\n        {\n            \"epoch\": self.epoch,\n            \"step\": self.step,\n            \"min_loss\": self.min_loss,\n            \"model\": self.model.state_dict(),\n        },\n        output_file,\n        _use_new_zipfile_serialization=False,\n    )\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.ClassificationTrainer.load","title":"load","text":"<pre><code>load(state: dict[str, Any]) -&gt; None\n</code></pre> <p>Resume from a trainer state</p> PARAMETER DESCRIPTION <code>state</code> <p>checkpoint dictionary</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def load(self, state: dict[str, Any]) -&gt; None:\n    \"\"\"Resume from a trainer state\n\n    Args:\n        state: checkpoint dictionary\n    \"\"\"\n    self.start_epoch = state[\"epoch\"]\n    self.epoch = self.start_epoch\n    self.step = state[\"step\"]\n    self.min_loss = state[\"min_loss\"]\n    self.model.load_state_dict(state[\"model\"])\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.ClassificationTrainer.to_cuda","title":"to_cuda","text":"<pre><code>to_cuda(x: Tensor, target: Tensor | list[dict[str, Tensor]]) -&gt; tuple[Tensor, Tensor | list[dict[str, Tensor]]]\n</code></pre> <p>Move input and target to GPU</p> PARAMETER DESCRIPTION <code>x</code> <p>input tensor</p> <p> TYPE: <code>Tensor</code> </p> <code>target</code> <p>target tensor or list of target dictionaries</p> <p> TYPE: <code>Tensor | list[dict[str, Tensor]]</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor, Tensor | list[dict[str, Tensor]]]</code> <p>tuple of input and target tensors</p> RAISES DESCRIPTION <code>ValueError</code> <p>if the device index is invalid</p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def to_cuda(\n    self, x: Tensor, target: Tensor | list[dict[str, Tensor]]\n) -&gt; tuple[Tensor, Tensor | list[dict[str, Tensor]]]:\n    \"\"\"Move input and target to GPU\n\n    Args:\n        x: input tensor\n        target: target tensor or list of target dictionaries\n\n    Returns:\n        tuple of input and target tensors\n\n    Raises:\n        ValueError: if the device index is invalid\n    \"\"\"\n    if isinstance(self.gpu, int):\n        if self.gpu &gt;= torch.cuda.device_count():\n            raise ValueError(\"Invalid device index\")\n        return self._to_cuda(x, target)  # type: ignore[arg-type]\n    return x, target\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.ClassificationTrainer.fit_n_epochs","title":"fit_n_epochs","text":"<pre><code>fit_n_epochs(num_epochs: int, lr: float, freeze_until: str | None = None, sched_type: str = 'onecycle', norm_weight_decay: float | None = None, **kwargs: Any) -&gt; None\n</code></pre> <p>Train the model for a given number of epochs.</p> PARAMETER DESCRIPTION <code>num_epochs</code> <p>number of epochs to train</p> <p> TYPE: <code>int</code> </p> <code>lr</code> <p>learning rate to be used by the scheduler</p> <p> TYPE: <code>float</code> </p> <code>freeze_until</code> <p>last layer to freeze</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>sched_type</code> <p>type of scheduler to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>'onecycle'</code> </p> <code>norm_weight_decay</code> <p>weight decay to apply to normalization parameters</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>keyword args passed to the <code>LRScheduler</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def fit_n_epochs(\n    self,\n    num_epochs: int,\n    lr: float,\n    freeze_until: str | None = None,\n    sched_type: str = \"onecycle\",\n    norm_weight_decay: float | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Train the model for a given number of epochs.\n\n    Args:\n        num_epochs: number of epochs to train\n        lr: learning rate to be used by the scheduler\n        freeze_until: last layer to freeze\n        sched_type: type of scheduler to use\n        norm_weight_decay: weight decay to apply to normalization parameters\n        **kwargs: keyword args passed to the [`LRScheduler`][torch.optim.lr_scheduler.LRScheduler]\n    \"\"\"\n    freeze_model(self.model.train(), freeze_until)\n    # Update param groups &amp; LR\n    self._reset_opt(lr, norm_weight_decay)\n    # Scheduler\n    self._reset_scheduler(lr, num_epochs, sched_type, **kwargs)\n\n    if self.amp:\n        self.scaler = GradScaler(\"cuda\")\n\n    mb = master_bar(range(num_epochs))\n    for _ in mb:\n        self._fit_epoch(mb)\n        eval_metrics = self.evaluate()\n\n        # master bar\n        mb.main_bar.comment = f\"Epoch {self.epoch}/{self.start_epoch + num_epochs}\"\n        mb.write(f\"Epoch {self.epoch}/{self.start_epoch + num_epochs} - {self._eval_metrics_str(eval_metrics)}\")\n\n        if eval_metrics[\"val_loss\"] &lt; self.min_loss:\n            print(  # noqa: T201\n                f\"Validation loss decreased {self.min_loss:.4} --&gt; {eval_metrics['val_loss']:.4}: saving state...\"\n            )\n            self.min_loss = eval_metrics[\"val_loss\"]\n            self.save(self.output_file)\n\n        if self.on_epoch_end is not None:\n            self.on_epoch_end(eval_metrics)\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.ClassificationTrainer.find_lr","title":"find_lr","text":"<pre><code>find_lr(freeze_until: str | None = None, start_lr: float = 1e-07, end_lr: float = 1, norm_weight_decay: float | None = None, num_it: int = 100) -&gt; None\n</code></pre> <p>Gridsearch the optimal learning rate for the training as described in \"Cyclical Learning Rates for Training Neural Networks\".</p> PARAMETER DESCRIPTION <code>freeze_until</code> <p>last layer to freeze</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>start_lr</code> <p>initial learning rate</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-07</code> </p> <code>end_lr</code> <p>final learning rate</p> <p> TYPE: <code>float</code> DEFAULT: <code>1</code> </p> <code>norm_weight_decay</code> <p>weight decay to apply to normalization parameters</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>num_it</code> <p>number of iterations to perform</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the number of iterations is greater than the number of available batches</p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def find_lr(\n    self,\n    freeze_until: str | None = None,\n    start_lr: float = 1e-7,\n    end_lr: float = 1,\n    norm_weight_decay: float | None = None,\n    num_it: int = 100,\n) -&gt; None:\n    \"\"\"Gridsearch the optimal learning rate for the training as described in\n    [\"Cyclical Learning Rates for Training Neural Networks\"](https://arxiv.org/pdf/1506.01186.pdf).\n\n    Args:\n       freeze_until: last layer to freeze\n       start_lr: initial learning rate\n       end_lr: final learning rate\n       norm_weight_decay: weight decay to apply to normalization parameters\n       num_it: number of iterations to perform\n\n    Raises:\n        ValueError: if the number of iterations is greater than the number of available batches\n    \"\"\"\n    if num_it &gt; len(self.train_loader):\n        raise ValueError(\"the value of `num_it` needs to be lower than the number of available batches\")\n\n    freeze_model(self.model.train(), freeze_until)\n    # Update param groups &amp; LR\n    self._reset_opt(start_lr, norm_weight_decay)\n    gamma = (end_lr / start_lr) ** (1 / (num_it - 1))\n    scheduler = MultiplicativeLR(self.optimizer, lambda step: gamma)\n\n    self.lr_recorder = [start_lr * gamma**idx for idx in range(num_it)]\n    self.loss_recorder = []\n\n    if self.amp:\n        self.scaler = GradScaler(\"cuda\")\n\n    for batch_idx, (x, target) in enumerate(self.train_loader):\n        x, target = self.to_cuda(x, target)\n\n        # Forward\n        batch_loss: Tensor = self._get_loss(x, target)  # type: ignore[assignment]\n        self._backprop_step(batch_loss)\n        # Update LR\n        scheduler.step()\n\n        # Record\n        if torch.isnan(batch_loss) or torch.isinf(batch_loss):\n            if batch_idx == 0:\n                raise ValueError(\"loss value is NaN or inf.\")\n            break\n        self.loss_recorder.append(batch_loss.item())\n        # Stop after the number of iterations\n        if batch_idx + 1 == num_it:\n            break\n\n    self.lr_recorder = self.lr_recorder[: len(self.loss_recorder)]\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.ClassificationTrainer.plot_recorder","title":"plot_recorder","text":"<pre><code>plot_recorder(beta: float = 0.95, **kwargs: Any) -&gt; None\n</code></pre> <p>Display the results of the LR grid search</p> PARAMETER DESCRIPTION <code>beta</code> <p>smoothing factor</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.95</code> </p> <code>**kwargs</code> <p>keyword args of <code>matplotlib.pyplot.show</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>AssertionError</code> <p>if the number of learning rate recorder and loss recorder are not the same or if the number of learning rate recorder is 0</p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def plot_recorder(self, beta: float = 0.95, **kwargs: Any) -&gt; None:\n    \"\"\"Display the results of the LR grid search\n\n    Args:\n        beta: smoothing factor\n        **kwargs: keyword args of [`matplotlib.pyplot.show`][matplotlib.pyplot.show]\n\n    Raises:\n        AssertionError: if the number of learning rate recorder and loss recorder are not the same or if the number of learning rate recorder is 0\n    \"\"\"\n    if len(self.lr_recorder) != len(self.loss_recorder) or len(self.lr_recorder) == 0:\n        raise AssertionError(\"Please run the `lr_find` method first\")\n\n    # Exp moving average of loss\n    smoothed_losses = []\n    avg_loss = 0.0\n    for idx, loss in enumerate(self.loss_recorder):\n        avg_loss = beta * avg_loss + (1 - beta) * loss\n        smoothed_losses.append(avg_loss / (1 - beta ** (idx + 1)))\n\n    # Properly rescale Y-axis\n    data_slice = slice(\n        min(len(self.loss_recorder) // 10, 10),\n        -min(len(self.loss_recorder) // 20, 5) if len(self.loss_recorder) &gt;= 20 else len(self.loss_recorder),\n    )\n    vals: np.ndarray = np.array(smoothed_losses[data_slice])\n    min_idx = vals.argmin()\n    max_val = vals.max() if min_idx is None else vals[: min_idx + 1].max()\n    delta = max_val - vals[min_idx]\n\n    plt.plot(self.lr_recorder[data_slice], smoothed_losses[data_slice])\n    plt.xscale(\"log\")\n    plt.xlabel(\"Learning Rate\")\n    plt.ylabel(\"Training loss\")\n    plt.ylim(vals[min_idx] - 0.1 * delta, max_val + 0.2 * delta)\n    plt.grid(True, linestyle=\"--\", axis=\"x\")\n    plt.show(**kwargs)\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.ClassificationTrainer.check_setup","title":"check_setup","text":"<pre><code>check_setup(freeze_until: str | None = None, lr: float = 0.0003, norm_weight_decay: float | None = None, num_it: int = 100, **kwargs: Any) -&gt; None\n</code></pre> <p>Check whether you can overfit one batch</p> PARAMETER DESCRIPTION <code>freeze_until</code> <p>last layer to freeze</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>lr</code> <p>learning rate to be used for training</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0003</code> </p> <code>norm_weight_decay</code> <p>weight decay to apply to normalization parameters</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>num_it</code> <p>number of iterations to perform</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>**kwargs</code> <p>keyword args of <code>matplotlib.pyplot.show</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the loss value is NaN or inf</p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def check_setup(\n    self,\n    freeze_until: str | None = None,\n    lr: float = 3e-4,\n    norm_weight_decay: float | None = None,\n    num_it: int = 100,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Check whether you can overfit one batch\n\n    Args:\n        freeze_until: last layer to freeze\n        lr: learning rate to be used for training\n        norm_weight_decay: weight decay to apply to normalization parameters\n        num_it: number of iterations to perform\n        **kwargs: keyword args of [`matplotlib.pyplot.show`][matplotlib.pyplot.show]\n\n    Raises:\n        ValueError: if the loss value is NaN or inf\n    \"\"\"\n    freeze_model(self.model.train(), freeze_until)\n    # Update param groups &amp; LR\n    self._reset_opt(lr, norm_weight_decay)\n\n    x, target = next(iter(self.train_loader))\n    x, target = self.to_cuda(x, target)\n\n    losses = []\n\n    if self.amp:\n        self.scaler = GradScaler(\"cuda\")\n\n    for _ in range(num_it):\n        # Forward\n        batch_loss: Tensor = self._get_loss(x, target)  # type: ignore[assignment]\n        # Backprop\n        self._backprop_step(batch_loss)\n\n        if torch.isnan(batch_loss) or torch.isinf(batch_loss):\n            raise ValueError(\"loss value is NaN or inf.\")\n\n        losses.append(batch_loss.item())\n\n    plt.plot(np.arange(len(losses)), losses)\n    plt.xlabel(\"Optimization steps\")\n    plt.ylabel(\"Training loss\")\n    plt.grid(True, linestyle=\"--\", axis=\"x\")\n    plt.show(**kwargs)\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.ClassificationTrainer.evaluate","title":"evaluate","text":"<pre><code>evaluate() -&gt; dict[str, float]\n</code></pre> <p>Evaluate the model on the validation set</p> RETURNS DESCRIPTION <code>dict[str, float]</code> <p>evaluation metrics</p> Source code in <code>holocron/trainer/classification.py</code> <pre><code>@torch.inference_mode()\ndef evaluate(self) -&gt; dict[str, float]:\n    \"\"\"Evaluate the model on the validation set\n\n    Returns:\n        evaluation metrics\n    \"\"\"\n    self.model.eval()\n\n    val_loss, top1, top5, num_samples, num_valid_batches = 0.0, 0, 0, 0, 0\n    for x, target in self.val_loader:\n        x, target = self.to_cuda(x, target)\n\n        loss, out = self._get_loss(x, target, return_logits=True)  # ty: ignore[invalid-argument-type]\n\n        # Safeguard for NaN loss\n        if not torch.isnan(loss) and not torch.isinf(loss):\n            val_loss += loss.item()\n            num_valid_batches += 1\n\n        pred = out.topk(5, dim=1)[1] if out.shape[1] &gt;= 5 else out.argmax(dim=1, keepdim=True)\n        correct = pred.eq(target.view(-1, 1).expand_as(pred))  # ty: ignore[possibly-missing-attribute]\n        top1 += cast(int, correct[:, 0].sum().item())\n        if out.shape[1] &gt;= 5:\n            top5 += cast(int, correct.any(dim=1).sum().item())\n\n        num_samples += x.shape[0]\n\n    val_loss /= num_valid_batches\n\n    return {\"val_loss\": val_loss, \"acc1\": top1 / num_samples, \"acc5\": top5 / num_samples}\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.ClassificationTrainer.plot_top_losses","title":"plot_top_losses","text":"<pre><code>plot_top_losses(mean: tuple[float, float, float], std: tuple[float, float, float], classes: Sequence[str] | None = None, num_samples: int = 12, **kwargs: Any) -&gt; None\n</code></pre> <p>Plot the top losses</p> PARAMETER DESCRIPTION <code>mean</code> <p>mean of the dataset</p> <p> TYPE: <code>tuple[float, float, float]</code> </p> <code>std</code> <p>standard deviation of the dataset</p> <p> TYPE: <code>tuple[float, float, float]</code> </p> <code>classes</code> <p>list of classes</p> <p> TYPE: <code>Sequence[str] | None</code> DEFAULT: <code>None</code> </p> <code>num_samples</code> <p>number of samples to plot</p> <p> TYPE: <code>int</code> DEFAULT: <code>12</code> </p> <code>**kwargs</code> <p>keyword args of <code>matplotlib.pyplot.show</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>AssertionError</code> <p>if the argument 'classes' is not specified for multi-class classification</p> Source code in <code>holocron/trainer/classification.py</code> <pre><code>@torch.inference_mode()\ndef plot_top_losses(\n    self,\n    mean: tuple[float, float, float],\n    std: tuple[float, float, float],\n    classes: Sequence[str] | None = None,\n    num_samples: int = 12,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Plot the top losses\n\n    Args:\n        mean: mean of the dataset\n        std: standard deviation of the dataset\n        classes: list of classes\n        num_samples: number of samples to plot\n        **kwargs: keyword args of [`matplotlib.pyplot.show`][matplotlib.pyplot.show]\n\n    Raises:\n        AssertionError: if the argument 'classes' is not specified for multi-class classification\n    \"\"\"\n    # Record loss, prob, target, image\n    losses = np.zeros(num_samples, dtype=np.float32)\n    preds = np.zeros(num_samples, dtype=int)\n    probs = np.zeros(num_samples, dtype=np.float32)\n    targets = np.zeros(num_samples, dtype=np.float32 if self.is_binary else int)\n    images = [None] * num_samples\n\n    # Switch to unreduced loss\n    reduction = self.criterion.reduction\n    self.criterion.reduction = \"none\"  # type: ignore[assignment]\n    self.model.eval()\n\n    train_iter = iter(self.train_loader)\n\n    for x, target in tqdm(train_iter):\n        x, target = self.to_cuda(x, target)\n\n        # Forward\n        batch_loss, logits = self._get_loss(x, target, return_logits=True)  # ty: ignore[invalid-argument-type]\n\n        # Binary\n        if self.is_binary:\n            batch_loss = batch_loss.squeeze(1)\n            probs_ = torch.sigmoid(logits.squeeze(1))\n        else:\n            probs_ = torch.softmax(logits, 1).max(dim=1).values\n\n        if torch.any(batch_loss &gt; losses.min()):\n            idcs = np.concatenate((losses, batch_loss.cpu().numpy())).argsort()[-num_samples:]\n            kept_idcs = [idx for idx in idcs if idx &lt; num_samples]\n            added_idcs = [idx - num_samples for idx in idcs if idx &gt;= num_samples]\n            # Update\n            losses = np.concatenate((losses[kept_idcs], batch_loss.cpu().numpy()[added_idcs]))\n            probs = np.concatenate((probs[kept_idcs], probs_.cpu().numpy()))\n            if not self.is_binary:\n                preds = np.concatenate((preds[kept_idcs], logits[added_idcs].argmax(dim=1).cpu().numpy()))\n            targets = np.concatenate((targets[kept_idcs], target[added_idcs].cpu().numpy()))  # ty: ignore[invalid-argument-type]\n            imgs = x[added_idcs].cpu() * torch.tensor(std).view(-1, 1, 1)\n            imgs += torch.tensor(mean).view(-1, 1, 1)\n            images = [images[idx] for idx in kept_idcs] + [to_pil_image(img) for img in imgs]\n\n    self.criterion.reduction = reduction\n\n    if not self.is_binary and classes is None:\n        raise AssertionError(\"arg 'classes' must be specified for multi-class classification\")\n\n    # Final sort\n    idcs_ = losses.argsort()[::-1]\n    losses, preds, probs, targets = losses[idcs_], preds[idcs_], probs[idcs_], targets[idcs_]\n    images = [images[idx] for idx in idcs_]\n\n    # Plot it\n    num_cols = 4\n    num_rows = math.ceil(num_samples / num_cols)\n    _, axes = plt.subplots(num_rows, num_cols, figsize=(20, 5))\n    for idx, (img, pred, prob, target, loss) in enumerate(zip(images, preds, probs, targets, losses, strict=True)):\n        row = int(idx / num_cols)\n        col = idx - num_cols * row\n        axes[row][col].imshow(img)\n        # Loss, prob, target\n        if self.is_binary:\n            axes[row][col].title.set_text(f\"{loss:.3} / {prob:.2} / {target:.2}\")\n        # Loss, pred (prob), target\n        else:\n            axes[row][col].title.set_text(\n                f\"{loss:.3} / {classes[pred]} ({prob:.1%}) / {classes[target]}\"  # type: ignore[index]\n            )\n        axes[row][col].axis(\"off\")\n\n    plt.show(**kwargs)\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.BinaryClassificationTrainer","title":"BinaryClassificationTrainer","text":"<pre><code>BinaryClassificationTrainer(model: Module, train_loader: DataLoader, val_loader: DataLoader, criterion: Module, optimizer: Optimizer, gpu: int | None = None, output_file: str = './checkpoint.pth', amp: bool = False, skip_nan_loss: bool = False, nan_tolerance: int = 5, gradient_acc: int = 1, gradient_clip: float | None = None, on_epoch_end: Callable[[dict[str, float]], Any] | None = None)\n</code></pre> <p>               Bases: <code>ClassificationTrainer</code></p> <p>Image binary classification trainer class.</p> PARAMETER DESCRIPTION <code>model</code> <p>model to train</p> <p> TYPE: <code>Module</code> </p> <code>train_loader</code> <p>training loader</p> <p> TYPE: <code>DataLoader</code> </p> <code>val_loader</code> <p>validation loader</p> <p> TYPE: <code>DataLoader</code> </p> <code>criterion</code> <p>loss criterion</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>parameter optimizer</p> <p> TYPE: <code>Optimizer</code> </p> <code>gpu</code> <p>index of the GPU to use</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>output_file</code> <p>path where checkpoints will be saved</p> <p> TYPE: <code>str</code> DEFAULT: <code>'./checkpoint.pth'</code> </p> <code>amp</code> <p>whether to use automatic mixed precision</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    train_loader: DataLoader,\n    val_loader: DataLoader,\n    criterion: nn.Module,\n    optimizer: torch.optim.Optimizer,\n    gpu: int | None = None,\n    output_file: str = \"./checkpoint.pth\",\n    amp: bool = False,\n    skip_nan_loss: bool = False,\n    nan_tolerance: int = 5,\n    gradient_acc: int = 1,\n    gradient_clip: float | None = None,\n    on_epoch_end: Callable[[dict[str, float]], Any] | None = None,\n) -&gt; None:\n    self.model = model\n    self.train_loader = train_loader\n    self.val_loader = val_loader\n    self.criterion = criterion\n    self.optimizer = optimizer\n    self.amp = amp\n    self.scaler: GradScaler\n    self.on_epoch_end = on_epoch_end\n    self.skip_nan_loss = skip_nan_loss\n    self.nan_tolerance = nan_tolerance\n    self.gradient_acc = gradient_acc\n    self.grad_clip = gradient_clip\n\n    # Output file\n    self.output_file = output_file\n\n    # Initialize\n    self.step = 0\n    self.start_epoch = 0\n    self.epoch = 0\n    self._grad_count = 0\n    self.min_loss = math.inf\n    self.gpu = gpu\n    self._params: tuple[ParamSeq, ParamSeq] = ([], [])\n    self.lr_recorder: list[float] = []\n    self.loss_recorder: list[float] = []\n    self.set_device(gpu)\n    self._reset_opt(self.optimizer.defaults[\"lr\"])\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.BinaryClassificationTrainer.set_device","title":"set_device","text":"<pre><code>set_device(gpu: int | None = None) -&gt; None\n</code></pre> <p>Move tensor objects to the target GPU</p> PARAMETER DESCRIPTION <code>gpu</code> <p>index of the target GPU device</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>AssertionError</code> <p>if PyTorch cannot access the GPU</p> <code>ValueError</code> <p>if the device index is invalid</p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def set_device(self, gpu: int | None = None) -&gt; None:\n    \"\"\"Move tensor objects to the target GPU\n\n    Args:\n        gpu: index of the target GPU device\n\n    Raises:\n        AssertionError: if PyTorch cannot access the GPU\n        ValueError: if the device index is invalid\n    \"\"\"\n    if isinstance(gpu, int):\n        if not torch.cuda.is_available():\n            raise AssertionError(\"PyTorch cannot access your GPU. Please investigate!\")\n        if gpu &gt;= torch.cuda.device_count():\n            raise ValueError(\"Invalid device index\")\n        torch.cuda.set_device(gpu)\n        self.model = self.model.cuda()\n        if isinstance(self.criterion, torch.nn.Module):\n            self.criterion = self.criterion.cuda()\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.BinaryClassificationTrainer.save","title":"save","text":"<pre><code>save(output_file: str) -&gt; None\n</code></pre> <p>Save a trainer checkpoint</p> PARAMETER DESCRIPTION <code>output_file</code> <p>destination file path</p> <p> TYPE: <code>str</code> </p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def save(self, output_file: str) -&gt; None:\n    \"\"\"Save a trainer checkpoint\n\n    Args:\n        output_file: destination file path\n    \"\"\"\n    torch.save(\n        {\n            \"epoch\": self.epoch,\n            \"step\": self.step,\n            \"min_loss\": self.min_loss,\n            \"model\": self.model.state_dict(),\n        },\n        output_file,\n        _use_new_zipfile_serialization=False,\n    )\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.BinaryClassificationTrainer.load","title":"load","text":"<pre><code>load(state: dict[str, Any]) -&gt; None\n</code></pre> <p>Resume from a trainer state</p> PARAMETER DESCRIPTION <code>state</code> <p>checkpoint dictionary</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def load(self, state: dict[str, Any]) -&gt; None:\n    \"\"\"Resume from a trainer state\n\n    Args:\n        state: checkpoint dictionary\n    \"\"\"\n    self.start_epoch = state[\"epoch\"]\n    self.epoch = self.start_epoch\n    self.step = state[\"step\"]\n    self.min_loss = state[\"min_loss\"]\n    self.model.load_state_dict(state[\"model\"])\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.BinaryClassificationTrainer.to_cuda","title":"to_cuda","text":"<pre><code>to_cuda(x: Tensor, target: Tensor | list[dict[str, Tensor]]) -&gt; tuple[Tensor, Tensor | list[dict[str, Tensor]]]\n</code></pre> <p>Move input and target to GPU</p> PARAMETER DESCRIPTION <code>x</code> <p>input tensor</p> <p> TYPE: <code>Tensor</code> </p> <code>target</code> <p>target tensor or list of target dictionaries</p> <p> TYPE: <code>Tensor | list[dict[str, Tensor]]</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor, Tensor | list[dict[str, Tensor]]]</code> <p>tuple of input and target tensors</p> RAISES DESCRIPTION <code>ValueError</code> <p>if the device index is invalid</p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def to_cuda(\n    self, x: Tensor, target: Tensor | list[dict[str, Tensor]]\n) -&gt; tuple[Tensor, Tensor | list[dict[str, Tensor]]]:\n    \"\"\"Move input and target to GPU\n\n    Args:\n        x: input tensor\n        target: target tensor or list of target dictionaries\n\n    Returns:\n        tuple of input and target tensors\n\n    Raises:\n        ValueError: if the device index is invalid\n    \"\"\"\n    if isinstance(self.gpu, int):\n        if self.gpu &gt;= torch.cuda.device_count():\n            raise ValueError(\"Invalid device index\")\n        return self._to_cuda(x, target)  # type: ignore[arg-type]\n    return x, target\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.BinaryClassificationTrainer.fit_n_epochs","title":"fit_n_epochs","text":"<pre><code>fit_n_epochs(num_epochs: int, lr: float, freeze_until: str | None = None, sched_type: str = 'onecycle', norm_weight_decay: float | None = None, **kwargs: Any) -&gt; None\n</code></pre> <p>Train the model for a given number of epochs.</p> PARAMETER DESCRIPTION <code>num_epochs</code> <p>number of epochs to train</p> <p> TYPE: <code>int</code> </p> <code>lr</code> <p>learning rate to be used by the scheduler</p> <p> TYPE: <code>float</code> </p> <code>freeze_until</code> <p>last layer to freeze</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>sched_type</code> <p>type of scheduler to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>'onecycle'</code> </p> <code>norm_weight_decay</code> <p>weight decay to apply to normalization parameters</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>keyword args passed to the <code>LRScheduler</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def fit_n_epochs(\n    self,\n    num_epochs: int,\n    lr: float,\n    freeze_until: str | None = None,\n    sched_type: str = \"onecycle\",\n    norm_weight_decay: float | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Train the model for a given number of epochs.\n\n    Args:\n        num_epochs: number of epochs to train\n        lr: learning rate to be used by the scheduler\n        freeze_until: last layer to freeze\n        sched_type: type of scheduler to use\n        norm_weight_decay: weight decay to apply to normalization parameters\n        **kwargs: keyword args passed to the [`LRScheduler`][torch.optim.lr_scheduler.LRScheduler]\n    \"\"\"\n    freeze_model(self.model.train(), freeze_until)\n    # Update param groups &amp; LR\n    self._reset_opt(lr, norm_weight_decay)\n    # Scheduler\n    self._reset_scheduler(lr, num_epochs, sched_type, **kwargs)\n\n    if self.amp:\n        self.scaler = GradScaler(\"cuda\")\n\n    mb = master_bar(range(num_epochs))\n    for _ in mb:\n        self._fit_epoch(mb)\n        eval_metrics = self.evaluate()\n\n        # master bar\n        mb.main_bar.comment = f\"Epoch {self.epoch}/{self.start_epoch + num_epochs}\"\n        mb.write(f\"Epoch {self.epoch}/{self.start_epoch + num_epochs} - {self._eval_metrics_str(eval_metrics)}\")\n\n        if eval_metrics[\"val_loss\"] &lt; self.min_loss:\n            print(  # noqa: T201\n                f\"Validation loss decreased {self.min_loss:.4} --&gt; {eval_metrics['val_loss']:.4}: saving state...\"\n            )\n            self.min_loss = eval_metrics[\"val_loss\"]\n            self.save(self.output_file)\n\n        if self.on_epoch_end is not None:\n            self.on_epoch_end(eval_metrics)\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.BinaryClassificationTrainer.find_lr","title":"find_lr","text":"<pre><code>find_lr(freeze_until: str | None = None, start_lr: float = 1e-07, end_lr: float = 1, norm_weight_decay: float | None = None, num_it: int = 100) -&gt; None\n</code></pre> <p>Gridsearch the optimal learning rate for the training as described in \"Cyclical Learning Rates for Training Neural Networks\".</p> PARAMETER DESCRIPTION <code>freeze_until</code> <p>last layer to freeze</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>start_lr</code> <p>initial learning rate</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-07</code> </p> <code>end_lr</code> <p>final learning rate</p> <p> TYPE: <code>float</code> DEFAULT: <code>1</code> </p> <code>norm_weight_decay</code> <p>weight decay to apply to normalization parameters</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>num_it</code> <p>number of iterations to perform</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the number of iterations is greater than the number of available batches</p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def find_lr(\n    self,\n    freeze_until: str | None = None,\n    start_lr: float = 1e-7,\n    end_lr: float = 1,\n    norm_weight_decay: float | None = None,\n    num_it: int = 100,\n) -&gt; None:\n    \"\"\"Gridsearch the optimal learning rate for the training as described in\n    [\"Cyclical Learning Rates for Training Neural Networks\"](https://arxiv.org/pdf/1506.01186.pdf).\n\n    Args:\n       freeze_until: last layer to freeze\n       start_lr: initial learning rate\n       end_lr: final learning rate\n       norm_weight_decay: weight decay to apply to normalization parameters\n       num_it: number of iterations to perform\n\n    Raises:\n        ValueError: if the number of iterations is greater than the number of available batches\n    \"\"\"\n    if num_it &gt; len(self.train_loader):\n        raise ValueError(\"the value of `num_it` needs to be lower than the number of available batches\")\n\n    freeze_model(self.model.train(), freeze_until)\n    # Update param groups &amp; LR\n    self._reset_opt(start_lr, norm_weight_decay)\n    gamma = (end_lr / start_lr) ** (1 / (num_it - 1))\n    scheduler = MultiplicativeLR(self.optimizer, lambda step: gamma)\n\n    self.lr_recorder = [start_lr * gamma**idx for idx in range(num_it)]\n    self.loss_recorder = []\n\n    if self.amp:\n        self.scaler = GradScaler(\"cuda\")\n\n    for batch_idx, (x, target) in enumerate(self.train_loader):\n        x, target = self.to_cuda(x, target)\n\n        # Forward\n        batch_loss: Tensor = self._get_loss(x, target)  # type: ignore[assignment]\n        self._backprop_step(batch_loss)\n        # Update LR\n        scheduler.step()\n\n        # Record\n        if torch.isnan(batch_loss) or torch.isinf(batch_loss):\n            if batch_idx == 0:\n                raise ValueError(\"loss value is NaN or inf.\")\n            break\n        self.loss_recorder.append(batch_loss.item())\n        # Stop after the number of iterations\n        if batch_idx + 1 == num_it:\n            break\n\n    self.lr_recorder = self.lr_recorder[: len(self.loss_recorder)]\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.BinaryClassificationTrainer.plot_recorder","title":"plot_recorder","text":"<pre><code>plot_recorder(beta: float = 0.95, **kwargs: Any) -&gt; None\n</code></pre> <p>Display the results of the LR grid search</p> PARAMETER DESCRIPTION <code>beta</code> <p>smoothing factor</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.95</code> </p> <code>**kwargs</code> <p>keyword args of <code>matplotlib.pyplot.show</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>AssertionError</code> <p>if the number of learning rate recorder and loss recorder are not the same or if the number of learning rate recorder is 0</p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def plot_recorder(self, beta: float = 0.95, **kwargs: Any) -&gt; None:\n    \"\"\"Display the results of the LR grid search\n\n    Args:\n        beta: smoothing factor\n        **kwargs: keyword args of [`matplotlib.pyplot.show`][matplotlib.pyplot.show]\n\n    Raises:\n        AssertionError: if the number of learning rate recorder and loss recorder are not the same or if the number of learning rate recorder is 0\n    \"\"\"\n    if len(self.lr_recorder) != len(self.loss_recorder) or len(self.lr_recorder) == 0:\n        raise AssertionError(\"Please run the `lr_find` method first\")\n\n    # Exp moving average of loss\n    smoothed_losses = []\n    avg_loss = 0.0\n    for idx, loss in enumerate(self.loss_recorder):\n        avg_loss = beta * avg_loss + (1 - beta) * loss\n        smoothed_losses.append(avg_loss / (1 - beta ** (idx + 1)))\n\n    # Properly rescale Y-axis\n    data_slice = slice(\n        min(len(self.loss_recorder) // 10, 10),\n        -min(len(self.loss_recorder) // 20, 5) if len(self.loss_recorder) &gt;= 20 else len(self.loss_recorder),\n    )\n    vals: np.ndarray = np.array(smoothed_losses[data_slice])\n    min_idx = vals.argmin()\n    max_val = vals.max() if min_idx is None else vals[: min_idx + 1].max()\n    delta = max_val - vals[min_idx]\n\n    plt.plot(self.lr_recorder[data_slice], smoothed_losses[data_slice])\n    plt.xscale(\"log\")\n    plt.xlabel(\"Learning Rate\")\n    plt.ylabel(\"Training loss\")\n    plt.ylim(vals[min_idx] - 0.1 * delta, max_val + 0.2 * delta)\n    plt.grid(True, linestyle=\"--\", axis=\"x\")\n    plt.show(**kwargs)\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.BinaryClassificationTrainer.check_setup","title":"check_setup","text":"<pre><code>check_setup(freeze_until: str | None = None, lr: float = 0.0003, norm_weight_decay: float | None = None, num_it: int = 100, **kwargs: Any) -&gt; None\n</code></pre> <p>Check whether you can overfit one batch</p> PARAMETER DESCRIPTION <code>freeze_until</code> <p>last layer to freeze</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>lr</code> <p>learning rate to be used for training</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0003</code> </p> <code>norm_weight_decay</code> <p>weight decay to apply to normalization parameters</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>num_it</code> <p>number of iterations to perform</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>**kwargs</code> <p>keyword args of <code>matplotlib.pyplot.show</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the loss value is NaN or inf</p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def check_setup(\n    self,\n    freeze_until: str | None = None,\n    lr: float = 3e-4,\n    norm_weight_decay: float | None = None,\n    num_it: int = 100,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Check whether you can overfit one batch\n\n    Args:\n        freeze_until: last layer to freeze\n        lr: learning rate to be used for training\n        norm_weight_decay: weight decay to apply to normalization parameters\n        num_it: number of iterations to perform\n        **kwargs: keyword args of [`matplotlib.pyplot.show`][matplotlib.pyplot.show]\n\n    Raises:\n        ValueError: if the loss value is NaN or inf\n    \"\"\"\n    freeze_model(self.model.train(), freeze_until)\n    # Update param groups &amp; LR\n    self._reset_opt(lr, norm_weight_decay)\n\n    x, target = next(iter(self.train_loader))\n    x, target = self.to_cuda(x, target)\n\n    losses = []\n\n    if self.amp:\n        self.scaler = GradScaler(\"cuda\")\n\n    for _ in range(num_it):\n        # Forward\n        batch_loss: Tensor = self._get_loss(x, target)  # type: ignore[assignment]\n        # Backprop\n        self._backprop_step(batch_loss)\n\n        if torch.isnan(batch_loss) or torch.isinf(batch_loss):\n            raise ValueError(\"loss value is NaN or inf.\")\n\n        losses.append(batch_loss.item())\n\n    plt.plot(np.arange(len(losses)), losses)\n    plt.xlabel(\"Optimization steps\")\n    plt.ylabel(\"Training loss\")\n    plt.grid(True, linestyle=\"--\", axis=\"x\")\n    plt.show(**kwargs)\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.BinaryClassificationTrainer.plot_top_losses","title":"plot_top_losses","text":"<pre><code>plot_top_losses(mean: tuple[float, float, float], std: tuple[float, float, float], classes: Sequence[str] | None = None, num_samples: int = 12, **kwargs: Any) -&gt; None\n</code></pre> <p>Plot the top losses</p> PARAMETER DESCRIPTION <code>mean</code> <p>mean of the dataset</p> <p> TYPE: <code>tuple[float, float, float]</code> </p> <code>std</code> <p>standard deviation of the dataset</p> <p> TYPE: <code>tuple[float, float, float]</code> </p> <code>classes</code> <p>list of classes</p> <p> TYPE: <code>Sequence[str] | None</code> DEFAULT: <code>None</code> </p> <code>num_samples</code> <p>number of samples to plot</p> <p> TYPE: <code>int</code> DEFAULT: <code>12</code> </p> <code>**kwargs</code> <p>keyword args of <code>matplotlib.pyplot.show</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>AssertionError</code> <p>if the argument 'classes' is not specified for multi-class classification</p> Source code in <code>holocron/trainer/classification.py</code> <pre><code>@torch.inference_mode()\ndef plot_top_losses(\n    self,\n    mean: tuple[float, float, float],\n    std: tuple[float, float, float],\n    classes: Sequence[str] | None = None,\n    num_samples: int = 12,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Plot the top losses\n\n    Args:\n        mean: mean of the dataset\n        std: standard deviation of the dataset\n        classes: list of classes\n        num_samples: number of samples to plot\n        **kwargs: keyword args of [`matplotlib.pyplot.show`][matplotlib.pyplot.show]\n\n    Raises:\n        AssertionError: if the argument 'classes' is not specified for multi-class classification\n    \"\"\"\n    # Record loss, prob, target, image\n    losses = np.zeros(num_samples, dtype=np.float32)\n    preds = np.zeros(num_samples, dtype=int)\n    probs = np.zeros(num_samples, dtype=np.float32)\n    targets = np.zeros(num_samples, dtype=np.float32 if self.is_binary else int)\n    images = [None] * num_samples\n\n    # Switch to unreduced loss\n    reduction = self.criterion.reduction\n    self.criterion.reduction = \"none\"  # type: ignore[assignment]\n    self.model.eval()\n\n    train_iter = iter(self.train_loader)\n\n    for x, target in tqdm(train_iter):\n        x, target = self.to_cuda(x, target)\n\n        # Forward\n        batch_loss, logits = self._get_loss(x, target, return_logits=True)  # ty: ignore[invalid-argument-type]\n\n        # Binary\n        if self.is_binary:\n            batch_loss = batch_loss.squeeze(1)\n            probs_ = torch.sigmoid(logits.squeeze(1))\n        else:\n            probs_ = torch.softmax(logits, 1).max(dim=1).values\n\n        if torch.any(batch_loss &gt; losses.min()):\n            idcs = np.concatenate((losses, batch_loss.cpu().numpy())).argsort()[-num_samples:]\n            kept_idcs = [idx for idx in idcs if idx &lt; num_samples]\n            added_idcs = [idx - num_samples for idx in idcs if idx &gt;= num_samples]\n            # Update\n            losses = np.concatenate((losses[kept_idcs], batch_loss.cpu().numpy()[added_idcs]))\n            probs = np.concatenate((probs[kept_idcs], probs_.cpu().numpy()))\n            if not self.is_binary:\n                preds = np.concatenate((preds[kept_idcs], logits[added_idcs].argmax(dim=1).cpu().numpy()))\n            targets = np.concatenate((targets[kept_idcs], target[added_idcs].cpu().numpy()))  # ty: ignore[invalid-argument-type]\n            imgs = x[added_idcs].cpu() * torch.tensor(std).view(-1, 1, 1)\n            imgs += torch.tensor(mean).view(-1, 1, 1)\n            images = [images[idx] for idx in kept_idcs] + [to_pil_image(img) for img in imgs]\n\n    self.criterion.reduction = reduction\n\n    if not self.is_binary and classes is None:\n        raise AssertionError(\"arg 'classes' must be specified for multi-class classification\")\n\n    # Final sort\n    idcs_ = losses.argsort()[::-1]\n    losses, preds, probs, targets = losses[idcs_], preds[idcs_], probs[idcs_], targets[idcs_]\n    images = [images[idx] for idx in idcs_]\n\n    # Plot it\n    num_cols = 4\n    num_rows = math.ceil(num_samples / num_cols)\n    _, axes = plt.subplots(num_rows, num_cols, figsize=(20, 5))\n    for idx, (img, pred, prob, target, loss) in enumerate(zip(images, preds, probs, targets, losses, strict=True)):\n        row = int(idx / num_cols)\n        col = idx - num_cols * row\n        axes[row][col].imshow(img)\n        # Loss, prob, target\n        if self.is_binary:\n            axes[row][col].title.set_text(f\"{loss:.3} / {prob:.2} / {target:.2}\")\n        # Loss, pred (prob), target\n        else:\n            axes[row][col].title.set_text(\n                f\"{loss:.3} / {classes[pred]} ({prob:.1%}) / {classes[target]}\"  # type: ignore[index]\n            )\n        axes[row][col].axis(\"off\")\n\n    plt.show(**kwargs)\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.BinaryClassificationTrainer.evaluate","title":"evaluate","text":"<pre><code>evaluate() -&gt; dict[str, float]\n</code></pre> <p>Evaluate the model on the validation set</p> RETURNS DESCRIPTION <code>dict[str, float]</code> <p>evaluation metrics</p> Source code in <code>holocron/trainer/classification.py</code> <pre><code>@torch.inference_mode()\ndef evaluate(self) -&gt; dict[str, float]:\n    \"\"\"Evaluate the model on the validation set\n\n    Returns:\n        evaluation metrics\n    \"\"\"\n    self.model.eval()\n\n    val_loss, top1, num_samples, num_valid_batches = 0.0, 0.0, 0, 0\n    for x, target in self.val_loader:\n        x, target = self.to_cuda(x, target)\n\n        loss, out = self._get_loss(x, target, return_logits=True)  # ty: ignore[invalid-argument-type]\n\n        # Safeguard for NaN loss\n        if not torch.isnan(loss) and not torch.isinf(loss):\n            val_loss += loss.item()\n            num_valid_batches += 1\n\n        top1 += torch.sum((target.view_as(out) &gt;= 0.5) == (torch.sigmoid(out) &gt;= 0.5)).item() / out[0].numel()  # ty: ignore[possibly-missing-attribute]\n\n        num_samples += x.shape[0]\n\n    val_loss /= num_valid_batches\n\n    return {\"val_loss\": val_loss, \"acc\": top1 / num_samples}\n</code></pre>"},{"location":"reference/trainer/#semantic-segmentation","title":"Semantic segmentation","text":""},{"location":"reference/trainer/#holocron.trainer.SegmentationTrainer","title":"SegmentationTrainer","text":"<pre><code>SegmentationTrainer(*args: Any, num_classes: int = 10, **kwargs: Any)\n</code></pre> <p>               Bases: <code>Trainer</code></p> <p>Semantic segmentation trainer class.</p> PARAMETER DESCRIPTION <code>*args</code> <p>args of <code>Trainer</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>()</code> </p> <code>num_classes</code> <p>number of output classes</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>**kwargs</code> <p>keyword args of <code>Trainer</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>holocron/trainer/segmentation.py</code> <pre><code>def __init__(self, *args: Any, num_classes: int = 10, **kwargs: Any) -&gt; None:\n    super().__init__(*args, **kwargs)\n    self.num_classes = num_classes\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.SegmentationTrainer.set_device","title":"set_device","text":"<pre><code>set_device(gpu: int | None = None) -&gt; None\n</code></pre> <p>Move tensor objects to the target GPU</p> PARAMETER DESCRIPTION <code>gpu</code> <p>index of the target GPU device</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>AssertionError</code> <p>if PyTorch cannot access the GPU</p> <code>ValueError</code> <p>if the device index is invalid</p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def set_device(self, gpu: int | None = None) -&gt; None:\n    \"\"\"Move tensor objects to the target GPU\n\n    Args:\n        gpu: index of the target GPU device\n\n    Raises:\n        AssertionError: if PyTorch cannot access the GPU\n        ValueError: if the device index is invalid\n    \"\"\"\n    if isinstance(gpu, int):\n        if not torch.cuda.is_available():\n            raise AssertionError(\"PyTorch cannot access your GPU. Please investigate!\")\n        if gpu &gt;= torch.cuda.device_count():\n            raise ValueError(\"Invalid device index\")\n        torch.cuda.set_device(gpu)\n        self.model = self.model.cuda()\n        if isinstance(self.criterion, torch.nn.Module):\n            self.criterion = self.criterion.cuda()\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.SegmentationTrainer.save","title":"save","text":"<pre><code>save(output_file: str) -&gt; None\n</code></pre> <p>Save a trainer checkpoint</p> PARAMETER DESCRIPTION <code>output_file</code> <p>destination file path</p> <p> TYPE: <code>str</code> </p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def save(self, output_file: str) -&gt; None:\n    \"\"\"Save a trainer checkpoint\n\n    Args:\n        output_file: destination file path\n    \"\"\"\n    torch.save(\n        {\n            \"epoch\": self.epoch,\n            \"step\": self.step,\n            \"min_loss\": self.min_loss,\n            \"model\": self.model.state_dict(),\n        },\n        output_file,\n        _use_new_zipfile_serialization=False,\n    )\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.SegmentationTrainer.load","title":"load","text":"<pre><code>load(state: dict[str, Any]) -&gt; None\n</code></pre> <p>Resume from a trainer state</p> PARAMETER DESCRIPTION <code>state</code> <p>checkpoint dictionary</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def load(self, state: dict[str, Any]) -&gt; None:\n    \"\"\"Resume from a trainer state\n\n    Args:\n        state: checkpoint dictionary\n    \"\"\"\n    self.start_epoch = state[\"epoch\"]\n    self.epoch = self.start_epoch\n    self.step = state[\"step\"]\n    self.min_loss = state[\"min_loss\"]\n    self.model.load_state_dict(state[\"model\"])\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.SegmentationTrainer.to_cuda","title":"to_cuda","text":"<pre><code>to_cuda(x: Tensor, target: Tensor | list[dict[str, Tensor]]) -&gt; tuple[Tensor, Tensor | list[dict[str, Tensor]]]\n</code></pre> <p>Move input and target to GPU</p> PARAMETER DESCRIPTION <code>x</code> <p>input tensor</p> <p> TYPE: <code>Tensor</code> </p> <code>target</code> <p>target tensor or list of target dictionaries</p> <p> TYPE: <code>Tensor | list[dict[str, Tensor]]</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor, Tensor | list[dict[str, Tensor]]]</code> <p>tuple of input and target tensors</p> RAISES DESCRIPTION <code>ValueError</code> <p>if the device index is invalid</p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def to_cuda(\n    self, x: Tensor, target: Tensor | list[dict[str, Tensor]]\n) -&gt; tuple[Tensor, Tensor | list[dict[str, Tensor]]]:\n    \"\"\"Move input and target to GPU\n\n    Args:\n        x: input tensor\n        target: target tensor or list of target dictionaries\n\n    Returns:\n        tuple of input and target tensors\n\n    Raises:\n        ValueError: if the device index is invalid\n    \"\"\"\n    if isinstance(self.gpu, int):\n        if self.gpu &gt;= torch.cuda.device_count():\n            raise ValueError(\"Invalid device index\")\n        return self._to_cuda(x, target)  # type: ignore[arg-type]\n    return x, target\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.SegmentationTrainer.fit_n_epochs","title":"fit_n_epochs","text":"<pre><code>fit_n_epochs(num_epochs: int, lr: float, freeze_until: str | None = None, sched_type: str = 'onecycle', norm_weight_decay: float | None = None, **kwargs: Any) -&gt; None\n</code></pre> <p>Train the model for a given number of epochs.</p> PARAMETER DESCRIPTION <code>num_epochs</code> <p>number of epochs to train</p> <p> TYPE: <code>int</code> </p> <code>lr</code> <p>learning rate to be used by the scheduler</p> <p> TYPE: <code>float</code> </p> <code>freeze_until</code> <p>last layer to freeze</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>sched_type</code> <p>type of scheduler to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>'onecycle'</code> </p> <code>norm_weight_decay</code> <p>weight decay to apply to normalization parameters</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>keyword args passed to the <code>LRScheduler</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def fit_n_epochs(\n    self,\n    num_epochs: int,\n    lr: float,\n    freeze_until: str | None = None,\n    sched_type: str = \"onecycle\",\n    norm_weight_decay: float | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Train the model for a given number of epochs.\n\n    Args:\n        num_epochs: number of epochs to train\n        lr: learning rate to be used by the scheduler\n        freeze_until: last layer to freeze\n        sched_type: type of scheduler to use\n        norm_weight_decay: weight decay to apply to normalization parameters\n        **kwargs: keyword args passed to the [`LRScheduler`][torch.optim.lr_scheduler.LRScheduler]\n    \"\"\"\n    freeze_model(self.model.train(), freeze_until)\n    # Update param groups &amp; LR\n    self._reset_opt(lr, norm_weight_decay)\n    # Scheduler\n    self._reset_scheduler(lr, num_epochs, sched_type, **kwargs)\n\n    if self.amp:\n        self.scaler = GradScaler(\"cuda\")\n\n    mb = master_bar(range(num_epochs))\n    for _ in mb:\n        self._fit_epoch(mb)\n        eval_metrics = self.evaluate()\n\n        # master bar\n        mb.main_bar.comment = f\"Epoch {self.epoch}/{self.start_epoch + num_epochs}\"\n        mb.write(f\"Epoch {self.epoch}/{self.start_epoch + num_epochs} - {self._eval_metrics_str(eval_metrics)}\")\n\n        if eval_metrics[\"val_loss\"] &lt; self.min_loss:\n            print(  # noqa: T201\n                f\"Validation loss decreased {self.min_loss:.4} --&gt; {eval_metrics['val_loss']:.4}: saving state...\"\n            )\n            self.min_loss = eval_metrics[\"val_loss\"]\n            self.save(self.output_file)\n\n        if self.on_epoch_end is not None:\n            self.on_epoch_end(eval_metrics)\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.SegmentationTrainer.find_lr","title":"find_lr","text":"<pre><code>find_lr(freeze_until: str | None = None, start_lr: float = 1e-07, end_lr: float = 1, norm_weight_decay: float | None = None, num_it: int = 100) -&gt; None\n</code></pre> <p>Gridsearch the optimal learning rate for the training as described in \"Cyclical Learning Rates for Training Neural Networks\".</p> PARAMETER DESCRIPTION <code>freeze_until</code> <p>last layer to freeze</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>start_lr</code> <p>initial learning rate</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-07</code> </p> <code>end_lr</code> <p>final learning rate</p> <p> TYPE: <code>float</code> DEFAULT: <code>1</code> </p> <code>norm_weight_decay</code> <p>weight decay to apply to normalization parameters</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>num_it</code> <p>number of iterations to perform</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the number of iterations is greater than the number of available batches</p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def find_lr(\n    self,\n    freeze_until: str | None = None,\n    start_lr: float = 1e-7,\n    end_lr: float = 1,\n    norm_weight_decay: float | None = None,\n    num_it: int = 100,\n) -&gt; None:\n    \"\"\"Gridsearch the optimal learning rate for the training as described in\n    [\"Cyclical Learning Rates for Training Neural Networks\"](https://arxiv.org/pdf/1506.01186.pdf).\n\n    Args:\n       freeze_until: last layer to freeze\n       start_lr: initial learning rate\n       end_lr: final learning rate\n       norm_weight_decay: weight decay to apply to normalization parameters\n       num_it: number of iterations to perform\n\n    Raises:\n        ValueError: if the number of iterations is greater than the number of available batches\n    \"\"\"\n    if num_it &gt; len(self.train_loader):\n        raise ValueError(\"the value of `num_it` needs to be lower than the number of available batches\")\n\n    freeze_model(self.model.train(), freeze_until)\n    # Update param groups &amp; LR\n    self._reset_opt(start_lr, norm_weight_decay)\n    gamma = (end_lr / start_lr) ** (1 / (num_it - 1))\n    scheduler = MultiplicativeLR(self.optimizer, lambda step: gamma)\n\n    self.lr_recorder = [start_lr * gamma**idx for idx in range(num_it)]\n    self.loss_recorder = []\n\n    if self.amp:\n        self.scaler = GradScaler(\"cuda\")\n\n    for batch_idx, (x, target) in enumerate(self.train_loader):\n        x, target = self.to_cuda(x, target)\n\n        # Forward\n        batch_loss: Tensor = self._get_loss(x, target)  # type: ignore[assignment]\n        self._backprop_step(batch_loss)\n        # Update LR\n        scheduler.step()\n\n        # Record\n        if torch.isnan(batch_loss) or torch.isinf(batch_loss):\n            if batch_idx == 0:\n                raise ValueError(\"loss value is NaN or inf.\")\n            break\n        self.loss_recorder.append(batch_loss.item())\n        # Stop after the number of iterations\n        if batch_idx + 1 == num_it:\n            break\n\n    self.lr_recorder = self.lr_recorder[: len(self.loss_recorder)]\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.SegmentationTrainer.plot_recorder","title":"plot_recorder","text":"<pre><code>plot_recorder(beta: float = 0.95, **kwargs: Any) -&gt; None\n</code></pre> <p>Display the results of the LR grid search</p> PARAMETER DESCRIPTION <code>beta</code> <p>smoothing factor</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.95</code> </p> <code>**kwargs</code> <p>keyword args of <code>matplotlib.pyplot.show</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>AssertionError</code> <p>if the number of learning rate recorder and loss recorder are not the same or if the number of learning rate recorder is 0</p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def plot_recorder(self, beta: float = 0.95, **kwargs: Any) -&gt; None:\n    \"\"\"Display the results of the LR grid search\n\n    Args:\n        beta: smoothing factor\n        **kwargs: keyword args of [`matplotlib.pyplot.show`][matplotlib.pyplot.show]\n\n    Raises:\n        AssertionError: if the number of learning rate recorder and loss recorder are not the same or if the number of learning rate recorder is 0\n    \"\"\"\n    if len(self.lr_recorder) != len(self.loss_recorder) or len(self.lr_recorder) == 0:\n        raise AssertionError(\"Please run the `lr_find` method first\")\n\n    # Exp moving average of loss\n    smoothed_losses = []\n    avg_loss = 0.0\n    for idx, loss in enumerate(self.loss_recorder):\n        avg_loss = beta * avg_loss + (1 - beta) * loss\n        smoothed_losses.append(avg_loss / (1 - beta ** (idx + 1)))\n\n    # Properly rescale Y-axis\n    data_slice = slice(\n        min(len(self.loss_recorder) // 10, 10),\n        -min(len(self.loss_recorder) // 20, 5) if len(self.loss_recorder) &gt;= 20 else len(self.loss_recorder),\n    )\n    vals: np.ndarray = np.array(smoothed_losses[data_slice])\n    min_idx = vals.argmin()\n    max_val = vals.max() if min_idx is None else vals[: min_idx + 1].max()\n    delta = max_val - vals[min_idx]\n\n    plt.plot(self.lr_recorder[data_slice], smoothed_losses[data_slice])\n    plt.xscale(\"log\")\n    plt.xlabel(\"Learning Rate\")\n    plt.ylabel(\"Training loss\")\n    plt.ylim(vals[min_idx] - 0.1 * delta, max_val + 0.2 * delta)\n    plt.grid(True, linestyle=\"--\", axis=\"x\")\n    plt.show(**kwargs)\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.SegmentationTrainer.check_setup","title":"check_setup","text":"<pre><code>check_setup(freeze_until: str | None = None, lr: float = 0.0003, norm_weight_decay: float | None = None, num_it: int = 100, **kwargs: Any) -&gt; None\n</code></pre> <p>Check whether you can overfit one batch</p> PARAMETER DESCRIPTION <code>freeze_until</code> <p>last layer to freeze</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>lr</code> <p>learning rate to be used for training</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0003</code> </p> <code>norm_weight_decay</code> <p>weight decay to apply to normalization parameters</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>num_it</code> <p>number of iterations to perform</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>**kwargs</code> <p>keyword args of <code>matplotlib.pyplot.show</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the loss value is NaN or inf</p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def check_setup(\n    self,\n    freeze_until: str | None = None,\n    lr: float = 3e-4,\n    norm_weight_decay: float | None = None,\n    num_it: int = 100,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Check whether you can overfit one batch\n\n    Args:\n        freeze_until: last layer to freeze\n        lr: learning rate to be used for training\n        norm_weight_decay: weight decay to apply to normalization parameters\n        num_it: number of iterations to perform\n        **kwargs: keyword args of [`matplotlib.pyplot.show`][matplotlib.pyplot.show]\n\n    Raises:\n        ValueError: if the loss value is NaN or inf\n    \"\"\"\n    freeze_model(self.model.train(), freeze_until)\n    # Update param groups &amp; LR\n    self._reset_opt(lr, norm_weight_decay)\n\n    x, target = next(iter(self.train_loader))\n    x, target = self.to_cuda(x, target)\n\n    losses = []\n\n    if self.amp:\n        self.scaler = GradScaler(\"cuda\")\n\n    for _ in range(num_it):\n        # Forward\n        batch_loss: Tensor = self._get_loss(x, target)  # type: ignore[assignment]\n        # Backprop\n        self._backprop_step(batch_loss)\n\n        if torch.isnan(batch_loss) or torch.isinf(batch_loss):\n            raise ValueError(\"loss value is NaN or inf.\")\n\n        losses.append(batch_loss.item())\n\n    plt.plot(np.arange(len(losses)), losses)\n    plt.xlabel(\"Optimization steps\")\n    plt.ylabel(\"Training loss\")\n    plt.grid(True, linestyle=\"--\", axis=\"x\")\n    plt.show(**kwargs)\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.SegmentationTrainer.evaluate","title":"evaluate","text":"<pre><code>evaluate(ignore_index: int = 255) -&gt; dict[str, float]\n</code></pre> <p>Evaluate the model on the validation set</p> PARAMETER DESCRIPTION <code>ignore_index</code> <p>index of the class to ignore in evaluation</p> <p> TYPE: <code>int</code> DEFAULT: <code>255</code> </p> RETURNS DESCRIPTION <code>dict[str, float]</code> <p>evaluation metrics</p> Source code in <code>holocron/trainer/segmentation.py</code> <pre><code>@torch.inference_mode()\ndef evaluate(self, ignore_index: int = 255) -&gt; dict[str, float]:\n    \"\"\"Evaluate the model on the validation set\n\n    Args:\n        ignore_index: index of the class to ignore in evaluation\n\n    Returns:\n        evaluation metrics\n    \"\"\"\n    self.model.eval()\n\n    val_loss, mean_iou, num_valid_batches = 0.0, 0.0, 0\n    conf_mat = torch.zeros(\n        (self.num_classes, self.num_classes), dtype=torch.int64, device=next(self.model.parameters()).device\n    )\n    for x, target in self.val_loader:\n        x, target = self.to_cuda(x, target)\n\n        loss, out = self._get_loss(x, target, return_logits=True)  # ty: ignore[invalid-argument-type]\n\n        # Safeguard for NaN loss\n        if not torch.isnan(loss) and not torch.isinf(loss):\n            val_loss += loss.item()\n            num_valid_batches += 1\n\n        # borrowed from https://github.com/pytorch/vision/blob/master/references/segmentation/train.py\n        pred = out.argmax(dim=1).flatten()\n        target = target.flatten()  # ty: ignore[possibly-missing-attribute]\n        k = (target &gt;= 0) &amp; (target &lt; self.num_classes)\n        inds = self.num_classes * target[k].to(torch.int64) + pred[k]\n        nc = self.num_classes\n        conf_mat += torch.bincount(inds, minlength=nc**2).reshape(nc, nc)\n\n    val_loss /= num_valid_batches\n    acc_global = (torch.diag(conf_mat).sum() / conf_mat.sum()).item()\n    mean_iou = (torch.diag(conf_mat) / (conf_mat.sum(1) + conf_mat.sum(0) - torch.diag(conf_mat))).mean().item()\n\n    return {\"val_loss\": val_loss, \"acc_global\": acc_global, \"mean_iou\": mean_iou}\n</code></pre>"},{"location":"reference/trainer/#object-detection","title":"Object detection","text":""},{"location":"reference/trainer/#holocron.trainer.DetectionTrainer","title":"DetectionTrainer","text":"<pre><code>DetectionTrainer(model: Module, train_loader: DataLoader, val_loader: DataLoader, criterion: Module, optimizer: Optimizer, gpu: int | None = None, output_file: str = './checkpoint.pth', amp: bool = False, skip_nan_loss: bool = False, nan_tolerance: int = 5, gradient_acc: int = 1, gradient_clip: float | None = None, on_epoch_end: Callable[[dict[str, float]], Any] | None = None)\n</code></pre> <p>               Bases: <code>Trainer</code></p> <p>Object detection trainer class.</p> PARAMETER DESCRIPTION <code>model</code> <p>model to train</p> <p> TYPE: <code>Module</code> </p> <code>train_loader</code> <p>training loader</p> <p> TYPE: <code>DataLoader</code> </p> <code>val_loader</code> <p>validation loader</p> <p> TYPE: <code>DataLoader</code> </p> <code>criterion</code> <p>loss criterion</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>parameter optimizer</p> <p> TYPE: <code>Optimizer</code> </p> <code>gpu</code> <p>index of the GPU to use</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>output_file</code> <p>path where checkpoints will be saved</p> <p> TYPE: <code>str</code> DEFAULT: <code>'./checkpoint.pth'</code> </p> <code>amp</code> <p>whether to use automatic mixed precision</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>skip_nan_loss</code> <p>whether the optimizer step should be skipped when the loss is NaN</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>nan_tolerance</code> <p>number of consecutive batches with NaN loss before stopping the training</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>gradient_acc</code> <p>number of batches to accumulate the gradient of before performing the update step</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>gradient_clip</code> <p>the gradient clip value</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>on_epoch_end</code> <p>callback triggered at the end of an epoch</p> <p> TYPE: <code>Callable[[dict[str, float]], Any] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    train_loader: DataLoader,\n    val_loader: DataLoader,\n    criterion: nn.Module,\n    optimizer: torch.optim.Optimizer,\n    gpu: int | None = None,\n    output_file: str = \"./checkpoint.pth\",\n    amp: bool = False,\n    skip_nan_loss: bool = False,\n    nan_tolerance: int = 5,\n    gradient_acc: int = 1,\n    gradient_clip: float | None = None,\n    on_epoch_end: Callable[[dict[str, float]], Any] | None = None,\n) -&gt; None:\n    self.model = model\n    self.train_loader = train_loader\n    self.val_loader = val_loader\n    self.criterion = criterion\n    self.optimizer = optimizer\n    self.amp = amp\n    self.scaler: GradScaler\n    self.on_epoch_end = on_epoch_end\n    self.skip_nan_loss = skip_nan_loss\n    self.nan_tolerance = nan_tolerance\n    self.gradient_acc = gradient_acc\n    self.grad_clip = gradient_clip\n\n    # Output file\n    self.output_file = output_file\n\n    # Initialize\n    self.step = 0\n    self.start_epoch = 0\n    self.epoch = 0\n    self._grad_count = 0\n    self.min_loss = math.inf\n    self.gpu = gpu\n    self._params: tuple[ParamSeq, ParamSeq] = ([], [])\n    self.lr_recorder: list[float] = []\n    self.loss_recorder: list[float] = []\n    self.set_device(gpu)\n    self._reset_opt(self.optimizer.defaults[\"lr\"])\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.DetectionTrainer.set_device","title":"set_device","text":"<pre><code>set_device(gpu: int | None = None) -&gt; None\n</code></pre> <p>Move tensor objects to the target GPU</p> PARAMETER DESCRIPTION <code>gpu</code> <p>index of the target GPU device</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>AssertionError</code> <p>if PyTorch cannot access the GPU</p> <code>ValueError</code> <p>if the device index is invalid</p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def set_device(self, gpu: int | None = None) -&gt; None:\n    \"\"\"Move tensor objects to the target GPU\n\n    Args:\n        gpu: index of the target GPU device\n\n    Raises:\n        AssertionError: if PyTorch cannot access the GPU\n        ValueError: if the device index is invalid\n    \"\"\"\n    if isinstance(gpu, int):\n        if not torch.cuda.is_available():\n            raise AssertionError(\"PyTorch cannot access your GPU. Please investigate!\")\n        if gpu &gt;= torch.cuda.device_count():\n            raise ValueError(\"Invalid device index\")\n        torch.cuda.set_device(gpu)\n        self.model = self.model.cuda()\n        if isinstance(self.criterion, torch.nn.Module):\n            self.criterion = self.criterion.cuda()\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.DetectionTrainer.save","title":"save","text":"<pre><code>save(output_file: str) -&gt; None\n</code></pre> <p>Save a trainer checkpoint</p> PARAMETER DESCRIPTION <code>output_file</code> <p>destination file path</p> <p> TYPE: <code>str</code> </p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def save(self, output_file: str) -&gt; None:\n    \"\"\"Save a trainer checkpoint\n\n    Args:\n        output_file: destination file path\n    \"\"\"\n    torch.save(\n        {\n            \"epoch\": self.epoch,\n            \"step\": self.step,\n            \"min_loss\": self.min_loss,\n            \"model\": self.model.state_dict(),\n        },\n        output_file,\n        _use_new_zipfile_serialization=False,\n    )\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.DetectionTrainer.load","title":"load","text":"<pre><code>load(state: dict[str, Any]) -&gt; None\n</code></pre> <p>Resume from a trainer state</p> PARAMETER DESCRIPTION <code>state</code> <p>checkpoint dictionary</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def load(self, state: dict[str, Any]) -&gt; None:\n    \"\"\"Resume from a trainer state\n\n    Args:\n        state: checkpoint dictionary\n    \"\"\"\n    self.start_epoch = state[\"epoch\"]\n    self.epoch = self.start_epoch\n    self.step = state[\"step\"]\n    self.min_loss = state[\"min_loss\"]\n    self.model.load_state_dict(state[\"model\"])\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.DetectionTrainer.to_cuda","title":"to_cuda","text":"<pre><code>to_cuda(x: Tensor, target: Tensor | list[dict[str, Tensor]]) -&gt; tuple[Tensor, Tensor | list[dict[str, Tensor]]]\n</code></pre> <p>Move input and target to GPU</p> PARAMETER DESCRIPTION <code>x</code> <p>input tensor</p> <p> TYPE: <code>Tensor</code> </p> <code>target</code> <p>target tensor or list of target dictionaries</p> <p> TYPE: <code>Tensor | list[dict[str, Tensor]]</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor, Tensor | list[dict[str, Tensor]]]</code> <p>tuple of input and target tensors</p> RAISES DESCRIPTION <code>ValueError</code> <p>if the device index is invalid</p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def to_cuda(\n    self, x: Tensor, target: Tensor | list[dict[str, Tensor]]\n) -&gt; tuple[Tensor, Tensor | list[dict[str, Tensor]]]:\n    \"\"\"Move input and target to GPU\n\n    Args:\n        x: input tensor\n        target: target tensor or list of target dictionaries\n\n    Returns:\n        tuple of input and target tensors\n\n    Raises:\n        ValueError: if the device index is invalid\n    \"\"\"\n    if isinstance(self.gpu, int):\n        if self.gpu &gt;= torch.cuda.device_count():\n            raise ValueError(\"Invalid device index\")\n        return self._to_cuda(x, target)  # type: ignore[arg-type]\n    return x, target\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.DetectionTrainer.fit_n_epochs","title":"fit_n_epochs","text":"<pre><code>fit_n_epochs(num_epochs: int, lr: float, freeze_until: str | None = None, sched_type: str = 'onecycle', norm_weight_decay: float | None = None, **kwargs: Any) -&gt; None\n</code></pre> <p>Train the model for a given number of epochs.</p> PARAMETER DESCRIPTION <code>num_epochs</code> <p>number of epochs to train</p> <p> TYPE: <code>int</code> </p> <code>lr</code> <p>learning rate to be used by the scheduler</p> <p> TYPE: <code>float</code> </p> <code>freeze_until</code> <p>last layer to freeze</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>sched_type</code> <p>type of scheduler to use</p> <p> TYPE: <code>str</code> DEFAULT: <code>'onecycle'</code> </p> <code>norm_weight_decay</code> <p>weight decay to apply to normalization parameters</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>keyword args passed to the <code>LRScheduler</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def fit_n_epochs(\n    self,\n    num_epochs: int,\n    lr: float,\n    freeze_until: str | None = None,\n    sched_type: str = \"onecycle\",\n    norm_weight_decay: float | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Train the model for a given number of epochs.\n\n    Args:\n        num_epochs: number of epochs to train\n        lr: learning rate to be used by the scheduler\n        freeze_until: last layer to freeze\n        sched_type: type of scheduler to use\n        norm_weight_decay: weight decay to apply to normalization parameters\n        **kwargs: keyword args passed to the [`LRScheduler`][torch.optim.lr_scheduler.LRScheduler]\n    \"\"\"\n    freeze_model(self.model.train(), freeze_until)\n    # Update param groups &amp; LR\n    self._reset_opt(lr, norm_weight_decay)\n    # Scheduler\n    self._reset_scheduler(lr, num_epochs, sched_type, **kwargs)\n\n    if self.amp:\n        self.scaler = GradScaler(\"cuda\")\n\n    mb = master_bar(range(num_epochs))\n    for _ in mb:\n        self._fit_epoch(mb)\n        eval_metrics = self.evaluate()\n\n        # master bar\n        mb.main_bar.comment = f\"Epoch {self.epoch}/{self.start_epoch + num_epochs}\"\n        mb.write(f\"Epoch {self.epoch}/{self.start_epoch + num_epochs} - {self._eval_metrics_str(eval_metrics)}\")\n\n        if eval_metrics[\"val_loss\"] &lt; self.min_loss:\n            print(  # noqa: T201\n                f\"Validation loss decreased {self.min_loss:.4} --&gt; {eval_metrics['val_loss']:.4}: saving state...\"\n            )\n            self.min_loss = eval_metrics[\"val_loss\"]\n            self.save(self.output_file)\n\n        if self.on_epoch_end is not None:\n            self.on_epoch_end(eval_metrics)\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.DetectionTrainer.find_lr","title":"find_lr","text":"<pre><code>find_lr(freeze_until: str | None = None, start_lr: float = 1e-07, end_lr: float = 1, norm_weight_decay: float | None = None, num_it: int = 100) -&gt; None\n</code></pre> <p>Gridsearch the optimal learning rate for the training as described in \"Cyclical Learning Rates for Training Neural Networks\".</p> PARAMETER DESCRIPTION <code>freeze_until</code> <p>last layer to freeze</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>start_lr</code> <p>initial learning rate</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-07</code> </p> <code>end_lr</code> <p>final learning rate</p> <p> TYPE: <code>float</code> DEFAULT: <code>1</code> </p> <code>norm_weight_decay</code> <p>weight decay to apply to normalization parameters</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>num_it</code> <p>number of iterations to perform</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the number of iterations is greater than the number of available batches</p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def find_lr(\n    self,\n    freeze_until: str | None = None,\n    start_lr: float = 1e-7,\n    end_lr: float = 1,\n    norm_weight_decay: float | None = None,\n    num_it: int = 100,\n) -&gt; None:\n    \"\"\"Gridsearch the optimal learning rate for the training as described in\n    [\"Cyclical Learning Rates for Training Neural Networks\"](https://arxiv.org/pdf/1506.01186.pdf).\n\n    Args:\n       freeze_until: last layer to freeze\n       start_lr: initial learning rate\n       end_lr: final learning rate\n       norm_weight_decay: weight decay to apply to normalization parameters\n       num_it: number of iterations to perform\n\n    Raises:\n        ValueError: if the number of iterations is greater than the number of available batches\n    \"\"\"\n    if num_it &gt; len(self.train_loader):\n        raise ValueError(\"the value of `num_it` needs to be lower than the number of available batches\")\n\n    freeze_model(self.model.train(), freeze_until)\n    # Update param groups &amp; LR\n    self._reset_opt(start_lr, norm_weight_decay)\n    gamma = (end_lr / start_lr) ** (1 / (num_it - 1))\n    scheduler = MultiplicativeLR(self.optimizer, lambda step: gamma)\n\n    self.lr_recorder = [start_lr * gamma**idx for idx in range(num_it)]\n    self.loss_recorder = []\n\n    if self.amp:\n        self.scaler = GradScaler(\"cuda\")\n\n    for batch_idx, (x, target) in enumerate(self.train_loader):\n        x, target = self.to_cuda(x, target)\n\n        # Forward\n        batch_loss: Tensor = self._get_loss(x, target)  # type: ignore[assignment]\n        self._backprop_step(batch_loss)\n        # Update LR\n        scheduler.step()\n\n        # Record\n        if torch.isnan(batch_loss) or torch.isinf(batch_loss):\n            if batch_idx == 0:\n                raise ValueError(\"loss value is NaN or inf.\")\n            break\n        self.loss_recorder.append(batch_loss.item())\n        # Stop after the number of iterations\n        if batch_idx + 1 == num_it:\n            break\n\n    self.lr_recorder = self.lr_recorder[: len(self.loss_recorder)]\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.DetectionTrainer.plot_recorder","title":"plot_recorder","text":"<pre><code>plot_recorder(beta: float = 0.95, **kwargs: Any) -&gt; None\n</code></pre> <p>Display the results of the LR grid search</p> PARAMETER DESCRIPTION <code>beta</code> <p>smoothing factor</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.95</code> </p> <code>**kwargs</code> <p>keyword args of <code>matplotlib.pyplot.show</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>AssertionError</code> <p>if the number of learning rate recorder and loss recorder are not the same or if the number of learning rate recorder is 0</p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def plot_recorder(self, beta: float = 0.95, **kwargs: Any) -&gt; None:\n    \"\"\"Display the results of the LR grid search\n\n    Args:\n        beta: smoothing factor\n        **kwargs: keyword args of [`matplotlib.pyplot.show`][matplotlib.pyplot.show]\n\n    Raises:\n        AssertionError: if the number of learning rate recorder and loss recorder are not the same or if the number of learning rate recorder is 0\n    \"\"\"\n    if len(self.lr_recorder) != len(self.loss_recorder) or len(self.lr_recorder) == 0:\n        raise AssertionError(\"Please run the `lr_find` method first\")\n\n    # Exp moving average of loss\n    smoothed_losses = []\n    avg_loss = 0.0\n    for idx, loss in enumerate(self.loss_recorder):\n        avg_loss = beta * avg_loss + (1 - beta) * loss\n        smoothed_losses.append(avg_loss / (1 - beta ** (idx + 1)))\n\n    # Properly rescale Y-axis\n    data_slice = slice(\n        min(len(self.loss_recorder) // 10, 10),\n        -min(len(self.loss_recorder) // 20, 5) if len(self.loss_recorder) &gt;= 20 else len(self.loss_recorder),\n    )\n    vals: np.ndarray = np.array(smoothed_losses[data_slice])\n    min_idx = vals.argmin()\n    max_val = vals.max() if min_idx is None else vals[: min_idx + 1].max()\n    delta = max_val - vals[min_idx]\n\n    plt.plot(self.lr_recorder[data_slice], smoothed_losses[data_slice])\n    plt.xscale(\"log\")\n    plt.xlabel(\"Learning Rate\")\n    plt.ylabel(\"Training loss\")\n    plt.ylim(vals[min_idx] - 0.1 * delta, max_val + 0.2 * delta)\n    plt.grid(True, linestyle=\"--\", axis=\"x\")\n    plt.show(**kwargs)\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.DetectionTrainer.check_setup","title":"check_setup","text":"<pre><code>check_setup(freeze_until: str | None = None, lr: float = 0.0003, norm_weight_decay: float | None = None, num_it: int = 100, **kwargs: Any) -&gt; None\n</code></pre> <p>Check whether you can overfit one batch</p> PARAMETER DESCRIPTION <code>freeze_until</code> <p>last layer to freeze</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>lr</code> <p>learning rate to be used for training</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0003</code> </p> <code>norm_weight_decay</code> <p>weight decay to apply to normalization parameters</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>num_it</code> <p>number of iterations to perform</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>**kwargs</code> <p>keyword args of <code>matplotlib.pyplot.show</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the loss value is NaN or inf</p> Source code in <code>holocron/trainer/core.py</code> <pre><code>def check_setup(\n    self,\n    freeze_until: str | None = None,\n    lr: float = 3e-4,\n    norm_weight_decay: float | None = None,\n    num_it: int = 100,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Check whether you can overfit one batch\n\n    Args:\n        freeze_until: last layer to freeze\n        lr: learning rate to be used for training\n        norm_weight_decay: weight decay to apply to normalization parameters\n        num_it: number of iterations to perform\n        **kwargs: keyword args of [`matplotlib.pyplot.show`][matplotlib.pyplot.show]\n\n    Raises:\n        ValueError: if the loss value is NaN or inf\n    \"\"\"\n    freeze_model(self.model.train(), freeze_until)\n    # Update param groups &amp; LR\n    self._reset_opt(lr, norm_weight_decay)\n\n    x, target = next(iter(self.train_loader))\n    x, target = self.to_cuda(x, target)\n\n    losses = []\n\n    if self.amp:\n        self.scaler = GradScaler(\"cuda\")\n\n    for _ in range(num_it):\n        # Forward\n        batch_loss: Tensor = self._get_loss(x, target)  # type: ignore[assignment]\n        # Backprop\n        self._backprop_step(batch_loss)\n\n        if torch.isnan(batch_loss) or torch.isinf(batch_loss):\n            raise ValueError(\"loss value is NaN or inf.\")\n\n        losses.append(batch_loss.item())\n\n    plt.plot(np.arange(len(losses)), losses)\n    plt.xlabel(\"Optimization steps\")\n    plt.ylabel(\"Training loss\")\n    plt.grid(True, linestyle=\"--\", axis=\"x\")\n    plt.show(**kwargs)\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.DetectionTrainer.evaluate","title":"evaluate","text":"<pre><code>evaluate(iou_threshold: float = 0.5) -&gt; dict[str, float | None]\n</code></pre> <p>Evaluate the model on the validation set.</p> PARAMETER DESCRIPTION <code>iou_threshold</code> <p>IoU threshold for pair assignment</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> RETURNS DESCRIPTION <code>dict[str, float | None]</code> <p>evaluation metrics</p> Source code in <code>holocron/trainer/detection.py</code> <pre><code>@torch.inference_mode()\ndef evaluate(self, iou_threshold: float = 0.5) -&gt; dict[str, float | None]:\n    \"\"\"Evaluate the model on the validation set.\n\n    Args:\n        iou_threshold: IoU threshold for pair assignment\n\n    Returns:\n        evaluation metrics\n    \"\"\"\n    self.model.eval()\n\n    loc_assigns = 0\n    correct, clf_error, loc_fn, loc_fp, num_samples = 0, 0, 0, 0, 0\n\n    for x, target in self.val_loader:\n        x, target = self.to_cuda(x, target)\n\n        if self.amp:\n            with torch.amp.autocast(\"cuda\"):\n                detections = self.model(x)\n        else:\n            detections = self.model(x)\n\n        for dets, t in zip(detections, target, strict=True):\n            if t[\"boxes\"].shape[0] &gt; 0 and dets[\"boxes\"].shape[0] &gt; 0:\n                gt_indices, pred_indices = assign_iou(t[\"boxes\"], dets[\"boxes\"], iou_threshold)\n                loc_assigns += len(gt_indices)\n                correct_ = (t[\"labels\"][gt_indices] == dets[\"labels\"][pred_indices]).sum().item()\n            else:\n                gt_indices, pred_indices = [], []\n                correct_ = 0\n            correct += correct_\n            clf_error += len(gt_indices) - correct_\n            loc_fn += t[\"boxes\"].shape[0] - len(gt_indices)\n            loc_fp += dets[\"boxes\"].shape[0] - len(pred_indices)\n        num_samples += sum(t[\"boxes\"].shape[0] for t in target)\n\n    nb_preds = num_samples - loc_fn + loc_fp\n    # Localization\n    loc_err = 1 - 2 * loc_assigns / (nb_preds + num_samples) if nb_preds + num_samples &gt; 0 else None\n    # Classification\n    clf_err = 1 - correct / loc_assigns if loc_assigns &gt; 0 else None\n    # End-to-end\n    det_err = 1 - 2 * correct / (nb_preds + num_samples) if nb_preds + num_samples &gt; 0 else None\n    return {\"loc_err\": loc_err, \"clf_err\": clf_err, \"det_err\": det_err, \"val_loss\": loc_err}\n</code></pre>"},{"location":"reference/trainer/#miscellaneous","title":"Miscellaneous","text":""},{"location":"reference/trainer/#holocron.trainer.freeze_bn","title":"freeze_bn","text":"<pre><code>freeze_bn(mod: Module) -&gt; None\n</code></pre> <p>Prevents parameter and stats from updating in Batchnorm layers that are frozen</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from holocron.models import rexnet1_0x\n&gt;&gt;&gt; from holocron.trainer.utils import freeze_bn\n&gt;&gt;&gt; model = rexnet1_0x()\n&gt;&gt;&gt; freeze_bn(model)\n</code></pre> PARAMETER DESCRIPTION <code>mod</code> <p>model to train</p> <p> TYPE: <code>Module</code> </p> Source code in <code>holocron/trainer/utils.py</code> <pre><code>def freeze_bn(mod: nn.Module) -&gt; None:\n    \"\"\"Prevents parameter and stats from updating in Batchnorm layers that are frozen\n\n    Examples:\n        &gt;&gt;&gt; from holocron.models import rexnet1_0x\n        &gt;&gt;&gt; from holocron.trainer.utils import freeze_bn\n        &gt;&gt;&gt; model = rexnet1_0x()\n        &gt;&gt;&gt; freeze_bn(model)\n\n    Args:\n        mod: model to train\n    \"\"\"\n    # Loop on modules\n    for m in mod.modules():\n        if isinstance(m, _BatchNorm) and m.affine and all(not p.requires_grad for p in m.parameters()):\n            # Switch back to commented code when https://github.com/pytorch/pytorch/issues/37823 is resolved\n            m.track_running_stats = False  # ty: ignore[unresolved-attribute]\n            m.eval()\n</code></pre>"},{"location":"reference/trainer/#holocron.trainer.freeze_model","title":"freeze_model","text":"<pre><code>freeze_model(model: Module, last_frozen_layer: str | None = None, frozen_bn_stat_update: bool = False) -&gt; None\n</code></pre> <p>Freeze a specific range of model layers.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from holocron.models import rexnet1_0x\n&gt;&gt;&gt; from holocron.trainer.utils import freeze_model\n&gt;&gt;&gt; model = rexnet1_0x()\n&gt;&gt;&gt; freeze_model(model)\n</code></pre> PARAMETER DESCRIPTION <code>model</code> <p>model to train</p> <p> TYPE: <code>Module</code> </p> <code>last_frozen_layer</code> <p>last layer to freeze. Assumes layers have been registered in forward order</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>frozen_bn_stat_update</code> <p>force stats update in BN layers that are frozen</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the last frozen layer is not found</p> Source code in <code>holocron/trainer/utils.py</code> <pre><code>def freeze_model(\n    model: nn.Module,\n    last_frozen_layer: str | None = None,\n    frozen_bn_stat_update: bool = False,\n) -&gt; None:\n    \"\"\"Freeze a specific range of model layers.\n\n    Examples:\n        &gt;&gt;&gt; from holocron.models import rexnet1_0x\n        &gt;&gt;&gt; from holocron.trainer.utils import freeze_model\n        &gt;&gt;&gt; model = rexnet1_0x()\n        &gt;&gt;&gt; freeze_model(model)\n\n    Args:\n        model: model to train\n        last_frozen_layer: last layer to freeze. Assumes layers have been registered in forward order\n        frozen_bn_stat_update: force stats update in BN layers that are frozen\n\n    Raises:\n        ValueError: if the last frozen layer is not found\n    \"\"\"\n    # Unfreeze everything\n    for p in model.parameters():\n        p.requires_grad_(True)\n\n    # Loop on parameters\n    if isinstance(last_frozen_layer, str):\n        layer_reached = False\n        for n, p in model.named_parameters():\n            if not layer_reached or n.startswith(last_frozen_layer):\n                p.requires_grad_(False)\n            if n.startswith(last_frozen_layer):\n                layer_reached = True\n            # Once the last param of the layer is frozen, we break\n            elif layer_reached:\n                break\n        if not layer_reached:\n            raise ValueError(f\"Unable to locate child module {last_frozen_layer}\")\n\n    # Loop on modules\n    if not frozen_bn_stat_update:\n        freeze_bn(model)\n</code></pre>"},{"location":"reference/transforms/","title":"holocron.transforms","text":"<p><code>holocron.transforms</code> provides PIL and PyTorch tensor transformations.</p>"},{"location":"reference/transforms/#holocron.transforms.Resize","title":"Resize","text":"<pre><code>Resize(size: tuple[int, int], mode: ResizeMethod = SQUISH, pad_mode: str = 'constant', **kwargs: Any)\n</code></pre> <p>               Bases: <code>Resize</code></p> <p>Implements a more flexible resizing scheme.</p> <p></p> <p>from holocron.transforms import Resize pil_img = ... tf = Resize((224, 224), mode=\"pad\") resized_img = tf(pil_img)</p> PARAMETER DESCRIPTION <code>size</code> <p>the desired height and width of the image in pixels</p> <p> TYPE: <code>tuple[int, int]</code> </p> <code>mode</code> <p>the resizing scheme (\"squish\" is similar to PyTorch, \"pad\" will preserve the aspect ratio and pad)</p> <p> TYPE: <code>ResizeMethod</code> DEFAULT: <code>SQUISH</code> </p> <code>pad_mode</code> <p>padding mode when <code>mode</code> is \"pad\"</p> <p> TYPE: <code>str</code> DEFAULT: <code>'constant'</code> </p> <code>kwargs</code> <p>the keyword arguments of <code>torchvision.transforms.v2.Resize</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>holocron/transforms/interpolation.py</code> <pre><code>def __init__(\n    self,\n    size: tuple[int, int],\n    mode: ResizeMethod = ResizeMethod.SQUISH,\n    pad_mode: str = \"constant\",\n    **kwargs: Any,\n) -&gt; None:\n    if not isinstance(mode, ResizeMethod):\n        raise TypeError(\"mode is expected to be a ResizeMethod\")\n    if not isinstance(size, (tuple, list)) or len(size) != 2 or any(s &lt;= 0 for s in size):\n        raise ValueError(\"size is expected to be a sequence of 2 positive integers\")\n    super().__init__(size, **kwargs)\n    self.mode: ResizeMethod = mode\n    self.pad_mode: str = pad_mode\n    self.size: tuple[int, int]\n</code></pre>"},{"location":"reference/transforms/#holocron.transforms.RandomZoomOut","title":"RandomZoomOut","text":"<pre><code>RandomZoomOut(size: tuple[int, int], scale: tuple[float, float] = (0.5, 1.0), **kwargs: Any)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Implements a size reduction of the orignal image to provide a zoom out effect.</p> <p></p> <p>from holocron.transforms import RandomZoomOut pil_img = ... tf = RandomZoomOut((224, 224), scale=(0.3, 1.)) resized_img = tf(pil_img)</p> PARAMETER DESCRIPTION <code>size</code> <p>the desired height and width of the image in pixels</p> <p> TYPE: <code>tuple[int, int]</code> </p> <code>scale</code> <p>the range of relative area of the projected image to the desired size</p> <p> TYPE: <code>tuple[float, float]</code> DEFAULT: <code>(0.5, 1.0)</code> </p> <code>kwargs</code> <p>the keyword arguments of <code>torchvision.transforms.functional.resize</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>holocron/transforms/interpolation.py</code> <pre><code>def __init__(self, size: tuple[int, int], scale: tuple[float, float] = (0.5, 1.0), **kwargs: Any) -&gt; None:\n    if not isinstance(size, (tuple, list)) or len(size) != 2 or any(s &lt;= 0 for s in size):\n        raise ValueError(\"size is expected to be a sequence of 2 positive integers\")\n    if len(scale) != 2 or scale[0] &gt; scale[1]:\n        raise ValueError(\"scale is expected to be a couple of floats, the first one being small than the second\")\n    super().__init__()\n    self.size: tuple[int, int] = size\n    self.scale: tuple[float, float] = scale\n    self._kwargs: dict[str, Any] = kwargs\n</code></pre>"},{"location":"reference/utils.data/","title":"holocron.utils.data","text":""},{"location":"reference/utils.data/#batch-collate","title":"Batch collate","text":""},{"location":"reference/utils.data/#holocron.utils.data.Mixup","title":"Mixup","text":"<pre><code>Mixup(num_classes: int, alpha: float = 0.2)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Implements a batch collate function with MixUp strategy from \"mixup: Beyond Empirical Risk Minimization\".</p> <p>import torch from torch.utils.data._utils.collate import default_collate from holocron.utils.data import Mixup mix = Mixup(num_classes=10, alpha=0.4) loader = torch.utils.data.DataLoader(dataset, batch_size, collate_fn=lambda b: mix(*default_collate(b)))</p> PARAMETER DESCRIPTION <code>num_classes</code> <p>number of expected classes</p> <p> TYPE: <code>int</code> </p> <code>alpha</code> <p>mixup factor</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.2</code> </p> Source code in <code>holocron/utils/data/collate.py</code> <pre><code>def __init__(self, num_classes: int, alpha: float = 0.2) -&gt; None:\n    super().__init__()\n    self.num_classes: int = num_classes\n    if alpha &lt; 0:\n        raise ValueError(\"`alpha` only takes positive values\")\n    self.alpha: float = alpha\n</code></pre>"},{"location":"reference/utils/","title":"holocron.utils","text":"<p><code>holocron.utils</code> provides some utilities for general usage.</p>"},{"location":"reference/utils/#miscellaneous","title":"Miscellaneous","text":""},{"location":"reference/utils/#holocron.utils.find_image_size","title":"find_image_size","text":"<pre><code>find_image_size(dataset: Sequence[tuple[Image, Any]], **kwargs: Any) -&gt; None\n</code></pre> <p>Computes the best image size target for a given set of images</p> PARAMETER DESCRIPTION <code>dataset</code> <p>an iterator yielding a <code>PIL.Image.Image</code> and a target object</p> <p> TYPE: <code>Sequence[tuple[Image, Any]]</code> </p> <code>kwargs</code> <p>keyword args of <code>matplotlib.pyplot.show</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>holocron/utils/misc.py</code> <pre><code>def find_image_size(dataset: Sequence[tuple[Image.Image, Any]], **kwargs: Any) -&gt; None:\n    \"\"\"Computes the best image size target for a given set of images\n\n    Args:\n        dataset: an iterator yielding a [`PIL.Image.Image`][PIL.Image.Image] and a target object\n        kwargs: keyword args of [`matplotlib.pyplot.show`][matplotlib.pyplot.show]\n    \"\"\"\n    # Record height &amp; width\n    shapes_ = parallel(lambda x: x[0].size, dataset, progress=True)\n\n    shapes = np.asarray(shapes_)[:, ::-1]\n    ratios = shapes[:, 0] / shapes[:, 1]\n    sides = np.sqrt(shapes[:, 0] * shapes[:, 1])\n\n    # Compute median aspect ratio &amp; side\n    median_ratio = np.median(ratios)\n    median_side = np.median(sides)\n\n    height = round(median_side * sqrt(median_ratio))\n    width = round(median_side / sqrt(median_ratio))\n\n    # Double histogram\n    fig, axes = plt.subplots(1, 2)\n    axes[0].hist(ratios, bins=30, alpha=0.7)\n    axes[0].title.set_text(f\"Aspect ratio (median: {median_ratio:.2})\")\n    axes[0].grid(True, linestyle=\"--\", axis=\"x\")\n    axes[0].axvline(median_ratio, color=\"r\")\n    axes[1].hist(sides, bins=30, alpha=0.7)\n    axes[1].title.set_text(f\"Side (median: {int(median_side)})\")\n    axes[1].grid(True, linestyle=\"--\", axis=\"x\")\n    axes[1].axvline(median_side, color=\"r\")\n    fig.suptitle(f\"Median image size: ({height}, {width})\")\n    plt.show(**kwargs)\n</code></pre>"},{"location":"reference/models/models/","title":"holocron.models","text":"<p>The models subpackage contains definitions of models for addressing different tasks, including: image classification, pixelwise semantic segmentation, object detection, instance segmentation, person keypoint detection and video classification.</p>"},{"location":"reference/models/models/#classification","title":"Classification","text":"<p>Classification models expect a 4D image tensor as an input (N x C x H x W) and returns a 2D output (N x K). The output represents the classification scores for each output classes.</p>"},{"location":"reference/models/models/#supported-architectures","title":"Supported architectures","text":"<ul> <li>ResNet</li> <li>ResNeXt</li> <li>Res2Net</li> <li>TridentNet</li> <li>ConvNeXt</li> <li>PyConvResNet</li> <li>ReXNet</li> <li>SKNet</li> <li>DarkNet</li> <li>DarkNetV2</li> <li>DarkNetV3</li> <li>DarkNetV4</li> <li>RepVGG</li> <li>MobileOne</li> </ul>"},{"location":"reference/models/models/#available-checkpoints","title":"Available checkpoints","text":"<p>Here is the list of available checkpoints:</p> Checkpoint Acc@1 Acc@5 Params Size (MB) <code>CSPDarknet53_Checkpoint.IMAGENETTE</code> 94.50% 99.64% 26.6M 101.8 <code>CSPDarknet53_Mish_Checkpoint.IMAGENETTE</code> 94.65% 99.69% 26.6M 101.8 <code>ConvNeXt_Atto_Checkpoint.IMAGENETTE</code> 87.59% 98.32% 3.4M 12.9 <code>Darknet19_Checkpoint.IMAGENETTE</code> 93.86% 99.36% 19.8M 75.7 <code>Darknet53_Checkpoint.IMAGENETTE</code> 94.17% 99.57% 40.6M 155.1 <code>MobileOne_S0_Checkpoint.IMAGENETTE</code> 88.08% 98.83% 4.3M 16.9 <code>MobileOne_S1_Checkpoint.IMAGENETTE</code> 91.26% 99.18% 3.6M 13.9 <code>MobileOne_S2_Checkpoint.IMAGENETTE</code> 91.31% 99.21% 5.9M 22.8 <code>MobileOne_S3_Checkpoint.IMAGENETTE</code> 91.06% 99.31% 8.1M 31.5 <code>ReXNet1_0x_Checkpoint.IMAGENET1K</code> 77.86% 93.87% 4.8M 13.7 <code>ReXNet1_0x_Checkpoint.IMAGENETTE</code> 94.39% 99.62% 3.5M 13.7 <code>ReXNet1_3x_Checkpoint.IMAGENET1K</code> 79.50% 94.68% 7.6M 13.7 <code>ReXNet1_3x_Checkpoint.IMAGENETTE</code> 94.88% 99.39% 5.9M 22.8 <code>ReXNet1_5x_Checkpoint.IMAGENET1K</code> 80.31% 95.17% 9.7M 13.7 <code>ReXNet1_5x_Checkpoint.IMAGENETTE</code> 94.47% 99.62% 7.8M 30.2 <code>ReXNet2_0x_Checkpoint.IMAGENET1K</code> 80.31% 95.17% 16.4M 13.7 <code>ReXNet2_0x_Checkpoint.IMAGENETTE</code> 95.24% 99.57% 13.8M 53.1 <code>ReXNet2_2x_Checkpoint.IMAGENETTE</code> 95.44% 99.46% 16.7M 64.1 <code>RepVGG_A0_Checkpoint.IMAGENETTE</code> 92.92% 99.46% 24.7M 94.6 <code>RepVGG_A1_Checkpoint.IMAGENETTE</code> 93.78% 99.18% 30.1M 115.1 <code>RepVGG_A2_Checkpoint.IMAGENETTE</code> 93.63% 99.39% 48.6M 185.8 <code>RepVGG_B0_Checkpoint.IMAGENETTE</code> 92.69% 99.21% 31.8M 121.8 <code>RepVGG_B1_Checkpoint.IMAGENETTE</code> 93.96% 99.39% 100.8M 385.1 <code>RepVGG_B2_Checkpoint.IMAGENETTE</code> 94.14% 99.57% 157.5M 601.2 <code>Res2Net50_26w_4s_Checkpoint.IMAGENETTE</code> 93.94% 99.41% 23.7M 90.6 <code>ResNeXt50_32x4d_Checkpoint.IMAGENETTE</code> 94.55% 99.49% 23.0M 88.1 <code>ResNet18_Checkpoint.IMAGENETTE</code> 93.61% 99.46% 11.2M 42.7 <code>ResNet34_Checkpoint.IMAGENETTE</code> 93.81% 99.49% 21.3M 81.3 <code>ResNet50D_Checkpoint.IMAGENETTE</code> 94.65% 99.52% 23.5M 90.1 <code>ResNet50_Checkpoint.IMAGENETTE</code> 93.78% 99.54% 23.5M 90 <code>SKNet50_Checkpoint.IMAGENETTE</code> 94.37% 99.54% 35.2M 134.7"},{"location":"reference/models/models/#object-detection","title":"Object Detection","text":"<p>Object detection models expect a 4D image tensor as an input (N x C x H x W) and returns a list of dictionaries. Each dictionary has 3 keys: box coordinates, classification probability, classification label.</p> <pre><code>import holocron.models as models\nyolov2 = models.yolov2(num_classes=10)\n</code></pre>"},{"location":"reference/models/models/#yolo","title":"YOLO","text":""},{"location":"reference/models/models/#holocron.models.detection.YOLOv1","title":"YOLOv1","text":"<pre><code>YOLOv1(layout: list[list[int]], num_classes: int = 20, in_channels: int = 3, stem_channels: int = 64, num_anchors: int = 2, lambda_obj: float = 1, lambda_noobj: float = 0.5, lambda_class: float = 1, lambda_coords: float = 5.0, rpn_nms_thresh: float = 0.7, box_score_thresh: float = 0.05, head_hidden_nodes: int = 512, act_layer: Module | None = None, norm_layer: Callable[[int], Module] | None = None, drop_layer: Callable[..., Module] | None = None, conv_layer: Callable[..., Module] | None = None, backbone_norm_layer: Callable[[int], Module] | None = None)\n</code></pre> <p>               Bases: <code>_YOLO</code></p> Source code in <code>holocron/models/detection/yolo.py</code> <pre><code>def __init__(\n    self,\n    layout: list[list[int]],\n    num_classes: int = 20,\n    in_channels: int = 3,\n    stem_channels: int = 64,\n    num_anchors: int = 2,\n    lambda_obj: float = 1,\n    lambda_noobj: float = 0.5,\n    lambda_class: float = 1,\n    lambda_coords: float = 5.0,\n    rpn_nms_thresh: float = 0.7,\n    box_score_thresh: float = 0.05,\n    head_hidden_nodes: int = 512,  # In the original paper, 4096\n    act_layer: nn.Module | None = None,\n    norm_layer: Callable[[int], nn.Module] | None = None,\n    drop_layer: Callable[..., nn.Module] | None = None,\n    conv_layer: Callable[..., nn.Module] | None = None,\n    backbone_norm_layer: Callable[[int], nn.Module] | None = None,\n) -&gt; None:\n    super().__init__(\n        num_classes, rpn_nms_thresh, box_score_thresh, lambda_obj, lambda_noobj, lambda_class, lambda_coords\n    )\n\n    if act_layer is None:\n        act_layer = nn.LeakyReLU(0.1, inplace=True)\n\n    if backbone_norm_layer is None and norm_layer is not None:\n        backbone_norm_layer = norm_layer\n\n    self.backbone = DarknetBodyV1(layout, in_channels, stem_channels, act_layer, backbone_norm_layer)\n\n    self.block4 = nn.Sequential(\n        *conv_sequence(\n            1024,\n            1024,\n            act_layer,\n            norm_layer,\n            drop_layer,\n            conv_layer,\n            kernel_size=3,\n            padding=1,\n            bias=(norm_layer is None),\n        ),\n        *conv_sequence(\n            1024,\n            1024,\n            act_layer,\n            norm_layer,\n            drop_layer,\n            conv_layer,\n            kernel_size=3,\n            padding=1,\n            stride=2,\n            bias=(norm_layer is None),\n        ),\n        *conv_sequence(\n            1024,\n            1024,\n            act_layer,\n            norm_layer,\n            drop_layer,\n            conv_layer,\n            kernel_size=3,\n            padding=1,\n            bias=(norm_layer is None),\n        ),\n        *conv_sequence(\n            1024,\n            1024,\n            act_layer,\n            norm_layer,\n            drop_layer,\n            conv_layer,\n            kernel_size=3,\n            padding=1,\n            bias=(norm_layer is None),\n        ),\n    )\n\n    self.classifier = nn.Sequential(\n        nn.Flatten(),\n        nn.Linear(1024 * 7**2, head_hidden_nodes),\n        act_layer,\n        nn.Dropout(0.5),\n        nn.Linear(head_hidden_nodes, 7**2 * (num_anchors * 5 + num_classes)),\n    )\n    self.num_anchors: int = num_anchors\n\n    init_module(self.block4, \"leaky_relu\")\n    init_module(self.classifier, \"leaky_relu\")\n</code></pre>"},{"location":"reference/models/models/#holocron.models.detection.YOLOv1.to_isoboxes","title":"to_isoboxes  <code>staticmethod</code>","text":"<pre><code>to_isoboxes(b_coords: Tensor, grid_shape: tuple[int, int], clamp: bool = False) -&gt; Tensor\n</code></pre> <p>Converts xywh boxes to xyxy format.</p> PARAMETER DESCRIPTION <code>b_coords</code> <p>tensor of shape (..., 4) where the last dimension is xcenter,ycenter,w,h</p> <p> TYPE: <code>Tensor</code> </p> <code>grid_shape</code> <p>the size of the grid</p> <p> TYPE: <code>tuple[int, int]</code> </p> <code>clamp</code> <p>whether the coords should be clamped to the extreme values</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>tensor with the boxes using relative coords</p> Source code in <code>holocron/models/detection/yolo.py</code> <pre><code>@staticmethod\ndef to_isoboxes(b_coords: Tensor, grid_shape: tuple[int, int], clamp: bool = False) -&gt; Tensor:\n    \"\"\"Converts xywh boxes to xyxy format.\n\n    Args:\n        b_coords: tensor of shape (..., 4) where the last dimension is xcenter,ycenter,w,h\n        grid_shape: the size of the grid\n        clamp: whether the coords should be clamped to the extreme values\n\n    Returns:\n        tensor with the boxes using relative coords\n    \"\"\"\n    # Cell offset\n    c_x = torch.arange(grid_shape[1], dtype=torch.float, device=b_coords.device)\n    c_y = torch.arange(grid_shape[0], dtype=torch.float, device=b_coords.device)\n    # Box coordinates\n    b_x = (b_coords[..., 0] + c_x.reshape(1, 1, -1, 1)) / grid_shape[1]\n    b_y = (b_coords[..., 1] + c_y.reshape(1, -1, 1, 1)) / grid_shape[0]\n    xy = torch.stack((b_x, b_y), dim=-1)\n    wh = b_coords[..., 2:]\n    pred_xyxy = torch.cat((xy - wh / 2, xy + wh / 2), dim=-1).reshape(*b_coords.shape)\n    if clamp:\n        pred_xyxy.clamp_(0, 1)\n\n    return pred_xyxy\n</code></pre>"},{"location":"reference/models/models/#holocron.models.detection.YOLOv1.post_process","title":"post_process","text":"<pre><code>post_process(b_coords: Tensor, b_o: Tensor, b_scores: Tensor, grid_shape: tuple[int, int], rpn_nms_thresh: float = 0.7, box_score_thresh: float = 0.05) -&gt; list[dict[str, Tensor]]\n</code></pre> <p>Perform final filtering to produce detections</p> PARAMETER DESCRIPTION <code>b_coords</code> <p>relative coordinates in format (x, y, w, h)</p> <p> TYPE: <code>Tensor[N, H * W * num_anchors, 4]</code> </p> <code>b_o</code> <p>objectness scores</p> <p> TYPE: <code>Tensor[N, H * W * num_anchors]</code> </p> <code>b_scores</code> <p>classification scores</p> <p> TYPE: <code>Tensor[N, H * W * num_anchors, num_classes]</code> </p> <code>grid_shape</code> <p>the size of the grid</p> <p> TYPE: <code>Tuple[int, int]</code> </p> <code>rpn_nms_thresh</code> <p>IoU threshold for NMS</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.7</code> </p> <code>box_score_thresh</code> <p>minimum classification confidence threshold</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.05</code> </p> RETURNS DESCRIPTION <code>list[dict[str, Tensor]]</code> <p>list: detections dictionary Source code in <code>holocron/models/detection/yolo.py</code> <pre><code>def post_process(\n    self,\n    b_coords: Tensor,\n    b_o: Tensor,\n    b_scores: Tensor,\n    grid_shape: tuple[int, int],\n    rpn_nms_thresh: float = 0.7,\n    box_score_thresh: float = 0.05,\n) -&gt; list[dict[str, Tensor]]:\n    \"\"\"Perform final filtering to produce detections\n\n    Args:\n        b_coords (torch.Tensor[N, H * W * num_anchors, 4]): relative coordinates in format (x, y, w, h)\n        b_o (torch.Tensor[N, H * W * num_anchors]): objectness scores\n        b_scores (torch.Tensor[N, H * W * num_anchors, num_classes]): classification scores\n        grid_shape (Tuple[int, int]): the size of the grid\n        rpn_nms_thresh (float, optional): IoU threshold for NMS\n        box_score_thresh (float, optional): minimum classification confidence threshold\n\n    Returns:\n        list&lt;dict&gt;: detections dictionary\n    \"\"\"\n    # Convert box coords\n    pred_xyxy = self.to_isoboxes(\n        b_coords.reshape(-1, *grid_shape, self.num_anchors, 4),\n        grid_shape,\n        clamp=True,\n    ).reshape(b_o.shape[0], -1, 4)\n\n    detections = []\n    for idx in range(b_coords.shape[0]):\n        coords = torch.zeros((0, 4), dtype=b_o.dtype, device=b_o.device)\n        scores = torch.zeros(0, dtype=b_o.dtype, device=b_o.device)\n        labels = torch.zeros(0, dtype=torch.long, device=b_o.device)\n\n        # Objectness filter\n        obj_mask = b_o[idx] &gt;= 0.5\n        if torch.any(obj_mask):\n            coords = pred_xyxy[idx, obj_mask]\n            scores, labels = b_scores[idx, obj_mask].max(dim=-1)\n            # Multiply by the objectness\n            scores.mul_(b_o[idx, obj_mask])\n\n            # Confidence threshold\n            coords = coords[scores &gt;= box_score_thresh]\n            labels = labels[scores &gt;= box_score_thresh]\n            scores = scores[scores &gt;= box_score_thresh]\n\n            # NMS\n            kept_idxs = nms(coords, scores, iou_threshold=rpn_nms_thresh)\n            coords = coords[kept_idxs]\n            scores = scores[kept_idxs]\n            labels = labels[kept_idxs]\n\n        detections.append({\"boxes\": coords, \"scores\": scores, \"labels\": labels})\n\n    return detections\n</code></pre>"},{"location":"reference/models/models/#holocron.models.detection.YOLOv1.forward","title":"forward","text":"<pre><code>forward(x: Tensor, target: list[dict[str, Tensor]] | None = None) -&gt; dict[str, Tensor] | list[dict[str, Tensor]]\n</code></pre> <p>Perform detection on an image tensor and returns either the loss dictionary in training mode or the list of detections in eval mode.</p> PARAMETER DESCRIPTION <code>x</code> <p>input image tensor of shape (N, 3, H, W)</p> <p> TYPE: <code>Tensor</code> </p> <code>target</code> <p>each dict must have two keys <code>boxes</code> of type torch.Tensor[-1, 4] and <code>labels</code> of type torch.Tensor[-1]</p> <p> TYPE: <code>list[dict[str, Tensor]] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict[str, Tensor] | list[dict[str, Tensor]]</code> <p>loss dictionary in training mode or list of detections in eval mode</p> RAISES DESCRIPTION <code>ValueError</code> <p>if <code>target</code> is not specified in training mode</p> Source code in <code>holocron/models/detection/yolo.py</code> <pre><code>def forward(\n    self, x: Tensor, target: list[dict[str, Tensor]] | None = None\n) -&gt; dict[str, Tensor] | list[dict[str, Tensor]]:\n    \"\"\"Perform detection on an image tensor and returns either the loss dictionary in training mode\n    or the list of detections in eval mode.\n\n    Args:\n        x: input image tensor of shape (N, 3, H, W)\n        target: each dict must have two keys `boxes` of type torch.Tensor[-1, 4] and `labels` of type torch.Tensor[-1]\n\n    Returns:\n        loss dictionary in training mode or list of detections in eval mode\n\n    Raises:\n        ValueError: if `target` is not specified in training mode\n    \"\"\"\n    if self.training and target is None:\n        raise ValueError(\"`target` needs to be specified in training mode\")\n\n    if isinstance(x, (list, tuple)):\n        x = torch.stack(x, dim=0)  # ty: ignore[invalid-argument-type]\n\n    out = self._forward(x)\n\n    # (N, H * W * (num_anchors * 5 + num_classes) -&gt; (N, H, W, num_anchors)\n    b_coords, b_o, b_scores = self._format_outputs(out)\n\n    if self.training:\n        # Update losses\n        return self._compute_losses(b_coords, b_o, b_scores, target)  # type: ignore[arg-type]\n\n    # (B, H * W * num_anchors)\n    b_coords = b_coords.reshape(b_coords.shape[0], -1, 4)\n    b_o = b_o.reshape(b_o.shape[0], -1)\n    # Repeat for each anchor box\n    b_scores = b_scores.repeat_interleave(self.num_anchors, dim=3)\n    b_scores = b_scores.contiguous().reshape(b_scores.shape[0], -1, self.num_classes)\n\n    # Stack detections into a list\n    return self.post_process(b_coords, b_o, b_scores, (7, 7), self.rpn_nms_thresh, self.box_score_thresh)\n</code></pre>"},{"location":"reference/models/models/#holocron.models.detection.YOLOv2","title":"YOLOv2","text":"<pre><code>YOLOv2(layout: list[tuple[int, int]], num_classes: int = 20, in_channels: int = 3, stem_chanels: int = 32, anchors: Tensor | None = None, passthrough_ratio: int = 8, lambda_obj: float = 1, lambda_noobj: float = 0.5, lambda_class: float = 1, lambda_coords: float = 5, rpn_nms_thresh: float = 0.7, box_score_thresh: float = 0.05, act_layer: Module | None = None, norm_layer: Callable[[int], Module] | None = None, drop_layer: Callable[..., Module] | None = None, conv_layer: Callable[..., Module] | None = None, backbone_norm_layer: Callable[[int], Module] | None = None)\n</code></pre> <p>               Bases: <code>_YOLO</code></p> Source code in <code>holocron/models/detection/yolov2.py</code> <pre><code>def __init__(\n    self,\n    layout: list[tuple[int, int]],\n    num_classes: int = 20,\n    in_channels: int = 3,\n    stem_chanels: int = 32,\n    anchors: Tensor | None = None,\n    passthrough_ratio: int = 8,\n    lambda_obj: float = 1,\n    lambda_noobj: float = 0.5,\n    lambda_class: float = 1,\n    lambda_coords: float = 5,\n    rpn_nms_thresh: float = 0.7,\n    box_score_thresh: float = 0.05,\n    act_layer: nn.Module | None = None,\n    norm_layer: Callable[[int], nn.Module] | None = None,\n    drop_layer: Callable[..., nn.Module] | None = None,\n    conv_layer: Callable[..., nn.Module] | None = None,\n    backbone_norm_layer: Callable[[int], nn.Module] | None = None,\n) -&gt; None:\n    super().__init__(\n        num_classes, rpn_nms_thresh, box_score_thresh, lambda_obj, lambda_noobj, lambda_class, lambda_coords\n    )\n\n    if act_layer is None:\n        act_layer = nn.LeakyReLU(0.1, inplace=True)\n    if norm_layer is None:\n        norm_layer = nn.BatchNorm2d\n    if backbone_norm_layer is None:\n        backbone_norm_layer = norm_layer\n\n    # Priors computed using K-means\n    if anchors is None:\n        # cf. https://github.com/pjreddie/darknet/blob/master/cfg/yolov2-voc.cfg#L242\n        anchors = (\n            torch.tensor([\n                [1.3221, 1.73145],\n                [3.19275, 4.00944],\n                [5.05587, 8.09892],\n                [9.47112, 4.84053],\n                [11.2364, 10.0071],\n            ])\n            / 13\n        )\n\n    self.backbone = DarknetBodyV2(\n        layout, in_channels, stem_chanels, True, act_layer, backbone_norm_layer, drop_layer, conv_layer\n    )\n\n    self.block5 = nn.Sequential(\n        *conv_sequence(\n            layout[-1][0],\n            layout[-1][0],\n            act_layer,\n            norm_layer,\n            drop_layer,\n            conv_layer,\n            kernel_size=3,\n            padding=1,\n            bias=(norm_layer is None),\n        ),\n        *conv_sequence(\n            layout[-1][0],\n            layout[-1][0],\n            act_layer,\n            norm_layer,\n            drop_layer,\n            conv_layer,\n            kernel_size=3,\n            padding=1,\n            bias=(norm_layer is None),\n        ),\n    )\n\n    self.passthrough_layer = nn.Sequential(\n        *conv_sequence(\n            layout[-2][0],\n            layout[-2][0] // passthrough_ratio,\n            act_layer,\n            norm_layer,\n            drop_layer,\n            conv_layer,\n            kernel_size=1,\n            bias=(norm_layer is None),\n        ),\n        ConcatDownsample2d(scale_factor=2),\n    )\n\n    self.block6 = nn.Sequential(\n        *conv_sequence(\n            layout[-1][0] + layout[-2][0] // passthrough_ratio * 2**2,\n            layout[-1][0],\n            act_layer,\n            norm_layer,\n            drop_layer,\n            conv_layer,\n            kernel_size=3,\n            padding=1,\n            bias=(norm_layer is None),\n        )\n    )\n\n    # Each box has P_objectness, 4 coords, and score for each class\n    self.head = nn.Conv2d(layout[-1][0], anchors.shape[0] * (5 + num_classes), 1)\n\n    # Register losses\n    self.register_buffer(\"anchors\", anchors)\n\n    init_module(self.block5, \"leaky_relu\")\n    init_module(self.passthrough_layer, \"leaky_relu\")\n    init_module(self.block6, \"leaky_relu\")\n    # Initialize the head like a linear (default Conv2D init is the same as Linear)\n    if self.head.bias is not None:\n        self.head.bias.data.zero_()\n</code></pre>"},{"location":"reference/models/models/#holocron.models.detection.YOLOv2.post_process","title":"post_process","text":"<pre><code>post_process(b_coords: Tensor, b_o: Tensor, b_scores: Tensor, grid_shape: tuple[int, int], rpn_nms_thresh: float = 0.7, box_score_thresh: float = 0.05) -&gt; list[dict[str, Tensor]]\n</code></pre> <p>Perform final filtering to produce detections</p> PARAMETER DESCRIPTION <code>b_coords</code> <p>relative coordinates in format (x, y, w, h)</p> <p> TYPE: <code>Tensor[N, H * W * num_anchors, 4]</code> </p> <code>b_o</code> <p>objectness scores</p> <p> TYPE: <code>Tensor[N, H * W * num_anchors]</code> </p> <code>b_scores</code> <p>classification scores</p> <p> TYPE: <code>Tensor[N, H * W * num_anchors, num_classes]</code> </p> <code>grid_shape</code> <p>the size of the grid</p> <p> TYPE: <code>Tuple[int, int]</code> </p> <code>rpn_nms_thresh</code> <p>IoU threshold for NMS</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.7</code> </p> <code>box_score_thresh</code> <p>minimum classification confidence threshold</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.05</code> </p> RETURNS DESCRIPTION <code>list[dict[str, Tensor]]</code> <p>list: detections dictionary Source code in <code>holocron/models/detection/yolo.py</code> <pre><code>def post_process(\n    self,\n    b_coords: Tensor,\n    b_o: Tensor,\n    b_scores: Tensor,\n    grid_shape: tuple[int, int],\n    rpn_nms_thresh: float = 0.7,\n    box_score_thresh: float = 0.05,\n) -&gt; list[dict[str, Tensor]]:\n    \"\"\"Perform final filtering to produce detections\n\n    Args:\n        b_coords (torch.Tensor[N, H * W * num_anchors, 4]): relative coordinates in format (x, y, w, h)\n        b_o (torch.Tensor[N, H * W * num_anchors]): objectness scores\n        b_scores (torch.Tensor[N, H * W * num_anchors, num_classes]): classification scores\n        grid_shape (Tuple[int, int]): the size of the grid\n        rpn_nms_thresh (float, optional): IoU threshold for NMS\n        box_score_thresh (float, optional): minimum classification confidence threshold\n\n    Returns:\n        list&lt;dict&gt;: detections dictionary\n    \"\"\"\n    # Convert box coords\n    pred_xyxy = self.to_isoboxes(\n        b_coords.reshape(-1, *grid_shape, self.num_anchors, 4),\n        grid_shape,\n        clamp=True,\n    ).reshape(b_o.shape[0], -1, 4)\n\n    detections = []\n    for idx in range(b_coords.shape[0]):\n        coords = torch.zeros((0, 4), dtype=b_o.dtype, device=b_o.device)\n        scores = torch.zeros(0, dtype=b_o.dtype, device=b_o.device)\n        labels = torch.zeros(0, dtype=torch.long, device=b_o.device)\n\n        # Objectness filter\n        obj_mask = b_o[idx] &gt;= 0.5\n        if torch.any(obj_mask):\n            coords = pred_xyxy[idx, obj_mask]\n            scores, labels = b_scores[idx, obj_mask].max(dim=-1)\n            # Multiply by the objectness\n            scores.mul_(b_o[idx, obj_mask])\n\n            # Confidence threshold\n            coords = coords[scores &gt;= box_score_thresh]\n            labels = labels[scores &gt;= box_score_thresh]\n            scores = scores[scores &gt;= box_score_thresh]\n\n            # NMS\n            kept_idxs = nms(coords, scores, iou_threshold=rpn_nms_thresh)\n            coords = coords[kept_idxs]\n            scores = scores[kept_idxs]\n            labels = labels[kept_idxs]\n\n        detections.append({\"boxes\": coords, \"scores\": scores, \"labels\": labels})\n\n    return detections\n</code></pre>"},{"location":"reference/models/models/#holocron.models.detection.YOLOv2.to_isoboxes","title":"to_isoboxes  <code>staticmethod</code>","text":"<pre><code>to_isoboxes(b_coords: Tensor, grid_shape: tuple[int, int], clamp: bool = False) -&gt; Tensor\n</code></pre> <p>Converts xywh boxes to xyxy format.</p> PARAMETER DESCRIPTION <code>b_coords</code> <p>tensor of shape (..., 4) where the last dimension is xcenter,ycenter,w,h</p> <p> TYPE: <code>Tensor</code> </p> <code>grid_shape</code> <p>the size of the grid</p> <p> TYPE: <code>tuple[int, int]</code> </p> <code>clamp</code> <p>whether the coords should be clamped to the extreme values</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>tensor with the boxes using relative coords</p> Source code in <code>holocron/models/detection/yolov2.py</code> <pre><code>@staticmethod\ndef to_isoboxes(b_coords: Tensor, grid_shape: tuple[int, int], clamp: bool = False) -&gt; Tensor:\n    \"\"\"Converts xywh boxes to xyxy format.\n\n    Args:\n        b_coords: tensor of shape (..., 4) where the last dimension is xcenter,ycenter,w,h\n        grid_shape: the size of the grid\n        clamp: whether the coords should be clamped to the extreme values\n\n    Returns:\n        tensor with the boxes using relative coords\n    \"\"\"\n    xy = b_coords[..., :2]\n    wh = b_coords[..., 2:]\n    pred_xyxy = torch.cat((xy - wh / 2, xy + wh / 2), dim=-1).reshape(*b_coords.shape)\n    if clamp:\n        pred_xyxy.clamp_(0, 1)\n\n    return pred_xyxy\n</code></pre>"},{"location":"reference/models/models/#holocron.models.detection.YOLOv2.forward","title":"forward","text":"<pre><code>forward(x: Tensor | list[Tensor] | tuple[Tensor, ...], target: list[dict[str, Tensor]] | None = None) -&gt; dict[str, Tensor] | list[dict[str, Tensor]]\n</code></pre> <p>Perform detection on an image tensor and returns either the loss dictionary in training mode or the list of detections in eval mode.</p> PARAMETER DESCRIPTION <code>x</code> <p>input image tensor of shape (N, 3, H, W)</p> <p> TYPE: <code>Tensor | list[Tensor] | tuple[Tensor, ...]</code> </p> <code>target</code> <p>each dict must have two keys <code>boxes</code> of type torch.Tensor[-1, 4] and <code>labels</code> of type torch.Tensor[-1]</p> <p> TYPE: <code>list[dict[str, Tensor]] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict[str, Tensor] | list[dict[str, Tensor]]</code> <p>loss dictionary in training mode or list of detections in eval mode</p> RAISES DESCRIPTION <code>ValueError</code> <p>if <code>target</code> is not specified in training mode</p> Source code in <code>holocron/models/detection/yolov2.py</code> <pre><code>def forward(\n    self, x: Tensor | list[Tensor] | tuple[Tensor, ...], target: list[dict[str, Tensor]] | None = None\n) -&gt; dict[str, Tensor] | list[dict[str, Tensor]]:\n    \"\"\"Perform detection on an image tensor and returns either the loss dictionary in training mode\n    or the list of detections in eval mode.\n\n    Args:\n        x: input image tensor of shape (N, 3, H, W)\n        target: each dict must have two keys `boxes` of type torch.Tensor[-1, 4] and `labels` of type torch.Tensor[-1]\n\n    Returns:\n        loss dictionary in training mode or list of detections in eval mode\n\n    Raises:\n        ValueError: if `target` is not specified in training mode\n    \"\"\"\n    if self.training and target is None:\n        raise ValueError(\"`target` needs to be specified in training mode\")\n\n    if isinstance(x, (list, tuple)):\n        x = torch.stack(x, dim=0)  # ty: ignore[invalid-argument-type]\n\n    out = self._forward(x)\n\n    # (B, H, W, num_anchors)\n    b_coords, b_o, b_scores = self._format_outputs(out)\n\n    if self.training:\n        # Update losses\n        return self._compute_losses(b_coords, b_o, b_scores, target)  # type: ignore[arg-type]\n\n    # (B, H * W * num_anchors)\n    b_coords = b_coords.reshape(b_coords.shape[0], -1, 4)\n    b_o = b_o.reshape(b_o.shape[0], -1)\n    b_scores = b_scores.reshape(b_scores.shape[0], -1, self.num_classes)\n\n    # Stack detections into a list\n    return self.post_process(\n        b_coords,\n        b_o,\n        b_scores,\n        out.shape[-2:],  # type: ignore[arg-type]\n        self.rpn_nms_thresh,\n        self.box_score_thresh,\n    )\n</code></pre>"},{"location":"reference/models/models/#holocron.models.detection.YOLOv4","title":"YOLOv4","text":"<pre><code>YOLOv4(layout: list[tuple[int, int]], num_classes: int = 80, in_channels: int = 3, stem_channels: int = 32, anchors: Tensor | None = None, act_layer: Module | None = None, norm_layer: Callable[[int], Module] | None = None, drop_layer: Callable[..., Module] | None = None, conv_layer: Callable[..., Module] | None = None, backbone_norm_layer: Callable[[int], Module] | None = None)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>holocron/models/detection/yolov4.py</code> <pre><code>def __init__(\n    self,\n    layout: list[tuple[int, int]],\n    num_classes: int = 80,\n    in_channels: int = 3,\n    stem_channels: int = 32,\n    anchors: Tensor | None = None,\n    act_layer: nn.Module | None = None,\n    norm_layer: Callable[[int], nn.Module] | None = None,\n    drop_layer: Callable[..., nn.Module] | None = None,\n    conv_layer: Callable[..., nn.Module] | None = None,\n    backbone_norm_layer: Callable[[int], nn.Module] | None = None,\n) -&gt; None:\n    super().__init__()\n\n    if act_layer is None:\n        act_layer = nn.Mish(inplace=True)\n    if norm_layer is None:\n        norm_layer = nn.BatchNorm2d\n    if backbone_norm_layer is None:\n        backbone_norm_layer = norm_layer\n    if drop_layer is None:\n        drop_layer = DropBlock2d\n\n    # backbone\n    self.backbone = DarknetBodyV4(\n        layout, in_channels, stem_channels, 3, act_layer, backbone_norm_layer, drop_layer, conv_layer\n    )\n    # neck\n    self.neck = Neck([1024, 512, 256], act_layer, norm_layer, drop_layer, conv_layer)\n    # head\n    self.head = Yolov4Head(num_classes, anchors, act_layer, norm_layer, drop_layer, conv_layer)\n\n    init_module(self.neck, \"leaky_relu\")\n    init_module(self.head, \"leaky_relu\")\n</code></pre>"},{"location":"reference/models/models/#holocron.models.detection.yolov1","title":"yolov1","text":"<pre><code>yolov1(pretrained: bool = False, progress: bool = True, pretrained_backbone: bool = True, **kwargs: Any) -&gt; YOLOv1\n</code></pre> <p>YOLO model from \"You Only Look Once: Unified, Real-Time Object Detection\".</p> <p>YOLO's particularity is to make predictions in a grid (same size as last feature map). For each grid cell, the model predicts classification scores and a fixed number of boxes (default: 2). Each box in the cell gets 5 predictions: an objectness score, and 4 coordinates. The 4 coordinates are composed of: the 2-D coordinates of the predicted box center (relative to the cell), and the width and height of the predicted box (relative to the whole image).</p> <p>For training, YOLO uses a multi-part loss whose components are computed by:</p> \\[ \\mathcal{L}_{coords} = \\sum\\limits_{i=0}^{S^2} \\sum\\limits_{j=0}^{B} \\mathbb{1}_{ij}^{obj} \\Big[ (x_{ij} - \\hat{x}_{ij})\u00b2 + (y_{ij} - \\hat{y}_{ij})\u00b2 + (\\sqrt{w_{ij}} - \\sqrt{\\hat{w}_{ij}})\u00b2 + (\\sqrt{h_{ij}} - \\sqrt{\\hat{h}_{ij}})\u00b2 \\Big] \\] <p>where: \\(S\\) is size of the output feature map (7 for an input size \\((448, 448)\\)), \\(B\\) is the number of anchor boxes per grid cell (default: 2), \\(\\mathbb{1}_{ij}^{obj}\\) equals to 1 if a GT center falls inside the i-th grid cell and among the anchor boxes of that cell, has the highest IoU with the j-th box else 0, \\((x_{ij}, y_{ij}, w_{ij}, h_{ij})\\) are the coordinates of the ground truth assigned to the j-th anchor box of the i-th grid cell, \\((\\hat{x}_{ij}, \\hat{y}_{ij}, \\hat{w}_{ij}, \\hat{h}_{ij})\\) are the coordinate predictions for the j-th anchor box of the i-th grid cell.</p> \\[ \\mathcal{L}_{objectness} = \\sum\\limits_{i=0}^{S^2} \\sum\\limits_{j=0}^{B} \\Big[ \\mathbb{1}_{ij}^{obj} \\Big(C_{ij} - \\hat{C}_{ij} \\Big)^2 + \\lambda_{noobj} \\mathbb{1}_{ij}^{noobj} \\Big(C_{ij} - \\hat{C}_{ij} \\Big)^2 \\Big] \\] <p>where \\(\\lambda_{noobj}\\) is a positive coefficient (default: 0.5), \\(\\mathbb{1}_{ij}^{noobj} = 1 - \\mathbb{1}_{ij}^{obj}\\), \\(C_{ij}\\) equals the Intersection Over Union between the j-th anchor box in the i-th grid cell and its matched ground truth box if that box is matched with a ground truth else 0, and \\(\\hat{C}_{ij}\\) is the objectness score of the j-th anchor box in the i-th grid cell..</p> \\[ \\mathcal{L}_{classification} = \\sum\\limits_{i=0}^{S^2} \\mathbb{1}_{i}^{obj} \\sum\\limits_{c \\in classes} (p_i(c) - \\hat{p}_i(c))^2 \\] <p>where \\(\\mathbb{1}_{i}^{obj}\\) equals to 1 if a GT center falls inside the i-th grid cell else 0, \\(p_i(c)\\) equals 1 if the assigned ground truth to the i-th cell is classified as class \\(c\\), and \\(\\hat{p}_i(c)\\) is the predicted probability of class \\(c\\) in the i-th cell.</p> <p>And the full loss is given by:</p> \\[ \\mathcal{L}_{YOLOv1} = \\lambda_{coords} \\cdot \\mathcal{L}_{coords} + \\mathcal{L}_{objectness} + \\mathcal{L}_{classification} \\] <p>where \\(\\lambda_{coords}\\) is a positive coefficient (default: 5).</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>pretrained_backbone</code> <p>If True, backbone parameters will have been pretrained on Imagenette</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>YOLOv1</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>YOLOv1</code> <p>detection module</p> Source code in <code>holocron/models/detection/yolo.py</code> <pre><code>def yolov1(pretrained: bool = False, progress: bool = True, pretrained_backbone: bool = True, **kwargs: Any) -&gt; YOLOv1:\n    r\"\"\"YOLO model from\n    [\"You Only Look Once: Unified, Real-Time Object Detection\"](https://pjreddie.com/media/files/papers/yolo_1.pdf).\n\n    YOLO's particularity is to make predictions in a grid (same size as last feature map). For each grid cell,\n    the model predicts classification scores and a fixed number of boxes (default: 2). Each box in the cell gets\n    5 predictions: an objectness score, and 4 coordinates. The 4 coordinates are composed of: the 2-D coordinates of\n    the predicted box center (relative to the cell), and the width and height of the predicted box (relative to\n    the whole image).\n\n    For training, YOLO uses a multi-part loss whose components are computed by:\n\n    $$\n    \\mathcal{L}_{coords} = \\sum\\limits_{i=0}^{S^2} \\sum\\limits_{j=0}^{B}\n    \\mathbb{1}_{ij}^{obj} \\Big[\n    (x_{ij} - \\hat{x}_{ij})\u00b2 + (y_{ij} - \\hat{y}_{ij})\u00b2 +\n    (\\sqrt{w_{ij}} - \\sqrt{\\hat{w}_{ij}})\u00b2 + (\\sqrt{h_{ij}} - \\sqrt{\\hat{h}_{ij}})\u00b2\n    \\Big]\n    $$\n\n    where:\n    $S$ is size of the output feature map (7 for an input size $(448, 448)$),\n    $B$ is the number of anchor boxes per grid cell (default: 2),\n    $\\mathbb{1}_{ij}^{obj}$ equals to 1 if a GT center falls inside the i-th grid cell and among the\n    anchor boxes of that cell, has the highest IoU with the j-th box else 0,\n    $(x_{ij}, y_{ij}, w_{ij}, h_{ij})$ are the coordinates of the ground truth assigned to\n    the j-th anchor box of the i-th grid cell,\n    $(\\hat{x}_{ij}, \\hat{y}_{ij}, \\hat{w}_{ij}, \\hat{h}_{ij})$ are the coordinate predictions\n    for the j-th anchor box of the i-th grid cell.\n\n    $$\n    \\mathcal{L}_{objectness} = \\sum\\limits_{i=0}^{S^2} \\sum\\limits_{j=0}^{B}\n    \\Big[ \\mathbb{1}_{ij}^{obj} \\Big(C_{ij} - \\hat{C}_{ij} \\Big)^2\n    + \\lambda_{noobj} \\mathbb{1}_{ij}^{noobj} \\Big(C_{ij} - \\hat{C}_{ij} \\Big)^2\n    \\Big]\n    $$\n\n    where $\\lambda_{noobj}$ is a positive coefficient (default: 0.5),\n    $\\mathbb{1}_{ij}^{noobj} = 1 - \\mathbb{1}_{ij}^{obj}$,\n    $C_{ij}$ equals the Intersection Over Union between the j-th anchor box in the i-th grid cell and its\n    matched ground truth box if that box is matched with a ground truth else 0,\n    and $\\hat{C}_{ij}$ is the objectness score of the j-th anchor box in the i-th grid cell..\n\n    $$\n    \\mathcal{L}_{classification} = \\sum\\limits_{i=0}^{S^2}\n    \\mathbb{1}_{i}^{obj} \\sum\\limits_{c \\in classes}\n    (p_i(c) - \\hat{p}_i(c))^2\n    $$\n\n    where $\\mathbb{1}_{i}^{obj}$ equals to 1 if a GT center falls inside the i-th grid cell else 0,\n    $p_i(c)$ equals 1 if the assigned ground truth to the i-th cell is classified as class $c$,\n    and $\\hat{p}_i(c)$ is the predicted probability of class $c$ in the i-th cell.\n\n    And the full loss is given by:\n\n    $$\n    \\mathcal{L}_{YOLOv1} = \\lambda_{coords} \\cdot \\mathcal{L}_{coords} +\n    \\mathcal{L}_{objectness} + \\mathcal{L}_{classification}\n    $$\n\n    where $\\lambda_{coords}$ is a positive coefficient (default: 5).\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        progress: If True, displays a progress bar of the download to stderr\n        pretrained_backbone: If True, backbone parameters will have been pretrained on Imagenette\n        kwargs: keyword args of [`YOLOv1`][holocron.models.detection.yolo.YOLOv1]\n\n    Returns:\n        detection module\n    \"\"\"\n    return _yolo(\n        \"yolov1\",\n        pretrained,\n        progress,\n        pretrained_backbone,\n        [[192], [128, 256, 256, 512], [*([256, 512] * 4), 512, 1024], [512, 1024] * 2],\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/models/models/#semantic-segmentation","title":"Semantic Segmentation","text":"<p>Semantic segmentation models expect a 4D image tensor as an input (N x C x H x W) and returns a classification score tensor of size (N x K x Ho x Wo).</p> <pre><code>import holocron.models as models\nunet = models.unet(num_classes=10)\n</code></pre>"},{"location":"reference/models/models/#u-net","title":"U-Net","text":""},{"location":"reference/models/models/#holocron.models.segmentation.UNet","title":"UNet","text":"<pre><code>UNet(layout: list[int], in_channels: int = 3, num_classes: int = 10, act_layer: Module | None = None, norm_layer: Callable[[int], Module] | None = None, drop_layer: Callable[..., Module] | None = None, conv_layer: Callable[..., Module] | None = None, same_padding: bool = True, bilinear_upsampling: bool = True)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Implements a U-Net architecture</p> PARAMETER DESCRIPTION <code>layout</code> <p>number of channels after each contracting block</p> <p> TYPE: <code>list[int]</code> </p> <code>in_channels</code> <p>number of channels in the input tensor</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>number of output classes</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>act_layer</code> <p>activation layer</p> <p> TYPE: <code>Module | None</code> DEFAULT: <code>None</code> </p> <code>norm_layer</code> <p>normalization layer</p> <p> TYPE: <code>Callable[[int], Module] | None</code> DEFAULT: <code>None</code> </p> <code>drop_layer</code> <p>dropout layer</p> <p> TYPE: <code>Callable[..., Module] | None</code> DEFAULT: <code>None</code> </p> <code>conv_layer</code> <p>convolutional layer</p> <p> TYPE: <code>Callable[..., Module] | None</code> DEFAULT: <code>None</code> </p> <code>same_padding</code> <p>enforces same padding in convolutions</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>bilinear_upsampling</code> <p>replaces transposed conv by bilinear interpolation for upsampling</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>holocron/models/segmentation/unet.py</code> <pre><code>def __init__(\n    self,\n    layout: list[int],\n    in_channels: int = 3,\n    num_classes: int = 10,\n    act_layer: nn.Module | None = None,\n    norm_layer: Callable[[int], nn.Module] | None = None,\n    drop_layer: Callable[..., nn.Module] | None = None,\n    conv_layer: Callable[..., nn.Module] | None = None,\n    same_padding: bool = True,\n    bilinear_upsampling: bool = True,\n) -&gt; None:\n    super().__init__()\n\n    if act_layer is None:\n        act_layer = nn.ReLU(inplace=True)\n\n    # Contracting path\n    self.encoder = nn.ModuleList([])\n    layout_ = [in_channels, *layout]\n    pool = False\n    for in_chan, out_chan in pairwise(layout_):\n        self.encoder.append(\n            down_path(in_chan, out_chan, pool, int(same_padding), act_layer, norm_layer, drop_layer, conv_layer)\n        )\n        pool = True\n\n    self.bridge = nn.Sequential(\n        nn.MaxPool2d((2, 2)),\n        *conv_sequence(\n            layout[-1], 2 * layout[-1], act_layer, norm_layer, drop_layer, conv_layer, kernel_size=3, padding=1\n        ),\n        *conv_sequence(\n            2 * layout[-1], layout[-1], act_layer, norm_layer, drop_layer, conv_layer, kernel_size=3, padding=1\n        ),\n    )\n\n    # Expansive path\n    self.decoder = nn.ModuleList([])\n    layout_ = [chan // 2 if bilinear_upsampling else chan for chan in layout[::-1][:-1]] + [layout[0]]\n    for in_chan, out_chan in zip([2 * layout[-1], *layout[::-1][:-1]], layout_, strict=True):\n        self.decoder.append(\n            UpPath(\n                in_chan,\n                out_chan,\n                bilinear_upsampling,\n                int(same_padding),\n                act_layer,\n                norm_layer,\n                drop_layer,\n                conv_layer,\n            )\n        )\n\n    # Classifier\n    self.classifier = nn.Conv2d(layout[0], num_classes, 1)\n\n    init_module(self, \"relu\")\n</code></pre>"},{"location":"reference/models/models/#holocron.models.segmentation.DynamicUNet","title":"DynamicUNet","text":"<pre><code>DynamicUNet(encoder: IntermediateLayerGetter, num_classes: int = 10, act_layer: Module | None = None, norm_layer: Callable[[int], Module] | None = None, drop_layer: Callable[..., Module] | None = None, conv_layer: Callable[..., Module] | None = None, same_padding: bool = True, input_shape: tuple[int, int, int] | None = None, final_upsampling: bool = False)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Implements a dymanic U-Net architecture</p> PARAMETER DESCRIPTION <code>encoder</code> <p>feature extractor used for encoding</p> <p> TYPE: <code>IntermediateLayerGetter</code> </p> <code>num_classes</code> <p>number of output classes</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>act_layer</code> <p>activation layer</p> <p> TYPE: <code>Module | None</code> DEFAULT: <code>None</code> </p> <code>norm_layer</code> <p>normalization layer</p> <p> TYPE: <code>Callable[[int], Module] | None</code> DEFAULT: <code>None</code> </p> <code>drop_layer</code> <p>dropout layer</p> <p> TYPE: <code>Callable[..., Module] | None</code> DEFAULT: <code>None</code> </p> <code>conv_layer</code> <p>convolutional layer</p> <p> TYPE: <code>Callable[..., Module] | None</code> DEFAULT: <code>None</code> </p> <code>same_padding</code> <p>enforces same padding in convolutions</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>input_shape</code> <p>shape of the input tensor</p> <p> TYPE: <code>tuple[int, int, int] | None</code> DEFAULT: <code>None</code> </p> <code>final_upsampling</code> <p>if True, replaces transposed conv by bilinear interpolation for upsampling</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>holocron/models/segmentation/unet.py</code> <pre><code>def __init__(\n    self,\n    encoder: IntermediateLayerGetter,\n    num_classes: int = 10,\n    act_layer: nn.Module | None = None,\n    norm_layer: Callable[[int], nn.Module] | None = None,\n    drop_layer: Callable[..., nn.Module] | None = None,\n    conv_layer: Callable[..., nn.Module] | None = None,\n    same_padding: bool = True,\n    input_shape: tuple[int, int, int] | None = None,\n    final_upsampling: bool = False,\n) -&gt; None:\n    super().__init__()\n\n    if act_layer is None:\n        act_layer = nn.ReLU(inplace=True)\n\n    self.encoder = encoder\n    # Determine all feature map shapes\n    training_mode = self.encoder.training\n    self.encoder.eval()\n    input_shape = (3, 256, 256) if input_shape is None else input_shape\n    with torch.no_grad():\n        shapes = [v.shape[1:] for v in self.encoder(torch.zeros(1, *input_shape)).values()]\n    chans = [s[0] for s in shapes]\n    if training_mode:\n        self.encoder.train()\n\n    # Middle layers\n    self.bridge = nn.Sequential(\n        nn.BatchNorm2d(chans[-1]) if norm_layer is None else norm_layer(chans[-1]),\n        act_layer,\n        *conv_sequence(\n            chans[-1], 2 * chans[-1], act_layer, norm_layer, drop_layer, conv_layer, kernel_size=3, padding=1\n        ),\n        *conv_sequence(\n            2 * chans[-1], chans[-1], act_layer, norm_layer, drop_layer, conv_layer, kernel_size=3, padding=1\n        ),\n    )\n\n    # Expansive path\n    self.decoder = nn.ModuleList([])\n    layout = [*chans[::-1][1:], chans[0]]\n    for up_chan, out_chan in zip(chans[::-1], layout, strict=True):\n        self.decoder.append(\n            UBlock(up_chan, up_chan, out_chan, int(same_padding), act_layer, norm_layer, drop_layer, conv_layer)\n        )\n\n    # Final upsampling if sizes don't match\n    self.upsample: nn.Sequential | None = None\n    if final_upsampling:\n        self.upsample = nn.Sequential(\n            *conv_sequence(chans[0], chans[0] * 2**2, act_layer, norm_layer, drop_layer, conv_layer, kernel_size=1),\n            nn.PixelShuffle(upscale_factor=2),\n        )\n\n    # Classifier\n    self.classifier = nn.Conv2d(chans[0], num_classes, 1)\n\n    init_module(self, \"relu\")\n</code></pre>"},{"location":"reference/models/models/#holocron.models.segmentation.unet2","title":"unet2","text":"<pre><code>unet2(pretrained: bool = False, progress: bool = True, in_channels: int = 3, **kwargs: Any) -&gt; DynamicUNet\n</code></pre> <p>Modified version of U-Net from \"U-Net: Convolutional Networks for Biomedical Image Segmentation\" that includes a more advanced upscaling block inspired by fastai</p> <p></p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on PASCAL VOC2012</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>in_channels</code> <p>number of input channels</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>kwargs</code> <p>keyword args of <code>DynamicUNet</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>DynamicUNet</code> <p>semantic segmentation model</p> Source code in <code>holocron/models/segmentation/unet.py</code> <pre><code>def unet2(pretrained: bool = False, progress: bool = True, in_channels: int = 3, **kwargs: Any) -&gt; DynamicUNet:\n    \"\"\"Modified version of U-Net from\n    [\"U-Net: Convolutional Networks for Biomedical Image Segmentation\"](https://arxiv.org/pdf/1505.04597.pdf)\n    that includes a more advanced upscaling block inspired by [fastai](https://docs.fast.ai/vision.models.unet.html#DynamicUnet)\n\n    ![UNet architecture](https://github.com/frgfm/Holocron/releases/download/v0.1.3/unet.png)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on PASCAL VOC2012\n        progress: If True, displays a progress bar of the download to stderr\n        in_channels: number of input channels\n        kwargs: keyword args of [`DynamicUNet`][holocron.models.segmentation.unet.DynamicUNet]\n\n    Returns:\n        semantic segmentation model\n    \"\"\"\n    backbone = UNetBackbone(default_cfgs[\"unet2\"][\"encoder_layout\"], in_channels=in_channels).features\n\n    return _dynamic_unet(\"unet2\", backbone, pretrained, progress, **kwargs)  # ty: ignore[invalid-argument-type]\n</code></pre>"},{"location":"reference/models/models/#holocron.models.segmentation.unet_tvvgg11","title":"unet_tvvgg11","text":"<pre><code>unet_tvvgg11(pretrained: bool = False, pretrained_backbone: bool = True, progress: bool = True, **kwargs: Any) -&gt; DynamicUNet\n</code></pre> <p>U-Net from \"U-Net: Convolutional Networks for Biomedical Image Segmentation\" with a VGG-11 backbone used as encoder, and more advanced upscaling blocks inspired by fastai</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on PASCAL VOC2012</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>pretrained_backbone</code> <p>If True, the encoder will load pretrained parameters from ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>DynamicUNet</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>DynamicUNet</code> <p>semantic segmentation model</p> Source code in <code>holocron/models/segmentation/unet.py</code> <pre><code>def unet_tvvgg11(\n    pretrained: bool = False, pretrained_backbone: bool = True, progress: bool = True, **kwargs: Any\n) -&gt; DynamicUNet:\n    \"\"\"U-Net from\n    [\"U-Net: Convolutional Networks for Biomedical Image Segmentation\"](https://arxiv.org/pdf/1505.04597.pdf)\n    with a VGG-11 backbone used as encoder, and more advanced upscaling blocks inspired by [fastai](https://docs.fast.ai/vision.models.unet.html#DynamicUnet)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on PASCAL VOC2012\n        pretrained_backbone: If True, the encoder will load pretrained parameters from ImageNet\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`DynamicUNet`][holocron.models.segmentation.unet.DynamicUNet]\n\n    Returns:\n        semantic segmentation model\n    \"\"\"\n    weights = get_model_weights(\"vgg11\").DEFAULT if pretrained_backbone and not pretrained else None  # ty: ignore[unresolved-attribute]\n    backbone: nn.Module = get_model(\"vgg11\", weights=weights).features  # ty: ignore[invalid-assignment]\n\n    return _dynamic_unet(\"unet_vgg11\", backbone, pretrained, progress, **kwargs)\n</code></pre>"},{"location":"reference/models/models/#holocron.models.segmentation.unet_tvresnet34","title":"unet_tvresnet34","text":"<pre><code>unet_tvresnet34(pretrained: bool = False, pretrained_backbone: bool = True, progress: bool = True, **kwargs: Any) -&gt; DynamicUNet\n</code></pre> <p>U-Net from \"U-Net: Convolutional Networks for Biomedical Image Segmentation\" with a ResNet-34 backbone used as encoder, and more advanced upscaling blocks inspired by fastai</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on PASCAL VOC2012</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>pretrained_backbone</code> <p>If True, the encoder will load pretrained parameters from ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>DynamicUNet</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>DynamicUNet</code> <p>semantic segmentation model</p> Source code in <code>holocron/models/segmentation/unet.py</code> <pre><code>def unet_tvresnet34(\n    pretrained: bool = False, pretrained_backbone: bool = True, progress: bool = True, **kwargs: Any\n) -&gt; DynamicUNet:\n    \"\"\"U-Net from\n    [\"U-Net: Convolutional Networks for Biomedical Image Segmentation\"](https://arxiv.org/pdf/1505.04597.pdf)\n    with a ResNet-34 backbone used as encoder, and more advanced upscaling blocks inspired by [fastai](https://docs.fast.ai/vision.models.unet.html#DynamicUnet)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on PASCAL VOC2012\n        pretrained_backbone: If True, the encoder will load pretrained parameters from ImageNet\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`DynamicUNet`][holocron.models.segmentation.unet.DynamicUNet]\n\n    Returns:\n        semantic segmentation model\n    \"\"\"\n    weights = get_model_weights(\"resnet34\").DEFAULT if pretrained_backbone and not pretrained else None  # ty: ignore[unresolved-attribute]\n    backbone = get_model(\"resnet34\", weights=weights)\n    kwargs[\"final_upsampling\"] = kwargs.get(\"final_upsampling\", True)\n\n    return _dynamic_unet(\"unet_tvresnet34\", backbone, pretrained, progress, **kwargs)\n</code></pre>"},{"location":"reference/models/models/#holocron.models.segmentation.unet_rexnet13","title":"unet_rexnet13","text":"<pre><code>unet_rexnet13(pretrained: bool = False, pretrained_backbone: bool = True, progress: bool = True, in_channels: int = 3, **kwargs: Any) -&gt; DynamicUNet\n</code></pre> <p>U-Net from \"U-Net: Convolutional Networks for Biomedical Image Segmentation\" with a ReXNet-1.3x backbone used as encoder, and more advanced upscaling blocks inspired by fastai.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on PASCAL VOC2012</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>pretrained_backbone</code> <p>If True, the encoder will load pretrained parameters from ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>in_channels</code> <p>the number of input channels</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>kwargs</code> <p>keyword args of <code>DynamicUNet</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>DynamicUNet</code> <p>semantic segmentation model</p> Source code in <code>holocron/models/segmentation/unet.py</code> <pre><code>def unet_rexnet13(\n    pretrained: bool = False,\n    pretrained_backbone: bool = True,\n    progress: bool = True,\n    in_channels: int = 3,\n    **kwargs: Any,\n) -&gt; DynamicUNet:\n    \"\"\"U-Net from\n    [\"U-Net: Convolutional Networks for Biomedical Image Segmentation\"](https://arxiv.org/pdf/1505.04597.pdf)\n    with a ReXNet-1.3x backbone used as encoder, and more advanced upscaling blocks inspired by [fastai](https://docs.fast.ai/vision.models.unet.html#DynamicUnet).\n\n    Args:\n        pretrained: If True, returns a model pre-trained on PASCAL VOC2012\n        pretrained_backbone: If True, the encoder will load pretrained parameters from ImageNet\n        progress: If True, displays a progress bar of the download to stderr\n        in_channels: the number of input channels\n        kwargs: keyword args of [`DynamicUNet`][holocron.models.segmentation.unet.DynamicUNet]\n\n    Returns:\n        semantic segmentation model\n    \"\"\"\n    backbone = rexnet1_3x(pretrained=pretrained_backbone and not pretrained, in_channels=in_channels).features\n    kwargs[\"final_upsampling\"] = kwargs.get(\"final_upsampling\", True)\n    kwargs[\"act_layer\"] = kwargs.get(\"act_layer\", nn.SiLU(inplace=True))\n    # hotfix of https://github.com/pytorch/vision/issues/3802\n    backbone[21] = nn.SiLU(inplace=True)  # ty: ignore[possibly-missing-implicit-call]\n\n    return _dynamic_unet(\"unet_rexnet13\", backbone, pretrained, progress, **kwargs)  # ty: ignore[invalid-argument-type]\n</code></pre>"},{"location":"reference/models/models/#holocron.models.segmentation.UNetp","title":"UNetp","text":"<pre><code>UNetp(layout: list[int], in_channels: int = 3, num_classes: int = 10, act_layer: Module | None = None, norm_layer: Callable[[int], Module] | None = None, drop_layer: Callable[..., Module] | None = None, conv_layer: Callable[..., Module] | None = None)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Implements a UNet+ architecture</p> PARAMETER DESCRIPTION <code>layout</code> <p>number of channels after each contracting block</p> <p> TYPE: <code>list[int]</code> </p> <code>in_channels</code> <p>number of channels in the input tensor</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>number of output classes</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>act_layer</code> <p>activation layer</p> <p> TYPE: <code>Module | None</code> DEFAULT: <code>None</code> </p> <code>norm_layer</code> <p>normalization layer</p> <p> TYPE: <code>Callable[[int], Module] | None</code> DEFAULT: <code>None</code> </p> <code>drop_layer</code> <p>dropout layer</p> <p> TYPE: <code>Callable[..., Module] | None</code> DEFAULT: <code>None</code> </p> <code>conv_layer</code> <p>convolutional layer</p> <p> TYPE: <code>Callable[..., Module] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>holocron/models/segmentation/unetpp.py</code> <pre><code>def __init__(\n    self,\n    layout: list[int],\n    in_channels: int = 3,\n    num_classes: int = 10,\n    act_layer: nn.Module | None = None,\n    norm_layer: Callable[[int], nn.Module] | None = None,\n    drop_layer: Callable[..., nn.Module] | None = None,\n    conv_layer: Callable[..., nn.Module] | None = None,\n) -&gt; None:\n    super().__init__()\n\n    if act_layer is None:\n        act_layer = nn.ReLU(inplace=True)\n\n    # Contracting path\n    self.encoder = nn.ModuleList([])\n    layout_ = [in_channels, *layout]\n    pool = False\n    for in_chan, out_chan in pairwise(layout_):\n        self.encoder.append(down_path(in_chan, out_chan, pool, 1, act_layer, norm_layer, drop_layer, conv_layer))\n        pool = True\n\n    self.bridge = nn.Sequential(\n        nn.MaxPool2d((2, 2)),\n        *conv_sequence(\n            layout[-1], 2 * layout[-1], act_layer, norm_layer, drop_layer, conv_layer, kernel_size=3, padding=1\n        ),\n        *conv_sequence(\n            2 * layout[-1], layout[-1], act_layer, norm_layer, drop_layer, conv_layer, kernel_size=3, padding=1\n        ),\n    )\n\n    # Expansive path\n    self.decoder = nn.ModuleList([])\n    layout_ = [layout[-1], *layout[1:][::-1]]\n    for left_chan, up_chan, num_cells in zip(layout[::-1], layout_, range(1, len(layout) + 1), strict=True):\n        self.decoder.append(\n            nn.ModuleList([\n                UpPath(left_chan + up_chan, left_chan, True, 1, act_layer, norm_layer, drop_layer, conv_layer)\n                for _ in range(num_cells)\n            ])\n        )\n\n    # Classifier\n    self.classifier = nn.Conv2d(layout[0], num_classes, 1)\n\n    init_module(self, \"relu\")\n</code></pre>"},{"location":"reference/models/models/#holocron.models.segmentation.UNetpp","title":"UNetpp","text":"<pre><code>UNetpp(layout: list[int], in_channels: int = 3, num_classes: int = 10, act_layer: Module | None = None, norm_layer: Callable[[int], Module] | None = None, drop_layer: Callable[..., Module] | None = None, conv_layer: Callable[..., Module] | None = None)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Implements a UNet++ architecture</p> PARAMETER DESCRIPTION <code>layout</code> <p>number of channels after each contracting block</p> <p> TYPE: <code>list[int]</code> </p> <code>in_channels</code> <p>number of channels in the input tensor</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>number of output classes</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>act_layer</code> <p>activation layer</p> <p> TYPE: <code>Module | None</code> DEFAULT: <code>None</code> </p> <code>norm_layer</code> <p>normalization layer</p> <p> TYPE: <code>Callable[[int], Module] | None</code> DEFAULT: <code>None</code> </p> <code>drop_layer</code> <p>dropout layer</p> <p> TYPE: <code>Callable[..., Module] | None</code> DEFAULT: <code>None</code> </p> <code>conv_layer</code> <p>convolutional layer</p> <p> TYPE: <code>Callable[..., Module] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>holocron/models/segmentation/unetpp.py</code> <pre><code>def __init__(\n    self,\n    layout: list[int],\n    in_channels: int = 3,\n    num_classes: int = 10,\n    act_layer: nn.Module | None = None,\n    norm_layer: Callable[[int], nn.Module] | None = None,\n    drop_layer: Callable[..., nn.Module] | None = None,\n    conv_layer: Callable[..., nn.Module] | None = None,\n) -&gt; None:\n    super().__init__()\n\n    if act_layer is None:\n        act_layer = nn.ReLU(inplace=True)\n\n    # Contracting path\n    self.encoder = nn.ModuleList([])\n    layout_ = [in_channels, *layout]\n    pool = False\n    for in_chan, out_chan in pairwise(layout_):\n        self.encoder.append(down_path(in_chan, out_chan, pool, 1, act_layer, norm_layer, drop_layer, conv_layer))\n        pool = True\n\n    self.bridge = nn.Sequential(\n        nn.MaxPool2d((2, 2)),\n        *conv_sequence(\n            layout[-1], 2 * layout[-1], act_layer, norm_layer, drop_layer, conv_layer, kernel_size=3, padding=1\n        ),\n        *conv_sequence(\n            2 * layout[-1], layout[-1], act_layer, norm_layer, drop_layer, conv_layer, kernel_size=3, padding=1\n        ),\n    )\n\n    # Expansive path\n    self.decoder = nn.ModuleList([])\n    layout_ = [layout[-1], *layout[1:][::-1]]\n    for left_chan, up_chan, num_cells in zip(layout[::-1], layout_, range(1, len(layout) + 1), strict=True):\n        self.decoder.append(\n            nn.ModuleList([\n                UpPath(\n                    up_chan + (idx + 1) * left_chan,\n                    left_chan,\n                    True,\n                    1,\n                    act_layer,\n                    norm_layer,\n                    drop_layer,\n                    conv_layer,\n                )\n                for idx in range(num_cells)\n            ])\n        )\n\n    # Classifier\n    self.classifier = nn.Conv2d(layout[0], num_classes, 1)\n\n    init_module(self, \"relu\")\n</code></pre>"},{"location":"reference/models/models/#holocron.models.segmentation.unetp","title":"unetp","text":"<pre><code>unetp(pretrained: bool = False, progress: bool = True, **kwargs: Any) -&gt; UNetp\n</code></pre> <p>UNet+ from \"UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation\"</p> <p></p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on PASCAL VOC2012</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>UNetp</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>UNetp</code> <p>semantic segmentation model</p> Source code in <code>holocron/models/segmentation/unetpp.py</code> <pre><code>def unetp(pretrained: bool = False, progress: bool = True, **kwargs: Any) -&gt; UNetp:\n    \"\"\"UNet+ from [\"UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation\"](https://arxiv.org/pdf/1912.05074.pdf)\n\n    ![UNet+ architecture](https://github.com/frgfm/Holocron/releases/download/v0.1.3/unetp.png)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on PASCAL VOC2012\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`UNetp`][holocron.models.segmentation.unetpp.UNetp]\n\n    Returns:\n        semantic segmentation model\n    \"\"\"\n    return _unet(\"unetp\", pretrained, progress, **kwargs)  # type: ignore[return-value]\n</code></pre>"},{"location":"reference/models/models/#holocron.models.segmentation.UNet3p","title":"UNet3p","text":"<pre><code>UNet3p(layout: list[int], in_channels: int = 3, num_classes: int = 10, act_layer: Module | None = None, norm_layer: Callable[[int], Module] | None = None, drop_layer: Callable[..., Module] | None = None, conv_layer: Callable[..., Module] | None = None)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Implements a UNet3+ architecture</p> PARAMETER DESCRIPTION <code>layout</code> <p>number of channels after each contracting block</p> <p> TYPE: <code>list[int]</code> </p> <code>in_channels</code> <p>number of channels in the input tensor</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>number of output classes</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>act_layer</code> <p>activation layer</p> <p> TYPE: <code>Module | None</code> DEFAULT: <code>None</code> </p> <code>norm_layer</code> <p>normalization layer</p> <p> TYPE: <code>Callable[[int], Module] | None</code> DEFAULT: <code>None</code> </p> <code>drop_layer</code> <p>dropout layer</p> <p> TYPE: <code>Callable[..., Module] | None</code> DEFAULT: <code>None</code> </p> <code>conv_layer</code> <p>convolutional layer</p> <p> TYPE: <code>Callable[..., Module] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>holocron/models/segmentation/unet3p.py</code> <pre><code>def __init__(\n    self,\n    layout: list[int],\n    in_channels: int = 3,\n    num_classes: int = 10,\n    act_layer: nn.Module | None = None,\n    norm_layer: Callable[[int], nn.Module] | None = None,\n    drop_layer: Callable[..., nn.Module] | None = None,\n    conv_layer: Callable[..., nn.Module] | None = None,\n) -&gt; None:\n    super().__init__()\n\n    if act_layer is None:\n        act_layer = nn.ReLU(inplace=True)\n    if norm_layer is None:\n        norm_layer = nn.BatchNorm2d\n\n    # Contracting path\n    self.encoder = nn.ModuleList([])\n    layout_ = [in_channels, *layout]\n    pool = False\n    for in_chan, out_chan in pairwise(layout_):\n        self.encoder.append(down_path(in_chan, out_chan, pool, 1, act_layer, norm_layer, drop_layer, conv_layer))\n        pool = True\n\n    # Expansive path\n    self.decoder = nn.ModuleList([])\n    for row in range(len(layout) - 1):\n        self.decoder.append(\n            FSAggreg(\n                layout[:row],\n                layout[row],\n                [len(layout) * layout[0]] * (len(layout) - 2 - row) + layout[-1:],\n                act_layer,\n                norm_layer,\n                drop_layer,\n                conv_layer,\n            )\n        )\n\n    # Classifier\n    self.classifier = nn.Conv2d(len(layout) * layout[0], num_classes, 1)\n\n    init_module(self, \"relu\")\n</code></pre>"},{"location":"reference/models/classification/convnext/","title":"ConvNeXt","text":"<p>The ConvNeXt model is based on the \"A ConvNet for the 2020s\" paper.</p>"},{"location":"reference/models/classification/convnext/#architecture-overview","title":"Architecture overview","text":"<p>This architecture compiles tricks from transformer-based vision models to improve a pure convolutional model.</p> <p></p> <p>The key takeaways from the paper are the following:</p> <ul> <li>update the stem convolution to act like a patchify layer of transformers</li> <li>increase block kernel size to 7</li> <li>switch to depth-wise convolutions</li> <li>reduce the amount of activations and normalization layers</li> </ul>"},{"location":"reference/models/classification/convnext/#model-builders","title":"Model builders","text":"<p>The following model builders can be used to instantiate a ConvNeXt model, with or without pre-trained weights. All the model builders internally rely on the <code>ConvNeXt</code> base class.</p>"},{"location":"reference/models/classification/convnext/#holocron.models.classification.ConvNeXt","title":"ConvNeXt","text":"<pre><code>ConvNeXt(num_blocks: list[int], planes: list[int], num_classes: int = 10, in_channels: int = 3, conv_layer: Callable[..., Module] | None = None, act_layer: Module | None = None, norm_layer: Callable[[int], Module] | None = None, drop_layer: Callable[..., Module] | None = None, stochastic_depth_prob: float = 0.0)\n</code></pre> <p>               Bases: <code>Sequential</code></p> Source code in <code>holocron/models/classification/convnext.py</code> <pre><code>def __init__(\n    self,\n    num_blocks: list[int],\n    planes: list[int],\n    num_classes: int = 10,\n    in_channels: int = 3,\n    conv_layer: Callable[..., nn.Module] | None = None,\n    act_layer: nn.Module | None = None,\n    norm_layer: Callable[[int], nn.Module] | None = None,\n    drop_layer: Callable[..., nn.Module] | None = None,\n    stochastic_depth_prob: float = 0.0,\n) -&gt; None:\n    if conv_layer is None:\n        conv_layer = nn.Conv2d\n    if norm_layer is None:\n        norm_layer = partial(LayerNorm2d, eps=1e-6)\n    if act_layer is None:\n        act_layer = nn.GELU()\n    self.dilation: int = 1\n\n    # Patchify-like stem\n    layers = conv_sequence(\n        in_channels,\n        planes[0],\n        None,\n        norm_layer,\n        drop_layer,\n        conv_layer,\n        kernel_size=4,\n        stride=4,\n        padding=0,\n        bias=True,\n    )\n\n    block_idx = 0\n    tot_blocks = sum(num_blocks)\n    for _num_blocks, _planes, _oplanes in zip(num_blocks, planes, [*planes[1:], planes[-1]], strict=True):\n        # adjust stochastic depth probability based on the depth of the stage block\n        sd_probs = [stochastic_depth_prob * (block_idx + _idx) / (tot_blocks - 1.0) for _idx in range(_num_blocks)]\n        stage: list[nn.Module] = [\n            Bottlenext(_planes, act_layer, norm_layer, drop_layer, stochastic_depth_prob=sd_prob)\n            for _idx, sd_prob in zip(range(_num_blocks), sd_probs, strict=True)\n        ]\n        if _planes != _oplanes:\n            stage.append(\n                nn.Sequential(\n                    LayerNorm2d(_planes),\n                    nn.Conv2d(_planes, _oplanes, kernel_size=2, stride=2),\n                )\n            )\n        layers.append(nn.Sequential(*stage))\n        block_idx += _num_blocks\n\n    super().__init__(\n        OrderedDict([\n            (\"features\", nn.Sequential(*layers)),\n            (\"pool\", GlobalAvgPool2d(flatten=True)),\n            (\n                \"head\",\n                nn.Sequential(\n                    nn.LayerNorm(planes[-1], eps=1e-6),\n                    nn.Linear(planes[-1], num_classes),\n                ),\n            ),\n        ])\n    )\n\n    # Init all layers\n    for m in self.modules():\n        if isinstance(m, (nn.Conv2d, nn.Linear)):\n            nn.init.trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n</code></pre>"},{"location":"reference/models/classification/convnext/#holocron.models.classification.convnext_atto","title":"convnext_atto","text":"<pre><code>convnext_atto(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; ConvNeXt\n</code></pre> <p>ConvNeXt-Atto variant of Ross Wightman inspired by \"A ConvNet for the 2020s\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNette</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ConvNeXt</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ConvNeXt</code> <p>classification model</p> Source code in <code>holocron/models/classification/convnext.py</code> <pre><code>def convnext_atto(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; ConvNeXt:\n    \"\"\"ConvNeXt-Atto variant of Ross Wightman inspired by\n    [\"A ConvNet for the 2020s\"](https://arxiv.org/pdf/2201.03545.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNette\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ConvNeXt`][holocron.models.classification.convnext.ConvNeXt]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.ConvNeXt_Atto_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(\n        pretrained,\n        checkpoint,\n        ConvNeXt_Atto_Checkpoint.DEFAULT.value,\n    )\n    return _convnext(checkpoint, progress, [2, 2, 6, 2], [40, 80, 160, 320], **kwargs)\n</code></pre>"},{"location":"reference/models/classification/convnext/#holocron.models.ConvNeXt_Atto_Checkpoint","title":"ConvNeXt_Atto_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/convnext/#holocron.models.ConvNeXt_Atto_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='convnext_atto', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/convnext_atto_224-f38217e7.pth', acc1=0.8759, acc5=0.9832, sha256='f38217e7361060e6fe00e8fa95b0e8774150190eed9e55c812bbd3b6ab378ce9', size=13535258, num_params=3377730, commit='d4a59999179b42fc0d3058ac6b76cc41f49dd56e', train_args='./imagenette2-320/ --arch convnext_atto --batch-size 64 --mixup-alpha 0.2 --amp --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/convnext/#holocron.models.ConvNeXt_Atto_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENETTE\n</code></pre>"},{"location":"reference/models/classification/convnext/#holocron.models.classification.convnext_femto","title":"convnext_femto","text":"<pre><code>convnext_femto(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; ConvNeXt\n</code></pre> <p>ConvNeXt-Femto variant of Ross Wightman inspired by \"A ConvNet for the 2020s\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNette</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ConvNeXt</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ConvNeXt</code> <p>classification model</p> Source code in <code>holocron/models/classification/convnext.py</code> <pre><code>def convnext_femto(\n    pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any\n) -&gt; ConvNeXt:\n    \"\"\"ConvNeXt-Femto variant of Ross Wightman inspired by\n    [\"A ConvNet for the 2020s\"](https://arxiv.org/pdf/2201.03545.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNette\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ConvNeXt`][holocron.models.classification.convnext.ConvNeXt]\n\n    Returns:\n        classification model\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(pretrained, checkpoint, None)\n    return _convnext(checkpoint, progress, [2, 2, 6, 2], [48, 96, 192, 384], **kwargs)\n</code></pre>"},{"location":"reference/models/classification/convnext/#holocron.models.classification.convnext_pico","title":"convnext_pico","text":"<pre><code>convnext_pico(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; ConvNeXt\n</code></pre> <p>ConvNeXt-Pico variant of Ross Wightman inspired by \"A ConvNet for the 2020s\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNette</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ConvNeXt</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ConvNeXt</code> <p>classification model</p> Source code in <code>holocron/models/classification/convnext.py</code> <pre><code>def convnext_pico(\n    pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any\n) -&gt; ConvNeXt:\n    \"\"\"ConvNeXt-Pico variant of Ross Wightman inspired by\n    [\"A ConvNet for the 2020s\"](https://arxiv.org/pdf/2201.03545.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNette\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ConvNeXt`][holocron.models.classification.convnext.ConvNeXt]\n\n    Returns:\n        classification model\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(pretrained, checkpoint, None)\n    return _convnext(checkpoint, progress, [2, 2, 6, 2], [64, 128, 256, 512], **kwargs)\n</code></pre>"},{"location":"reference/models/classification/convnext/#holocron.models.classification.convnext_nano","title":"convnext_nano","text":"<pre><code>convnext_nano(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; ConvNeXt\n</code></pre> <p>ConvNeXt-Nano variant of Ross Wightman inspired by \"A ConvNet for the 2020s\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNette</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ConvNeXt</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ConvNeXt</code> <p>classification model</p> Source code in <code>holocron/models/classification/convnext.py</code> <pre><code>def convnext_nano(\n    pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any\n) -&gt; ConvNeXt:\n    \"\"\"ConvNeXt-Nano variant of Ross Wightman inspired by\n    [\"A ConvNet for the 2020s\"](https://arxiv.org/pdf/2201.03545.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNette\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ConvNeXt`][holocron.models.classification.convnext.ConvNeXt]\n\n    Returns:\n        classification model\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(pretrained, checkpoint, None)\n    return _convnext(checkpoint, progress, [2, 2, 8, 2], [80, 160, 320, 640], **kwargs)\n</code></pre>"},{"location":"reference/models/classification/convnext/#holocron.models.classification.convnext_tiny","title":"convnext_tiny","text":"<pre><code>convnext_tiny(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; ConvNeXt\n</code></pre> <p>ConvNeXt-T from \"A ConvNet for the 2020s\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNette</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ConvNeXt</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ConvNeXt</code> <p>classification model</p> Source code in <code>holocron/models/classification/convnext.py</code> <pre><code>def convnext_tiny(\n    pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any\n) -&gt; ConvNeXt:\n    \"\"\"ConvNeXt-T from\n    [\"A ConvNet for the 2020s\"](https://arxiv.org/pdf/2201.03545.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNette\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ConvNeXt`][holocron.models.classification.convnext.ConvNeXt]\n\n    Returns:\n        classification model\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(pretrained, checkpoint, None)\n    return _convnext(checkpoint, progress, [3, 3, 9, 3], [96, 192, 384, 768], **kwargs)\n</code></pre>"},{"location":"reference/models/classification/convnext/#holocron.models.classification.convnext_small","title":"convnext_small","text":"<pre><code>convnext_small(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; ConvNeXt\n</code></pre> <p>ConvNeXt-S from \"A ConvNet for the 2020s\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNette</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ConvNeXt</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ConvNeXt</code> <p>classification model</p> Source code in <code>holocron/models/classification/convnext.py</code> <pre><code>def convnext_small(\n    pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any\n) -&gt; ConvNeXt:\n    \"\"\"ConvNeXt-S from\n    [\"A ConvNet for the 2020s\"](https://arxiv.org/pdf/2201.03545.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNette\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ConvNeXt`][holocron.models.classification.convnext.ConvNeXt]\n\n    Returns:\n        classification model\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(pretrained, checkpoint, None)\n    return _convnext(checkpoint, progress, [3, 3, 27, 3], [96, 192, 384, 768], **kwargs)\n</code></pre>"},{"location":"reference/models/classification/convnext/#holocron.models.classification.convnext_base","title":"convnext_base","text":"<pre><code>convnext_base(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; ConvNeXt\n</code></pre> <p>ConvNeXt-B from \"A ConvNet for the 2020s\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNette</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ConvNeXt</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ConvNeXt</code> <p>classification model</p> Source code in <code>holocron/models/classification/convnext.py</code> <pre><code>def convnext_base(\n    pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any\n) -&gt; ConvNeXt:\n    \"\"\"ConvNeXt-B from\n    [\"A ConvNet for the 2020s\"](https://arxiv.org/pdf/2201.03545.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNette\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ConvNeXt`][holocron.models.classification.convnext.ConvNeXt]\n\n    Returns:\n        classification model\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(pretrained, checkpoint, None)\n    return _convnext(checkpoint, progress, [3, 3, 27, 3], [128, 256, 512, 1024], **kwargs)\n</code></pre>"},{"location":"reference/models/classification/convnext/#holocron.models.classification.convnext_large","title":"convnext_large","text":"<pre><code>convnext_large(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; ConvNeXt\n</code></pre> <p>ConvNeXt-L from \"A ConvNet for the 2020s\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNette</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ConvNeXt</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ConvNeXt</code> <p>classification model</p> Source code in <code>holocron/models/classification/convnext.py</code> <pre><code>def convnext_large(\n    pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any\n) -&gt; ConvNeXt:\n    \"\"\"ConvNeXt-L from\n    [\"A ConvNet for the 2020s\"](https://arxiv.org/pdf/2201.03545.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNette\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ConvNeXt`][holocron.models.classification.convnext.ConvNeXt]\n\n    Returns:\n        classification model\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(pretrained, checkpoint, None)\n    return _convnext(checkpoint, progress, [3, 3, 27, 3], [192, 384, 768, 1536], **kwargs)\n</code></pre>"},{"location":"reference/models/classification/convnext/#holocron.models.classification.convnext_xl","title":"convnext_xl","text":"<pre><code>convnext_xl(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; ConvNeXt\n</code></pre> <p>ConvNeXt-XL from \"A ConvNet for the 2020s\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNette</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ConvNeXt</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ConvNeXt</code> <p>classification model</p> Source code in <code>holocron/models/classification/convnext.py</code> <pre><code>def convnext_xl(\n    pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any\n) -&gt; ConvNeXt:\n    \"\"\"ConvNeXt-XL from\n    [\"A ConvNet for the 2020s\"](https://arxiv.org/pdf/2201.03545.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNette\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ConvNeXt`][holocron.models.classification.convnext.ConvNeXt]\n\n    Returns:\n        classification model\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(pretrained, checkpoint, None)\n    return _convnext(checkpoint, progress, [3, 3, 27, 3], [256, 512, 1024, 2048], **kwargs)\n</code></pre>"},{"location":"reference/models/classification/darknet/","title":"DarkNet","text":"<p>The DarkNet model is based on the \"You Only Look Once: Unified, Real-Time Object Detection\" paper.</p>"},{"location":"reference/models/classification/darknet/#architecture-overview","title":"Architecture overview","text":"<p>This paper introduces a highway network with powerful feature representation abilities.</p> <p>The key takeaways from the paper are the following:</p> <ul> <li>improves the Inception architecture by using conv1x1</li> <li>replaces ReLU by LeakyReLU</li> </ul>"},{"location":"reference/models/classification/darknet/#model-builders","title":"Model builders","text":"<p>The following model builders can be used to instantiate a DarknetV1 model, with or without pre-trained weights. All the model builders internally rely on the <code>DarknetV1</code> base class.</p>"},{"location":"reference/models/classification/darknet/#holocron.models.classification.DarknetV1","title":"DarknetV1","text":"<pre><code>DarknetV1(layout: list[list[int]], num_classes: int = 10, in_channels: int = 3, stem_channels: int = 64, act_layer: Module | None = None, norm_layer: Callable[[int], Module] | None = None, drop_layer: Callable[..., Module] | None = None, conv_layer: Callable[..., Module] | None = None)\n</code></pre> <p>               Bases: <code>Sequential</code></p> Source code in <code>holocron/models/classification/darknet.py</code> <pre><code>def __init__(\n    self,\n    layout: list[list[int]],\n    num_classes: int = 10,\n    in_channels: int = 3,\n    stem_channels: int = 64,\n    act_layer: nn.Module | None = None,\n    norm_layer: Callable[[int], nn.Module] | None = None,\n    drop_layer: Callable[..., nn.Module] | None = None,\n    conv_layer: Callable[..., nn.Module] | None = None,\n) -&gt; None:\n    super().__init__(\n        OrderedDict([\n            (\n                \"features\",\n                DarknetBodyV1(layout, in_channels, stem_channels, act_layer, norm_layer, drop_layer, conv_layer),\n            ),\n            (\"pool\", GlobalAvgPool2d(flatten=True)),\n            (\"classifier\", nn.Linear(layout[2][-1], num_classes)),\n        ])\n    )\n\n    init_module(self, \"leaky_relu\")\n</code></pre>"},{"location":"reference/models/classification/darknet/#holocron.models.classification.darknet24","title":"darknet24","text":"<pre><code>darknet24(pretrained: bool = False, progress: bool = True, **kwargs: Any) -&gt; DarknetV1\n</code></pre> <p>Darknet-24 from \"You Only Look Once: Unified, Real-Time Object Detection\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>DarknetV1</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>DarknetV1</code> <p>classification model</p> Source code in <code>holocron/models/classification/darknet.py</code> <pre><code>def darknet24(pretrained: bool = False, progress: bool = True, **kwargs: Any) -&gt; DarknetV1:\n    \"\"\"Darknet-24 from\n    [\"You Only Look Once: Unified, Real-Time Object Detection\"](https://pjreddie.com/media/files/papers/yolo_1.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`DarknetV1`][holocron.models.classification.darknet.DarknetV1]\n\n    Returns:\n        classification model\n    \"\"\"\n    return _darknet(\n        \"darknet24\",\n        pretrained,\n        progress,\n        [[192], [128, 256, 256, 512], [*([256, 512] * 4), 512, 1024], [512, 1024] * 2],\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/models/classification/darknetv2/","title":"DarkNetV2","text":"<p>The DarkNetV2 model is based on the \"YOLO9000: Better, Faster, Stronger\" paper.</p>"},{"location":"reference/models/classification/darknetv2/#architecture-overview","title":"Architecture overview","text":"<p>This paper improves its version version by adding more recent gradient flow facilitators.</p> <p>The key takeaways from the paper are the following:</p> <ul> <li>adds batch normalization layers compared to DarkNetV1</li> </ul>"},{"location":"reference/models/classification/darknetv2/#model-builders","title":"Model builders","text":"<p>The following model builders can be used to instantiate a DarknetV2 model, with or without pre-trained weights. All the model builders internally rely on the <code>DarknetV2</code> base class.</p>"},{"location":"reference/models/classification/darknetv2/#holocron.models.classification.DarknetV2","title":"DarknetV2","text":"<pre><code>DarknetV2(layout: list[tuple[int, int]], num_classes: int = 10, in_channels: int = 3, stem_channels: int = 32, act_layer: Module | None = None, norm_layer: Callable[[int], Module] | None = None, drop_layer: Callable[..., Module] | None = None, conv_layer: Callable[..., Module] | None = None)\n</code></pre> <p>               Bases: <code>Sequential</code></p> Source code in <code>holocron/models/classification/darknetv2.py</code> <pre><code>def __init__(\n    self,\n    layout: list[tuple[int, int]],\n    num_classes: int = 10,\n    in_channels: int = 3,\n    stem_channels: int = 32,\n    act_layer: nn.Module | None = None,\n    norm_layer: Callable[[int], nn.Module] | None = None,\n    drop_layer: Callable[..., nn.Module] | None = None,\n    conv_layer: Callable[..., nn.Module] | None = None,\n) -&gt; None:\n    super().__init__(\n        OrderedDict([\n            (\n                \"features\",\n                DarknetBodyV2(\n                    layout, in_channels, stem_channels, False, act_layer, norm_layer, drop_layer, conv_layer\n                ),\n            ),\n            (\"classifier\", nn.Conv2d(layout[-1][0], num_classes, 1)),\n            (\"pool\", GlobalAvgPool2d(flatten=True)),\n        ])\n    )\n\n    init_module(self, \"leaky_relu\")\n</code></pre>"},{"location":"reference/models/classification/darknetv2/#holocron.models.classification.darknet19","title":"darknet19","text":"<pre><code>darknet19(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; DarknetV2\n</code></pre> <p>Darknet-19 from \"YOLO9000: Better, Faster, Stronger\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNette</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>DarknetV2</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>DarknetV2</code> <p>classification model</p> Source code in <code>holocron/models/classification/darknetv2.py</code> <pre><code>def darknet19(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; DarknetV2:\n    \"\"\"Darknet-19 from\n    [\"YOLO9000: Better, Faster, Stronger\"](https://pjreddie.com/media/files/papers/YOLO9000.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNette\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`DarknetV2`][holocron.models.classification.darknetv2.DarknetV2]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.Darknet19_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(\n        pretrained,\n        checkpoint,\n        Darknet19_Checkpoint.DEFAULT.value,\n    )\n    return _darknet(checkpoint, progress, [(64, 0), (128, 1), (256, 1), (512, 2), (1024, 2)], **kwargs)\n</code></pre>"},{"location":"reference/models/classification/darknetv2/#holocron.models.Darknet19_Checkpoint","title":"Darknet19_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/darknetv2/#holocron.models.Darknet19_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='darknet19', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/darknet19_224-32fd3f97.pth', acc1=0.9386, acc5=0.9936, sha256='32fd3f979586556554652d650c44a59747c7762d81140cadbcd795179a3877ec', size=79387724, num_params=19827626, commit='6e32c5b578711a2ef3731a8f8c61760ed9f03e58', train_args='./imagenette2-320/ --arch darknet19 --batch-size 64 --mixup-alpha 0.2 --amp --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/darknetv2/#holocron.models.Darknet19_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENETTE\n</code></pre>"},{"location":"reference/models/classification/darknetv3/","title":"DarkNetV3","text":"<p>The DarkNetV3 model is based on the \"YOLOv3: An Incremental Improvement\" paper.</p>"},{"location":"reference/models/classification/darknetv3/#architecture-overview","title":"Architecture overview","text":"<p>This paper makes a more powerful version than its predecedors by increasing depth and using ResNet tricks.</p> <p>The key takeaways from the paper are the following:</p> <ul> <li>adds residual connection compared to DarkNetV2</li> </ul>"},{"location":"reference/models/classification/darknetv3/#model-builders","title":"Model builders","text":"<p>The following model builders can be used to instantiate a DarknetV3 model, with or without pre-trained weights. All the model builders internally rely on the <code>DarknetV3</code> base class.</p>"},{"location":"reference/models/classification/darknetv3/#holocron.models.classification.DarknetV3","title":"DarknetV3","text":"<pre><code>DarknetV3(layout: list[tuple[int, int]], num_classes: int = 10, in_channels: int = 3, stem_channels: int = 32, act_layer: Module | None = None, norm_layer: Callable[[int], Module] | None = None, drop_layer: Callable[..., Module] | None = None, conv_layer: Callable[..., Module] | None = None)\n</code></pre> <p>               Bases: <code>Sequential</code></p> Source code in <code>holocron/models/classification/darknetv3.py</code> <pre><code>def __init__(\n    self,\n    layout: list[tuple[int, int]],\n    num_classes: int = 10,\n    in_channels: int = 3,\n    stem_channels: int = 32,\n    act_layer: nn.Module | None = None,\n    norm_layer: Callable[[int], nn.Module] | None = None,\n    drop_layer: Callable[..., nn.Module] | None = None,\n    conv_layer: Callable[..., nn.Module] | None = None,\n) -&gt; None:\n    super().__init__(\n        OrderedDict([\n            (\n                \"features\",\n                DarknetBodyV3(layout, in_channels, stem_channels, 1, act_layer, norm_layer, drop_layer, conv_layer),\n            ),\n            (\"pool\", GlobalAvgPool2d(flatten=True)),\n            (\"classifier\", nn.Linear(layout[-1][0], num_classes)),\n        ])\n    )\n\n    init_module(self, \"leaky_relu\")\n</code></pre>"},{"location":"reference/models/classification/darknetv3/#holocron.models.classification.darknet53","title":"darknet53","text":"<pre><code>darknet53(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; DarknetV3\n</code></pre> <p>Darknet-53 from \"YOLOv3: An Incremental Improvement\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>DarknetV3</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>DarknetV3</code> <p>classification model</p> Source code in <code>holocron/models/classification/darknetv3.py</code> <pre><code>def darknet53(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; DarknetV3:\n    \"\"\"Darknet-53 from\n    [\"YOLOv3: An Incremental Improvement\"](https://pjreddie.com/media/files/papers/YOLOv3.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`DarknetV3`][holocron.models.classification.darknetv3.DarknetV3]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.Darknet53_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(\n        pretrained,\n        checkpoint,\n        Darknet53_Checkpoint.DEFAULT.value,\n    )\n    return _darknet(checkpoint, progress, [(64, 1), (128, 2), (256, 8), (512, 8), (1024, 4)], **kwargs)\n</code></pre>"},{"location":"reference/models/classification/darknetv3/#holocron.models.Darknet53_Checkpoint","title":"Darknet53_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/darknetv3/#holocron.models.Darknet53_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='darknet53', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/darknet53_224-5015f3fd.pth', acc1=0.9417, acc5=0.9957, sha256='5015f3fdf0963342e0c54790127350375ba269d871feed48f8328b2e43cf7819', size=162584273, num_params=40595178, commit='6e32c5b578711a2ef3731a8f8c61760ed9f03e58', train_args='./imagenette2-320/ --arch darknet53 --batch-size 64 --mixup-alpha 0.2 --amp --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/darknetv3/#holocron.models.Darknet53_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENETTE\n</code></pre>"},{"location":"reference/models/classification/darknetv4/","title":"DarkNetV4","text":"<p>The DarkNetV4 model is based on the \"CSPNet: A New Backbone that can Enhance Learning Capability of CNN\" paper.</p>"},{"location":"reference/models/classification/darknetv4/#architecture-overview","title":"Architecture overview","text":"<p>This paper makes a more powerful version than its predecedors by increasing depth and using ResNet tricks.</p> <p>The key takeaways from the paper are the following:</p> <ul> <li>add cross-path connections to its predecessors</li> <li>explores newer non-linearities</li> </ul>"},{"location":"reference/models/classification/darknetv4/#model-builders","title":"Model builders","text":"<p>The following model builders can be used to instantiate a DarknetV3 model, with or without pre-trained weights. All the model builders internally rely on the <code>DarknetV4</code> base class.</p>"},{"location":"reference/models/classification/darknetv4/#holocron.models.classification.DarknetV4","title":"DarknetV4","text":"<pre><code>DarknetV4(layout: list[tuple[int, int]], num_classes: int = 10, in_channels: int = 3, stem_channels: int = 32, num_features: int = 1, act_layer: Module | None = None, norm_layer: Callable[[int], Module] | None = None, drop_layer: Callable[..., Module] | None = None, conv_layer: Callable[..., Module] | None = None)\n</code></pre> <p>               Bases: <code>Sequential</code></p> Source code in <code>holocron/models/classification/darknetv4.py</code> <pre><code>def __init__(\n    self,\n    layout: list[tuple[int, int]],\n    num_classes: int = 10,\n    in_channels: int = 3,\n    stem_channels: int = 32,\n    num_features: int = 1,\n    act_layer: nn.Module | None = None,\n    norm_layer: Callable[[int], nn.Module] | None = None,\n    drop_layer: Callable[..., nn.Module] | None = None,\n    conv_layer: Callable[..., nn.Module] | None = None,\n) -&gt; None:\n    super().__init__(\n        OrderedDict([\n            (\n                \"features\",\n                DarknetBodyV4(\n                    layout,\n                    in_channels,\n                    stem_channels,\n                    num_features,\n                    act_layer,\n                    norm_layer,\n                    drop_layer,\n                    conv_layer,\n                ),\n            ),\n            (\"pool\", GlobalAvgPool2d(flatten=True)),\n            (\"classifier\", nn.Linear(layout[-1][0], num_classes)),\n        ])\n    )\n\n    init_module(self, \"leaky_relu\")\n</code></pre>"},{"location":"reference/models/classification/darknetv4/#holocron.models.classification.cspdarknet53","title":"cspdarknet53","text":"<pre><code>cspdarknet53(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; DarknetV4\n</code></pre> <p>CSP-Darknet-53 from \"CSPNet: A New Backbone that can Enhance Learning Capability of CNN\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>DarknetV4</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>DarknetV4</code> <p>classification model</p> Source code in <code>holocron/models/classification/darknetv4.py</code> <pre><code>def cspdarknet53(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; DarknetV4:\n    \"\"\"CSP-Darknet-53 from\n    [\"CSPNet: A New Backbone that can Enhance Learning Capability of CNN\"](https://arxiv.org/pdf/1911.11929.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`DarknetV4`][holocron.models.classification.darknetv4.DarknetV4]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.CSPDarknet53_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(\n        pretrained,\n        checkpoint,\n        CSPDarknet53_Checkpoint.DEFAULT.value,\n    )\n    return _darknet(checkpoint, progress, [(64, 1), (128, 2), (256, 8), (512, 8), (1024, 4)], **kwargs)\n</code></pre>"},{"location":"reference/models/classification/darknetv4/#holocron.models.CSPDarknet53_Checkpoint","title":"CSPDarknet53_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/darknetv4/#holocron.models.CSPDarknet53_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='cspdarknet53', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/cspdarknet53_224-7a69463a.pth', acc1=0.945, acc5=0.9964, sha256='7a69463a4bd445beb6691dfd6ef7378efcf941f75d07d60034106ebedfcb82f8', size=106732575, num_params=26627434, commit='6e32c5b578711a2ef3731a8f8c61760ed9f03e58', train_args='./imagenette2-320/ --arch cspdarknet53 --batch-size 64 --mixup-alpha 0.2 --amp --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/darknetv4/#holocron.models.CSPDarknet53_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENETTE\n</code></pre>"},{"location":"reference/models/classification/darknetv4/#holocron.models.classification.cspdarknet53_mish","title":"cspdarknet53_mish","text":"<pre><code>cspdarknet53_mish(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; DarknetV4\n</code></pre> <p>Modified version of CSP-Darknet-53 from \"CSPNet: A New Backbone that can Enhance Learning Capability of CNN\" with Mish as activation layer and DropBlock as regularization layer.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>DarknetV4</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>DarknetV4</code> <p>classification model</p> Source code in <code>holocron/models/classification/darknetv4.py</code> <pre><code>def cspdarknet53_mish(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; DarknetV4:\n    \"\"\"Modified version of CSP-Darknet-53 from\n    [\"CSPNet: A New Backbone that can Enhance Learning Capability of CNN\"](https://arxiv.org/pdf/1911.11929.pdf)\n    with Mish as activation layer and DropBlock as regularization layer.\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`DarknetV4`][holocron.models.classification.darknetv4.DarknetV4]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.CSPDarknet53_Mish_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    kwargs[\"act_layer\"] = nn.Mish(inplace=True)\n    kwargs[\"drop_layer\"] = DropBlock2d\n\n    checkpoint = _handle_legacy_pretrained(\n        pretrained,\n        checkpoint,\n        CSPDarknet53_Mish_Checkpoint.DEFAULT.value,\n    )\n    return _darknet(checkpoint, progress, [(64, 1), (128, 2), (256, 8), (512, 8), (1024, 4)], **kwargs)\n</code></pre>"},{"location":"reference/models/classification/darknetv4/#holocron.models.CSPDarknet53_Mish_Checkpoint","title":"CSPDarknet53_Mish_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/darknetv4/#holocron.models.CSPDarknet53_Mish_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='cspdarknet53_mish', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/cspdarknet53_mish_224-1b660b3c.pth', acc1=0.9465, acc5=0.9969, sha256='1b660b3cb144195100c99ee3b9b863c37a5b5a59619c8de8c588b3d2af954b15', size=106737530, num_params=26627434, commit='6e32c5b578711a2ef3731a8f8c61760ed9f03e58', train_args='./imagenette2-320/ --arch cspdarknet53_mish --batch-size 32 --grad-acc 2 --mixup-alpha 0.2 --amp  --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/darknetv4/#holocron.models.CSPDarknet53_Mish_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENETTE\n</code></pre>"},{"location":"reference/models/classification/mobileone/","title":"MobileOne","text":"<p>The ResNet model is based on the \"An Improved One millisecond Mobile Backbone\" paper.</p>"},{"location":"reference/models/classification/mobileone/#architecture-overview","title":"Architecture overview","text":"<p>This architecture optimizes the model for inference speed at inference time on mobile device.</p> <p></p> <p>The key takeaways from the paper are the following:</p> <ul> <li>reuse the reparametrization concept of RepVGG while adding overparametrization in the block branches.</li> <li>each block is composed of two consecutive reparametrizeable blocks (in a similar fashion than RepVGG): a depth-wise convolutional block, a point-wise convolutional block.</li> </ul>"},{"location":"reference/models/classification/mobileone/#model-builders","title":"Model builders","text":"<p>The following model builders can be used to instantiate a MobileOne model, with or without pre-trained weights. All the model builders internally rely on the <code>MobileOne</code> base class.</p>"},{"location":"reference/models/classification/mobileone/#holocron.models.classification.MobileOne","title":"MobileOne","text":"<pre><code>MobileOne(num_blocks: list[int], width_multipliers: list[float], overparam_factor: int = 1, num_classes: int = 10, in_channels: int = 3, act_layer: Module | None = None, norm_layer: Callable[[int], Module] | None = None)\n</code></pre> <p>               Bases: <code>Sequential</code></p> Source code in <code>holocron/models/classification/mobileone.py</code> <pre><code>def __init__(\n    self,\n    num_blocks: list[int],\n    width_multipliers: list[float],\n    overparam_factor: int = 1,\n    num_classes: int = 10,\n    in_channels: int = 3,\n    act_layer: nn.Module | None = None,\n    norm_layer: Callable[[int], nn.Module] | None = None,\n) -&gt; None:\n    if norm_layer is None:\n        norm_layer = nn.BatchNorm2d\n    if act_layer is None:\n        act_layer = nn.ReLU(inplace=True)\n\n    base_planes = [64, 128, 256, 512]\n    planes = [round(mult * chans) for mult, chans in zip(width_multipliers, base_planes, strict=True)]\n\n    in_planes = min(64, planes[0])\n    # Stem\n    layers: list[nn.Module] = [MobileOneBlock(in_channels, in_planes, overparam_factor, 2, act_layer, norm_layer)]\n\n    # Consecutive convolutional blocks\n    for _num_blocks, _planes in zip(num_blocks, planes, strict=True):\n        # Stride &amp; channel changes\n        stage = [MobileOneBlock(in_planes, _planes, overparam_factor, 2, act_layer, norm_layer)]\n        # Depth\n        stage.extend([\n            MobileOneBlock(_planes, _planes, overparam_factor, 1, act_layer, norm_layer)\n            for _ in range(_num_blocks - 1)\n        ])\n        in_planes = _planes\n\n        layers.append(nn.Sequential(*stage))\n\n    super().__init__(\n        OrderedDict([\n            (\"features\", nn.Sequential(*layers)),\n            (\"pool\", GlobalAvgPool2d(flatten=True)),\n            (\"head\", nn.Linear(in_planes, num_classes)),\n        ])\n    )\n\n    # Init all layers\n    init.init_module(self, nonlinearity=\"relu\")\n</code></pre>"},{"location":"reference/models/classification/mobileone/#holocron.models.classification.MobileOne.reparametrize","title":"reparametrize","text":"<pre><code>reparametrize() -&gt; None\n</code></pre> <p>Reparametrize the block by fusing convolutions and BN in each branch, then fusing all branches</p> Source code in <code>holocron/models/classification/mobileone.py</code> <pre><code>def reparametrize(self) -&gt; None:\n    \"\"\"Reparametrize the block by fusing convolutions and BN in each branch, then fusing all branches\"\"\"\n    self.features: nn.Sequential\n    # Stem\n    self.features[0].reparametrize()\n    for stage in self.features[1:]:\n        for block in stage:\n            block.reparametrize()\n</code></pre>"},{"location":"reference/models/classification/mobileone/#holocron.models.classification.mobileone_s0","title":"mobileone_s0","text":"<pre><code>mobileone_s0(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; MobileOne\n</code></pre> <p>MobileOne-S0 from \"An Improved One millisecond Mobile Backbone\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>MobileOne</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>MobileOne</code> <p>classification model</p> Source code in <code>holocron/models/classification/mobileone.py</code> <pre><code>def mobileone_s0(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; MobileOne:\n    \"\"\"MobileOne-S0 from\n    [\"An Improved One millisecond Mobile Backbone\"](https://arxiv.org/pdf/2206.04040.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`MobileOne`][holocron.models.MobileOne]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.MobileOne_S0_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(\n        pretrained,\n        checkpoint,\n        MobileOne_S0_Checkpoint.DEFAULT,  # type: ignore[arg-type]\n    )\n    return _mobileone(checkpoint, progress, [0.75, 1.0, 1.0, 2.0], 4, **kwargs)\n</code></pre>"},{"location":"reference/models/classification/mobileone/#holocron.models.MobileOne_S0_Checkpoint","title":"MobileOne_S0_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/mobileone/#holocron.models.MobileOne_S0_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='mobileone_s0', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/mobileone_s0_224-9ddd1fe9.pth', acc1=0.8808, acc5=0.9883, sha256='9ddd1fe9d6c0a73d3c4d51d3c967a8a27ff5e545705afc557b4d4ac0f34395cb', size=17708169, num_params=4277991, commit='d4a59999179b42fc0d3058ac6b76cc41f49dd56e', train_args='./imagenette2-320/ --arch mobileone_s0 --batch-size 64 --mixup-alpha 0.2 --amp --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/mobileone/#holocron.models.MobileOne_S0_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENETTE\n</code></pre>"},{"location":"reference/models/classification/mobileone/#holocron.models.classification.mobileone_s1","title":"mobileone_s1","text":"<pre><code>mobileone_s1(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; MobileOne\n</code></pre> <p>MobileOne-S1 from \"An Improved One millisecond Mobile Backbone\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>MobileOne</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>MobileOne</code> <p>classification model</p> Source code in <code>holocron/models/classification/mobileone.py</code> <pre><code>def mobileone_s1(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; MobileOne:\n    \"\"\"MobileOne-S1 from\n    [\"An Improved One millisecond Mobile Backbone\"](https://arxiv.org/pdf/2206.04040.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`MobileOne`][holocron.models.MobileOne]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.MobileOne_S1_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(\n        pretrained,\n        checkpoint,\n        MobileOne_S1_Checkpoint.DEFAULT,  # type: ignore[arg-type]\n    )\n    return _mobileone(checkpoint, progress, [1.5, 1.5, 2.0, 2.5], 1, **kwargs)\n</code></pre>"},{"location":"reference/models/classification/mobileone/#holocron.models.MobileOne_S1_Checkpoint","title":"MobileOne_S1_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/mobileone/#holocron.models.MobileOne_S1_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='mobileone_s1', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/mobileone_s1_224-d4ec5433.pth', acc1=0.9126, acc5=0.9918, sha256='d4ec5433cff3d55d562b7a35fc0c95568ff8f4591bf822dd3e699535bdff90eb', size=14594817, num_params=3555188, commit='d4a59999179b42fc0d3058ac6b76cc41f49dd56e', train_args='./imagenette2-320/ --arch mobileone_s1 --batch-size 64 --mixup-alpha 0.2 --amp --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/mobileone/#holocron.models.MobileOne_S1_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENETTE\n</code></pre>"},{"location":"reference/models/classification/mobileone/#holocron.models.classification.mobileone_s2","title":"mobileone_s2","text":"<pre><code>mobileone_s2(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; MobileOne\n</code></pre> <p>MobileOne-S2 from \"An Improved One millisecond Mobile Backbone\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>MobileOne</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>MobileOne</code> <p>classification model</p> Source code in <code>holocron/models/classification/mobileone.py</code> <pre><code>def mobileone_s2(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; MobileOne:\n    \"\"\"MobileOne-S2 from\n    [\"An Improved One millisecond Mobile Backbone\"](https://arxiv.org/pdf/2206.04040.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`MobileOne`][holocron.models.classification.mobileone.MobileOne]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.MobileOne_S2_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(\n        pretrained,\n        checkpoint,\n        MobileOne_S2_Checkpoint.DEFAULT,  # type: ignore[arg-type]\n    )\n    return _mobileone(checkpoint, progress, [1.5, 2.0, 2.5, 4.0], 1, **kwargs)\n</code></pre>"},{"location":"reference/models/classification/mobileone/#holocron.models.MobileOne_S2_Checkpoint","title":"MobileOne_S2_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/mobileone/#holocron.models.MobileOne_S2_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='mobileone_s2', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/mobileone_s2_224-b748859c.pth', acc1=0.9131, acc5=0.9921, sha256='b748859c45a636ea22f0f68a3b7e75e5fb6ffb31178a5a3137931a21b4c41697', size=23866479, num_params=5854324, commit='d4a59999179b42fc0d3058ac6b76cc41f49dd56e', train_args='./imagenette2-320/ --arch mobileone_s2 --batch-size 64 --mixup-alpha 0.2 --amp --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/mobileone/#holocron.models.MobileOne_S2_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENETTE\n</code></pre>"},{"location":"reference/models/classification/mobileone/#holocron.models.classification.mobileone_s3","title":"mobileone_s3","text":"<pre><code>mobileone_s3(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; MobileOne\n</code></pre> <p>MobileOne-S3 from \"An Improved One millisecond Mobile Backbone\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>MobileOne</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>MobileOne</code> <p>classification model</p> Source code in <code>holocron/models/classification/mobileone.py</code> <pre><code>def mobileone_s3(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; MobileOne:\n    \"\"\"MobileOne-S3 from\n    [\"An Improved One millisecond Mobile Backbone\"](https://arxiv.org/pdf/2206.04040.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`MobileOne`][holocron.models.classification.mobileone.MobileOne]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.MobileOne_S3_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(\n        pretrained,\n        checkpoint,\n        MobileOne_S3_Checkpoint.DEFAULT,  # type: ignore[arg-type]\n    )\n    return _mobileone(checkpoint, progress, [2.0, 2.5, 3.0, 4.0], 1, **kwargs)\n</code></pre>"},{"location":"reference/models/classification/mobileone/#holocron.models.MobileOne_S3_Checkpoint","title":"MobileOne_S3_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/mobileone/#holocron.models.MobileOne_S3_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='mobileone_s3', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/mobileone_s3_224-7f357baf.pth', acc1=0.9106, acc5=0.9931, sha256='7f357baf0754136b4a02e7aec4129874db93ee462f43588b77def730db0b2bca', size=33080943, num_params=8140276, commit='d4a59999179b42fc0d3058ac6b76cc41f49dd56e', train_args='./imagenette2-320/ --arch mobileone_s3 --batch-size 64 --mixup-alpha 0.2 --amp --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/mobileone/#holocron.models.MobileOne_S3_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENETTE\n</code></pre>"},{"location":"reference/models/classification/pyconv_resnet/","title":"PyConvResNet","text":"<p>The PyConvResNet model is based on the \"Pyramidal Convolution: Rethinking Convolutional Neural Networks for Visual Recognition\" paper.</p>"},{"location":"reference/models/classification/pyconv_resnet/#architecture-overview","title":"Architecture overview","text":"<p>This paper explores an alternative approach for convolutional block in a pyramidal fashion.</p> <p></p> <p>The key takeaways from the paper are the following:</p> <ul> <li>replaces standard convolutions with pyramidal convolutions</li> <li>extends kernel size while increasing group size to balance the number of operations</li> </ul>"},{"location":"reference/models/classification/pyconv_resnet/#model-builders","title":"Model builders","text":"<p>The following model builders can be used to instantiate a PyConvResNet model, with or without pre-trained weights. All the model builders internally rely on the <code>ResNet</code> base class.</p>"},{"location":"reference/models/classification/pyconv_resnet/#holocron.models.classification.pyconv_resnet50","title":"pyconv_resnet50","text":"<pre><code>pyconv_resnet50(pretrained: bool = False, progress: bool = True, **kwargs: Any) -&gt; ResNet\n</code></pre> <p>PyConvResNet-50 from \"Pyramidal Convolution: Rethinking Convolutional Neural Networks for Visual Recognition\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ResNet</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ResNet</code> <p>classification model</p> Source code in <code>holocron/models/classification/pyconv_resnet.py</code> <pre><code>def pyconv_resnet50(pretrained: bool = False, progress: bool = True, **kwargs: Any) -&gt; ResNet:\n    \"\"\"PyConvResNet-50 from [\"Pyramidal Convolution: Rethinking Convolutional Neural Networks\n    for Visual Recognition\"](https://arxiv.org/pdf/2006.11538.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ResNet`][holocron.models.classification.resnet.ResNet]\n\n    Returns:\n        classification model\n    \"\"\"\n    return _pyconvresnet(\n        \"pyconv_resnet50\",\n        pretrained,\n        progress,\n        PyBottleneck,\n        [3, 4, 6, 3],\n        [64, 128, 256, 512],\n        64,\n        [[1, 4, 8, 16], [1, 4, 8], [1, 4], [1]],\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/models/classification/pyconv_resnet/#holocron.models.classification.pyconvhg_resnet50","title":"pyconvhg_resnet50","text":"<pre><code>pyconvhg_resnet50(pretrained: bool = False, progress: bool = True, **kwargs: Any) -&gt; ResNet\n</code></pre> <p>PyConvHGResNet-50 from \"Pyramidal Convolution: Rethinking Convolutional Neural Networks for Visual Recognition\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ResNet</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ResNet</code> <p>classification model</p> Source code in <code>holocron/models/classification/pyconv_resnet.py</code> <pre><code>def pyconvhg_resnet50(pretrained: bool = False, progress: bool = True, **kwargs: Any) -&gt; ResNet:\n    \"\"\"PyConvHGResNet-50 from [\"Pyramidal Convolution: Rethinking Convolutional Neural Networks\n    for Visual Recognition\"](https://arxiv.org/pdf/2006.11538.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ResNet`][holocron.models.classification.resnet.ResNet]\n\n    Returns:\n        classification model\n    \"\"\"\n    return _pyconvresnet(\n        \"pyconvhg_resnet50\",\n        pretrained,\n        progress,\n        PyHGBottleneck,\n        [3, 4, 6, 3],\n        [128, 256, 512, 1024],\n        2,\n        [[32, 32, 32, 32], [32, 64, 64], [32, 64], [32]],\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/models/classification/repvgg/","title":"RepVGG","text":"<p>The ResNet model is based on the \"RepVGG: Making VGG-style ConvNets Great Again\" paper.</p>"},{"location":"reference/models/classification/repvgg/#architecture-overview","title":"Architecture overview","text":"<p>This paper revisits the VGG architecture by adapting its parameter setting in training and inference mode to combine the original VGG speed and the block design of ResNet.</p> <p></p> <p>The key takeaways from the paper are the following:</p> <ul> <li>have different block architectures between training and inference modes</li> <li>the block is designed in a similar fashion as a ResNet bottleneck but in a way that all branches can be fused into a single one</li> <li>The more complex training architecture improves gradient flow and overall optimization, while its inference counterpart is optimized for minimum latency and memory usage</li> </ul>"},{"location":"reference/models/classification/repvgg/#model-builders","title":"Model builders","text":"<p>The following model builders can be used to instantiate a RepVGG model, with or without pre-trained weights. All the model builders internally rely on the <code>RepVGG</code> base class.</p>"},{"location":"reference/models/classification/repvgg/#holocron.models.classification.RepVGG","title":"RepVGG","text":"<pre><code>RepVGG(num_blocks: list[int], planes: list[int], width_multiplier: float, final_width_multiplier: float, num_classes: int = 10, in_channels: int = 3, act_layer: Module | None = None, norm_layer: Callable[[int], Module] | None = None)\n</code></pre> <p>               Bases: <code>Sequential</code></p> <p>Implements a reparametrized version of VGG as described in <code>\"RepVGG: Making VGG-style ConvNets Great Again\" &lt;https://arxiv.org/pdf/2101.03697.pdf&gt;</code>_</p> PARAMETER DESCRIPTION <code>num_blocks</code> <p>list of number of blocks per stage</p> <p> TYPE: <code>list[int]</code> </p> <code>planes</code> <p>list of output channels of each stage</p> <p> TYPE: <code>list[int]</code> </p> <code>width_multiplier</code> <p>multiplier for the output channels of all stages apart from the last</p> <p> TYPE: <code>float</code> </p> <code>final_width_multiplier</code> <p>multiplier for the output channels of the last stage</p> <p> TYPE: <code>float</code> </p> <code>num_classes</code> <p>number of output classes</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>in_channels</code> <p>number of input channels</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>act_layer</code> <p>the activation layer to use</p> <p> TYPE: <code>Module | None</code> DEFAULT: <code>None</code> </p> <code>norm_layer</code> <p>the normalization layer to use</p> <p> TYPE: <code>Callable[[int], Module] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>holocron/models/classification/repvgg.py</code> <pre><code>def __init__(\n    self,\n    num_blocks: list[int],\n    planes: list[int],\n    width_multiplier: float,\n    final_width_multiplier: float,\n    num_classes: int = 10,\n    in_channels: int = 3,\n    act_layer: nn.Module | None = None,\n    norm_layer: Callable[[int], nn.Module] | None = None,\n) -&gt; None:\n    if norm_layer is None:\n        norm_layer = nn.BatchNorm2d\n    if act_layer is None:\n        act_layer = nn.ReLU(inplace=True)\n\n    if len(num_blocks) != len(planes):\n        raise AssertionError(\"the length of `num_blocks` and `planes` are expected to be the same\")\n\n    stages: list[nn.Sequential] = []\n    # Assign the width multipliers\n    chans = [in_channels, int(min(1, width_multiplier) * planes[0])]\n    chans.extend([int(width_multiplier * chan) for chan in planes[1:-1]])\n    chans.append(int(final_width_multiplier * planes[-1]))\n\n    # Build the layers\n    for nb_blocks, in_chan, out_chan in zip(num_blocks, chans[:-1], chans[1:], strict=True):\n        layers = [RepBlock(in_chan, out_chan, 2, False, act_layer, norm_layer)]\n        layers.extend([RepBlock(out_chan, out_chan, 1, True, act_layer, norm_layer) for _ in range(nb_blocks)])\n        stages.append(nn.Sequential(*layers))\n\n    super().__init__(\n        OrderedDict([\n            (\"features\", nn.Sequential(*stages)),\n            (\"pool\", GlobalAvgPool2d(flatten=True)),\n            (\"head\", nn.Linear(chans[-1], num_classes)),\n        ])\n    )\n    # Init all layers\n    init.init_module(self, nonlinearity=\"relu\")\n</code></pre>"},{"location":"reference/models/classification/repvgg/#holocron.models.classification.RepVGG.reparametrize","title":"reparametrize","text":"<pre><code>reparametrize() -&gt; None\n</code></pre> <p>Reparametrize the block by fusing convolutions and BN in each branch, then fusing all branches</p> Source code in <code>holocron/models/classification/repvgg.py</code> <pre><code>def reparametrize(self) -&gt; None:\n    \"\"\"Reparametrize the block by fusing convolutions and BN in each branch, then fusing all branches\"\"\"\n    self.features: nn.Sequential\n    for stage in self.features:\n        for block in stage:\n            block.reparametrize()\n</code></pre>"},{"location":"reference/models/classification/repvgg/#holocron.models.classification.repvgg_a0","title":"repvgg_a0","text":"<pre><code>repvgg_a0(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; RepVGG\n</code></pre> <p>RepVGG-A0 from \"RepVGG: Making VGG-style ConvNets Great Again\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNette</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>RepVGG</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>RepVGG</code> <p>classification model</p> Source code in <code>holocron/models/classification/repvgg.py</code> <pre><code>def repvgg_a0(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; RepVGG:\n    \"\"\"RepVGG-A0 from\n    [\"RepVGG: Making VGG-style ConvNets Great Again\"](https://arxiv.org/pdf/2101.03697.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNette\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`RepVGG`][holocron.models.classification.repvgg.RepVGG]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.RepVGG_A0_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(\n        pretrained,\n        checkpoint,\n        RepVGG_A0_Checkpoint.DEFAULT.value,\n    )\n    return _repvgg(checkpoint, progress, [1, 2, 4, 14, 1], 0.75, 2.5, **kwargs)\n</code></pre>"},{"location":"reference/models/classification/repvgg/#holocron.models.RepVGG_A0_Checkpoint","title":"RepVGG_A0_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/repvgg/#holocron.models.RepVGG_A0_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='repvgg_a0', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/repvgg_a0_224-d3f54b28.pth', acc1=0.9292, acc5=0.9946, sha256='d3f54b28567fcd7e3e32ffbcffb5bb5c64fd97b7139cba0bfe9ad0bd7765cdaa', size=99183419, num_params=24741642, commit='d4a59999179b42fc0d3058ac6b76cc41f49dd56e', train_args='./imagenette2-320/ --arch repvgg_a0 --batch-size 64 --mixup-alpha 0.2 --amp --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/repvgg/#holocron.models.RepVGG_A0_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENETTE\n</code></pre>"},{"location":"reference/models/classification/repvgg/#holocron.models.classification.repvgg_a1","title":"repvgg_a1","text":"<pre><code>repvgg_a1(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; RepVGG\n</code></pre> <p>RepVGG-A1 from \"RepVGG: Making VGG-style ConvNets Great Again\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNette</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>RepVGG</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>RepVGG</code> <p>classification model</p> Source code in <code>holocron/models/classification/repvgg.py</code> <pre><code>def repvgg_a1(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; RepVGG:\n    \"\"\"RepVGG-A1 from\n    [\"RepVGG: Making VGG-style ConvNets Great Again\"](https://arxiv.org/pdf/2101.03697.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNette\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`RepVGG`][holocron.models.classification.repvgg.RepVGG]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.RepVGG_A1_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(\n        pretrained,\n        checkpoint,\n        RepVGG_A1_Checkpoint.DEFAULT.value,\n    )\n    return _repvgg(checkpoint, progress, [1, 2, 4, 14, 1], 1, 2.5, **kwargs)\n</code></pre>"},{"location":"reference/models/classification/repvgg/#holocron.models.RepVGG_A1_Checkpoint","title":"RepVGG_A1_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/repvgg/#holocron.models.RepVGG_A1_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='repvgg_a1', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/repvgg_a1_224-8d3269fb.pth', acc1=0.9378, acc5=0.9918, sha256='8d3269fb5181c0fe75ef617872238135f3002f41e82e5ef7492d62a402ffae50', size=120724868, num_params=30119946, commit='d4a59999179b42fc0d3058ac6b76cc41f49dd56e', train_args='./imagenette2-320/ --arch repvgg_a1 --batch-size 64 --mixup-alpha 0.2 --amp --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/repvgg/#holocron.models.RepVGG_A1_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENETTE\n</code></pre>"},{"location":"reference/models/classification/repvgg/#holocron.models.classification.repvgg_a2","title":"repvgg_a2","text":"<pre><code>repvgg_a2(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; RepVGG\n</code></pre> <p>RepVGG-A2 from \"RepVGG: Making VGG-style ConvNets Great Again\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNette</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>RepVGG</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>RepVGG</code> <p>classification model</p> Source code in <code>holocron/models/classification/repvgg.py</code> <pre><code>def repvgg_a2(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; RepVGG:\n    \"\"\"RepVGG-A2 from\n    [\"RepVGG: Making VGG-style ConvNets Great Again\"](https://arxiv.org/pdf/2101.03697.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNette\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`RepVGG`][holocron.models.classification.repvgg.RepVGG]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.RepVGG_A2_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(\n        pretrained,\n        checkpoint,\n        RepVGG_A2_Checkpoint.DEFAULT.value,\n    )\n    return _repvgg(checkpoint, progress, [1, 2, 4, 14, 1], 1.5, 2.75, **kwargs)\n</code></pre>"},{"location":"reference/models/classification/repvgg/#holocron.models.RepVGG_A2_Checkpoint","title":"RepVGG_A2_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/repvgg/#holocron.models.RepVGG_A2_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='repvgg_a2', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/repvgg_a2_224-cb442207.pth', acc1=0.9363, acc5=0.9939, sha256='cb442207d0c4627e3a16d7a8b4bf5342a182fd924cf4a044ac3a832014e7d4cf', size=194822538, num_params=48629514, commit='d4a59999179b42fc0d3058ac6b76cc41f49dd56e', train_args='./imagenette2-320/ --arch repvgg_a2 --batch-size 64 --mixup-alpha 0.2 --amp --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/repvgg/#holocron.models.RepVGG_A2_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENETTE\n</code></pre>"},{"location":"reference/models/classification/repvgg/#holocron.models.classification.repvgg_b0","title":"repvgg_b0","text":"<pre><code>repvgg_b0(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; RepVGG\n</code></pre> <p>RepVGG-B0 from \"RepVGG: Making VGG-style ConvNets Great Again\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNette</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>RepVGG</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>RepVGG</code> <p>classification model</p> Source code in <code>holocron/models/classification/repvgg.py</code> <pre><code>def repvgg_b0(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; RepVGG:\n    \"\"\"RepVGG-B0 from\n    [\"RepVGG: Making VGG-style ConvNets Great Again\"](https://arxiv.org/pdf/2101.03697.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNette\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`RepVGG`][holocron.models.classification.repvgg.RepVGG]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.RepVGG_B0_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(\n        pretrained,\n        checkpoint,\n        RepVGG_B0_Checkpoint.DEFAULT.value,\n    )\n    return _repvgg(checkpoint, progress, [1, 4, 6, 16, 1], 1, 2.5, **kwargs)\n</code></pre>"},{"location":"reference/models/classification/repvgg/#holocron.models.RepVGG_B0_Checkpoint","title":"RepVGG_B0_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/repvgg/#holocron.models.RepVGG_B0_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='repvgg_b0', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/repvgg_b0_224-fdcdd2b7.pth', acc1=0.9269, acc5=0.9921, sha256='fdcdd2b739f19b47572be5a98ec407c08935d02adf1ab0bf90d7bc92c710fe2d', size=127668600, num_params=31845642, commit='d4a59999179b42fc0d3058ac6b76cc41f49dd56e', train_args='./imagenette2-320/ --arch repvgg_b0 --batch-size 64 --mixup-alpha 0.2 --amp --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/repvgg/#holocron.models.RepVGG_B0_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENETTE\n</code></pre>"},{"location":"reference/models/classification/repvgg/#holocron.models.classification.repvgg_b1","title":"repvgg_b1","text":"<pre><code>repvgg_b1(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; RepVGG\n</code></pre> <p>RepVGG-B1 from \"RepVGG: Making VGG-style ConvNets Great Again\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNette</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>RepVGG</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>RepVGG</code> <p>classification model</p> Source code in <code>holocron/models/classification/repvgg.py</code> <pre><code>def repvgg_b1(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; RepVGG:\n    \"\"\"RepVGG-B1 from\n    [\"RepVGG: Making VGG-style ConvNets Great Again\"](https://arxiv.org/pdf/2101.03697.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNette\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`RepVGG`][holocron.models.RepVGG]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.RepVGG_B1_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(\n        pretrained,\n        checkpoint,\n        RepVGG_B1_Checkpoint.DEFAULT.value,\n    )\n    return _repvgg(checkpoint, progress, [1, 4, 6, 16, 1], 2, 4, **kwargs)\n</code></pre>"},{"location":"reference/models/classification/repvgg/#holocron.models.RepVGG_B1_Checkpoint","title":"RepVGG_B1_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/repvgg/#holocron.models.RepVGG_B1_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='repvgg_b1', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/repvgg_b1_224-3e5b28d7.pth', acc1=0.9396, acc5=0.9939, sha256='3e5b28d7803965546efadeb20abb84d8fef765dd08170677467a9c06294224c4', size=403763795, num_params=100829194, commit='d4a59999179b42fc0d3058ac6b76cc41f49dd56e', train_args='./imagenette2-320/ --arch repvgg_b1 --batch-size 64 --mixup-alpha 0.2 --amp --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/repvgg/#holocron.models.RepVGG_B1_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENETTE\n</code></pre>"},{"location":"reference/models/classification/repvgg/#holocron.models.classification.repvgg_b2","title":"repvgg_b2","text":"<pre><code>repvgg_b2(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; RepVGG\n</code></pre> <p>RepVGG-B2 from \"RepVGG: Making VGG-style ConvNets Great Again\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNette</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>RepVGG</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>RepVGG</code> <p>classification model</p> Source code in <code>holocron/models/classification/repvgg.py</code> <pre><code>def repvgg_b2(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; RepVGG:\n    \"\"\"RepVGG-B2 from\n    [\"RepVGG: Making VGG-style ConvNets Great Again\"](https://arxiv.org/pdf/2101.03697.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNette\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`RepVGG`][holocron.models.classification.repvgg.RepVGG]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.RepVGG_B2_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(\n        pretrained,\n        checkpoint,\n        RepVGG_B2_Checkpoint.DEFAULT.value,\n    )\n    return _repvgg(checkpoint, progress, [1, 4, 6, 16, 1], 2.5, 5, **kwargs)\n</code></pre>"},{"location":"reference/models/classification/repvgg/#holocron.models.RepVGG_B2_Checkpoint","title":"RepVGG_B2_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/repvgg/#holocron.models.RepVGG_B2_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='repvgg_b2', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/repvgg_b2_224-dc810d88.pth', acc1=0.9414, acc5=0.9957, sha256='dc810d889e8533f3ab24d75d8bf4cec84380abfb3b10ee01009997eab6a35d4b', size=630382163, num_params=157462410, commit='d4a59999179b42fc0d3058ac6b76cc41f49dd56e', train_args='./imagenette2-320/ --arch repvgg_b2 --batch-size 32 --grad-acc 2 --mixup-alpha 0.2 --amp --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/repvgg/#holocron.models.RepVGG_B2_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENETTE\n</code></pre>"},{"location":"reference/models/classification/res2net/","title":"Res2Net","text":"<p>The Res2Net model is based on the \"Res2Net: A New Multi-scale Backbone Architecture\" paper.</p>"},{"location":"reference/models/classification/res2net/#architecture-overview","title":"Architecture overview","text":"<p>This paper replaces the bottleneck block of ResNet architectures by a multi-scale version.</p> <p></p> <p>The key takeaways from the paper are the following:</p> <ul> <li>switch to efficient multi-scale convolutions using a cascade of conv 3x3</li> <li>adapt the block for cardinality &amp; SE blocks</li> </ul>"},{"location":"reference/models/classification/res2net/#model-builders","title":"Model builders","text":"<p>The following model builders can be used to instantiate a Res2Net model, with or without pre-trained weights. All the model builders internally rely on the <code>ResNet</code> base class.</p>"},{"location":"reference/models/classification/res2net/#holocron.models.classification.res2net50_26w_4s","title":"res2net50_26w_4s","text":"<pre><code>res2net50_26w_4s(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; ResNet\n</code></pre> <p>Res2Net-50 26wx4s from \"Res2Net: A New Multi-scale Backbone Architecture\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ResNet</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ResNet</code> <p>classification model</p> Source code in <code>holocron/models/classification/res2net.py</code> <pre><code>def res2net50_26w_4s(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; ResNet:\n    \"\"\"Res2Net-50 26wx4s from\n    [\"Res2Net: A New Multi-scale Backbone Architecture\"](https://arxiv.org/pdf/1904.01169.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ResNet`][holocron.models.classification.resnet.ResNet]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.Res2Net50_26w_4s_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(\n        pretrained,\n        checkpoint,\n        Res2Net50_26w_4s_Checkpoint.DEFAULT.value,\n    )\n    return _res2net(checkpoint, progress, [3, 4, 6, 3], [64, 128, 256, 512], 26, 4, **kwargs)\n</code></pre>"},{"location":"reference/models/classification/res2net/#holocron.models.Res2Net50_26w_4s_Checkpoint","title":"Res2Net50_26w_4s_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/res2net/#holocron.models.Res2Net50_26w_4s_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='res2net50_26w_4s', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/res2net50_26w_4s_224-345170e8.pth', acc1=0.9394, acc5=0.9941, sha256='345170e8ff75d10330af55674090b0d9aa751e14b6f3b4a95bb8ea6cdd65be4b', size=95020747, num_params=23670610, commit='6e32c5b578711a2ef3731a8f8c61760ed9f03e58', train_args='./imagenette2-320/ --arch res2net50_26w_4s --batch-size 64 --mixup-alpha 0.2 --amp --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/res2net/#holocron.models.Res2Net50_26w_4s_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENETTE\n</code></pre>"},{"location":"reference/models/classification/resnet/","title":"ResNet","text":"<p>The ResNet model is based on the \"Deep Residual Learning for Image Recognition\" paper.</p>"},{"location":"reference/models/classification/resnet/#architecture-overview","title":"Architecture overview","text":"<p>This paper introduces a few tricks to maximize the depth of convolutional architectures that can be trained.</p> <p>The key takeaways from the paper are the following:</p> <ul> <li>add a shortcut connection in bottleneck blocks to ease the gradient flow</li> <li>extensive use of batch normalization layers</li> </ul>"},{"location":"reference/models/classification/resnet/#model-builders","title":"Model builders","text":"<p>The following model builders can be used to instantiate a ResNeXt model, with or without pre-trained weights. All the model builders internally rely on the <code>ResNet</code> base class.</p>"},{"location":"reference/models/classification/resnet/#holocron.models.classification.ResNet","title":"ResNet","text":"<pre><code>ResNet(block: type[BasicBlock | Bottleneck], num_blocks: list[int], planes: list[int], num_classes: int = 10, in_channels: int = 3, zero_init_residual: bool = False, width_per_group: int = 64, conv_layer: Callable[..., Module] | None = None, act_layer: Module | None = None, norm_layer: Callable[[int], Module] | None = None, drop_layer: Callable[..., Module] | None = None, deep_stem: bool = False, stem_pool: bool = True, avg_downsample: bool = False, num_repeats: int = 1, block_args: dict[str, Any] | list[dict[str, Any]] | None = None)\n</code></pre> <p>               Bases: <code>Sequential</code></p> Source code in <code>holocron/models/classification/resnet.py</code> <pre><code>def __init__(  # noqa: PLR0912\n    self,\n    block: type[BasicBlock | Bottleneck],\n    num_blocks: list[int],\n    planes: list[int],\n    num_classes: int = 10,\n    in_channels: int = 3,\n    zero_init_residual: bool = False,\n    width_per_group: int = 64,\n    conv_layer: Callable[..., nn.Module] | None = None,\n    act_layer: nn.Module | None = None,\n    norm_layer: Callable[[int], nn.Module] | None = None,\n    drop_layer: Callable[..., nn.Module] | None = None,\n    deep_stem: bool = False,\n    stem_pool: bool = True,\n    avg_downsample: bool = False,\n    num_repeats: int = 1,\n    block_args: dict[str, Any] | list[dict[str, Any]] | None = None,\n) -&gt; None:\n    if conv_layer is None:\n        conv_layer = nn.Conv2d\n    if norm_layer is None:\n        norm_layer = nn.BatchNorm2d\n    if act_layer is None:\n        act_layer = nn.ReLU(inplace=True)\n    self.dilation: int = 1\n\n    in_planes = 64\n    # Deep stem from ResNet-C\n    if deep_stem:\n        layers = [\n            *conv_sequence(\n                in_channels,\n                in_planes // 2,\n                act_layer,\n                norm_layer,\n                drop_layer,\n                conv_layer,\n                kernel_size=3,\n                stride=2,\n                padding=1,\n                bias=(norm_layer is None),\n            ),\n            *conv_sequence(\n                in_planes // 2,\n                in_planes // 2,\n                act_layer,\n                norm_layer,\n                drop_layer,\n                conv_layer,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n                bias=(norm_layer is None),\n            ),\n            *conv_sequence(\n                in_planes // 2,\n                in_planes,\n                act_layer,\n                norm_layer,\n                drop_layer,\n                conv_layer,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n                bias=(norm_layer is None),\n            ),\n        ]\n    else:\n        layers = conv_sequence(\n            in_channels,\n            in_planes,\n            act_layer,\n            norm_layer,\n            drop_layer,\n            conv_layer,\n            kernel_size=7,\n            stride=2,\n            padding=3,\n            bias=(norm_layer is None),\n        )\n    if stem_pool:\n        layers.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n\n    # Optional tensor repetitions along channel axis (mainly for TridentNet)\n    if num_repeats &gt; 1:\n        layers.append(ChannelRepeat(num_repeats))\n\n    # Consecutive convolutional blocks\n    stride = 1\n    # Block args\n    if block_args is None:\n        block_args = {\"groups\": 1}\n    if not isinstance(block_args, list):\n        block_args = [block_args] * len(num_blocks)\n    for _num_blocks, _planes, _block_args in zip(num_blocks, planes, block_args, strict=True):\n        layers.append(\n            self._make_layer(\n                block,\n                _num_blocks,\n                in_planes,\n                _planes,\n                stride,\n                width_per_group,\n                act_layer=act_layer,\n                norm_layer=norm_layer,\n                drop_layer=drop_layer,\n                avg_downsample=avg_downsample,\n                num_repeats=num_repeats,\n                block_args=_block_args,\n            )\n        )\n        in_planes = block.expansion * _planes\n        stride = 2\n\n    super().__init__(\n        OrderedDict([\n            (\"features\", nn.Sequential(*layers)),\n            (\"pool\", GlobalAvgPool2d(flatten=True)),\n            (\"head\", nn.Linear(num_repeats * in_planes, num_classes)),\n        ])\n    )\n\n    # Init all layers\n    init.init_module(self, nonlinearity=\"relu\")\n\n    # Init shortcut\n    if zero_init_residual:\n        for m in self.modules():\n            if isinstance(m, Bottleneck):\n                m.convs[2][1].weight.data.zero_()  # ty: ignore[non-subscriptable,possibly-missing-attribute]\n            elif isinstance(m, BasicBlock):\n                m.convs[1][1].weight.data.zero_()  # ty: ignore[non-subscriptable,possibly-missing-attribute]\n</code></pre>"},{"location":"reference/models/classification/resnet/#holocron.models.classification.resnet18","title":"resnet18","text":"<pre><code>resnet18(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; ResNet\n</code></pre> <p>ResNet-18 from \"Deep Residual Learning for Image Recognition\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, loads that checkpoint</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ResNet</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ResNet</code> <p>classification model</p> Source code in <code>holocron/models/classification/resnet.py</code> <pre><code>def resnet18(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; ResNet:\n    \"\"\"ResNet-18 from\n    [\"Deep Residual Learning for Image Recognition\"](https://arxiv.org/pdf/1512.03385.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        checkpoint: If specified, loads that checkpoint\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ResNet`][holocron.models.ResNet]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.ResNet18_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(\n        pretrained,\n        checkpoint,\n        ResNet18_Checkpoint.DEFAULT.value,\n    )\n    return _resnet(checkpoint, progress, BasicBlock, [2, 2, 2, 2], [64, 128, 256, 512], **kwargs)\n</code></pre>"},{"location":"reference/models/classification/resnet/#holocron.models.ResNet18_Checkpoint","title":"ResNet18_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/resnet/#holocron.models.ResNet18_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='resnet18', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/resnet18_224-fc07006c.pth', acc1=0.9361, acc5=0.9946, sha256='fc07006c894cac8cf380fed699bc5a68463698753c954632f52bb8595040f781', size=44787043, num_params=11181642, commit='6e32c5b578711a2ef3731a8f8c61760ed9f03e58', train_args='./imagenette2-320/ --arch resnet18 --batch-size 64 --mixup-alpha 0.2 --amp --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/resnet/#holocron.models.ResNet18_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENETTE\n</code></pre>"},{"location":"reference/models/classification/resnet/#holocron.models.classification.resnet34","title":"resnet34","text":"<pre><code>resnet34(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; ResNet\n</code></pre> <p>ResNet-34 from \"Deep Residual Learning for Image Recognition\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, load that checkpoint on the model</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ResNet</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ResNet</code> <p>classification model</p> Source code in <code>holocron/models/classification/resnet.py</code> <pre><code>def resnet34(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; ResNet:\n    \"\"\"ResNet-34 from\n    [\"Deep Residual Learning for Image Recognition\"](https://arxiv.org/pdf/1512.03385.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        checkpoint: If specified, load that checkpoint on the model\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ResNet`][holocron.models.ResNet]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.ResNet34_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    return _resnet(checkpoint, progress, BasicBlock, [3, 4, 6, 3], [64, 128, 256, 512], **kwargs)\n</code></pre>"},{"location":"reference/models/classification/resnet/#holocron.models.ResNet34_Checkpoint","title":"ResNet34_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/resnet/#holocron.models.ResNet34_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='resnet34', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/resnet34_224-412b0792.pth', acc1=0.9381, acc5=0.9949, sha256='412b07927cc1938ee3add8d0f6bb18b42786646182f674d75f1433d086914485', size=85267035, num_params=21289802, commit='6e32c5b578711a2ef3731a8f8c61760ed9f03e58', train_args='./imagenette2-320/ --arch resnet34 --batch-size 64 --mixup-alpha 0.2 --amp --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/resnet/#holocron.models.ResNet34_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENETTE\n</code></pre>"},{"location":"reference/models/classification/resnet/#holocron.models.classification.resnet50","title":"resnet50","text":"<pre><code>resnet50(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; ResNet\n</code></pre> <p>ResNet-50 from \"Deep Residual Learning for Image Recognition\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, load that checkpoint on the model</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ResNet</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ResNet</code> <p>classification model</p> Source code in <code>holocron/models/classification/resnet.py</code> <pre><code>def resnet50(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; ResNet:\n    \"\"\"ResNet-50 from\n    [\"Deep Residual Learning for Image Recognition\"](https://arxiv.org/pdf/1512.03385.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        checkpoint: If specified, load that checkpoint on the model\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ResNet`][holocron.models.ResNet]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.ResNet50_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(\n        pretrained,\n        checkpoint,\n        ResNet50_Checkpoint.DEFAULT.value,\n    )\n    return _resnet(checkpoint, progress, Bottleneck, [3, 4, 6, 3], [64, 128, 256, 512], **kwargs)\n</code></pre>"},{"location":"reference/models/classification/resnet/#holocron.models.ResNet50_Checkpoint","title":"ResNet50_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/resnet/#holocron.models.ResNet50_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='resnet50', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/resnet50_224-5b913f0b.pth', acc1=0.9378, acc5=0.9954, sha256='5b913f0b8148b483ba15541ab600cf354ca42b326e4896c4c3dbc51eb1e80e70', size=94384682, num_params=23528522, commit='6e32c5b578711a2ef3731a8f8c61760ed9f03e58', train_args='./imagenette2-320/ --arch resnet50 --batch-size 64 --mixup-alpha 0.2 --amp --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/resnet/#holocron.models.ResNet50_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENETTE\n</code></pre>"},{"location":"reference/models/classification/resnet/#holocron.models.classification.resnet50d","title":"resnet50d","text":"<pre><code>resnet50d(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; ResNet\n</code></pre> <p>ResNet-50-D from \"Bag of Tricks for Image Classification with Convolutional Neural Networks\" https://arxiv.org/pdf/1812.01187.pdf`_</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, load that checkpoint on the model</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ResNet</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ResNet</code> <p>classification model</p> Source code in <code>holocron/models/classification/resnet.py</code> <pre><code>def resnet50d(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; ResNet:\n    \"\"\"ResNet-50-D from\n    [\"Bag of Tricks for Image Classification with Convolutional Neural Networks\"](https://arxiv.org/pdf/1812.01187.pdf)\n    &lt;https://arxiv.org/pdf/1812.01187.pdf&gt;`_\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        checkpoint: If specified, load that checkpoint on the model\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ResNet`][holocron.models.classification.resnet.ResNet]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.ResNet50D_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    return _resnet(\n        checkpoint,\n        progress,\n        Bottleneck,\n        [3, 4, 6, 3],\n        [64, 128, 256, 512],\n        deep_stem=True,\n        avg_downsample=True,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/models/classification/resnet/#holocron.models.ResNet50D_Checkpoint","title":"ResNet50D_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/resnet/#holocron.models.ResNet50D_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='resnet50d', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/resnet50d_224-6218d936.pth', acc1=0.9465, acc5=0.9952, sha256='6218d936fa67c0047f1ec65564213db538aa826d84f2df1d4fa3224531376e6c', size=94464810, num_params=23547754, commit='6e32c5b578711a2ef3731a8f8c61760ed9f03e58', train_args='./imagenette2-320/ --arch resnet50d --batch-size 64 --mixup-alpha 0.2 --amp --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/resnet/#holocron.models.ResNet50D_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENETTE\n</code></pre>"},{"location":"reference/models/classification/resnet/#holocron.models.classification.resnet101","title":"resnet101","text":"<pre><code>resnet101(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; ResNet\n</code></pre> <p>ResNet-101 from \"Deep Residual Learning for Image Recognition\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, load that checkpoint on the model</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of _resnet</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ResNet</code> <p>torch.nn.Module: classification model</p> Source code in <code>holocron/models/classification/resnet.py</code> <pre><code>def resnet101(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; ResNet:\n    \"\"\"ResNet-101 from\n    [\"Deep Residual Learning for Image Recognition\"](https://arxiv.org/pdf/1512.03385.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        checkpoint: If specified, load that checkpoint on the model\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of _resnet\n\n    Returns:\n        torch.nn.Module: classification model\n    \"\"\"\n    return _resnet(checkpoint, progress, Bottleneck, [3, 4, 23, 3], [64, 128, 256, 512], **kwargs)\n</code></pre>"},{"location":"reference/models/classification/resnet/#holocron.models.classification.resnet152","title":"resnet152","text":"<pre><code>resnet152(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; ResNet\n</code></pre> <p>ResNet-152 from \"Deep Residual Learning for Image Recognition\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, load that checkpoint on the model</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ResNet</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ResNet</code> <p>classification model</p> Source code in <code>holocron/models/classification/resnet.py</code> <pre><code>def resnet152(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; ResNet:\n    \"\"\"ResNet-152 from\n    [\"Deep Residual Learning for Image Recognition\"](https://arxiv.org/pdf/1512.03385.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        checkpoint: If specified, load that checkpoint on the model\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ResNet`][holocron.models.classification.resnet.ResNet]\n\n    Returns:\n        classification model\n    \"\"\"\n    return _resnet(checkpoint, progress, Bottleneck, [3, 8, 86, 3], [64, 128, 256, 512], **kwargs)\n</code></pre>"},{"location":"reference/models/classification/resnext/","title":"ResNeXt","text":"<p>The ResNeXt model is based on the \"Aggregated Residual Transformations for Deep Neural Networks\" paper.</p>"},{"location":"reference/models/classification/resnext/#architecture-overview","title":"Architecture overview","text":"<p>This paper improves the ResNet architecture by increasing the width of bottleneck blocks</p> <p>The key takeaways from the paper are the following:</p> <ul> <li>increases the number of channels in bottlenecks</li> <li>switches to group convolutions to balance the number of operations</li> </ul>"},{"location":"reference/models/classification/resnext/#model-builders","title":"Model builders","text":"<p>The following model builders can be used to instantiate a ResNet model, with or without pre-trained weights. All the model builders internally rely on the <code>ResNet</code> base class.</p>"},{"location":"reference/models/classification/resnext/#holocron.models.classification.resnext50_32x4d","title":"resnext50_32x4d","text":"<pre><code>resnext50_32x4d(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; ResNet\n</code></pre> <p>ResNeXt-50 from \"Aggregated Residual Transformations for Deep Neural Networks\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, load that checkpoint on the model</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ResNet</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ResNet</code> <p>classification model</p> Source code in <code>holocron/models/classification/resnet.py</code> <pre><code>def resnext50_32x4d(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; ResNet:\n    \"\"\"ResNeXt-50 from\n    [\"Aggregated Residual Transformations for Deep Neural Networks\"](https://arxiv.org/pdf/1611.05431.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        checkpoint: If specified, load that checkpoint on the model\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ResNet`][holocron.models.ResNet]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.ResNeXt50_32x4d_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    kwargs[\"width_per_group\"] = 4\n    block_args = {\"groups\": 32}\n    return _resnet(\n        checkpoint,\n        progress,\n        Bottleneck,\n        [3, 4, 6, 3],\n        [64, 128, 256, 512],\n        block_args=block_args,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/models/classification/resnext/#holocron.models.ResNeXt50_32x4d_Checkpoint","title":"ResNeXt50_32x4d_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/resnext/#holocron.models.ResNeXt50_32x4d_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='resnext50_32x4d', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/resnext50_32x4d_224-5832c4ce.pth', acc1=0.9455, acc5=0.9949, sha256='5832c4ce33522a9eb7a8b5abe31cf30621721a92d4f99b4b332a007d81d071fe', size=92332638, num_params=23000394, commit='6e32c5b578711a2ef3731a8f8c61760ed9f03e58', train_args='./imagenette2-320/ --arch resnext50_32x4d --batch-size 64 --mixup-alpha 0.2 --amp --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/resnext/#holocron.models.ResNeXt50_32x4d_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENETTE\n</code></pre>"},{"location":"reference/models/classification/resnext/#holocron.models.classification.resnext101_32x8d","title":"resnext101_32x8d","text":"<pre><code>resnext101_32x8d(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; ResNet\n</code></pre> <p>ResNeXt-101 from \"Aggregated Residual Transformations for Deep Neural Networks\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, load that checkpoint on the model</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ResNet</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ResNet</code> <p>classification model</p> Source code in <code>holocron/models/classification/resnet.py</code> <pre><code>def resnext101_32x8d(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; ResNet:\n    \"\"\"ResNeXt-101 from\n    [\"Aggregated Residual Transformations for Deep Neural Networks\"](https://arxiv.org/pdf/1611.05431.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        checkpoint: If specified, load that checkpoint on the model\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ResNet`][holocron.models.classification.resnet.ResNet]\n\n    Returns:\n        classification model\n    \"\"\"\n    kwargs[\"width_per_group\"] = 8\n    block_args = {\"groups\": 32}\n    return _resnet(\n        checkpoint,\n        progress,\n        Bottleneck,\n        [3, 4, 23, 3],\n        [64, 128, 256, 512],\n        block_args=block_args,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/models/classification/rexnet/","title":"ReXNet","text":"<p>The ResNet model is based on the \"ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network\" paper.</p>"},{"location":"reference/models/classification/rexnet/#architecture-overview","title":"Architecture overview","text":"<p>This paper investigates the effect of channel configuration in convolutional bottlenecks.</p> <p>The key takeaways from the paper are the following:</p> <ul> <li>increasing the depth ratio of conv 1x1 and inverted bottlenecks</li> <li>replace ReLU6 with SiLU</li> </ul>"},{"location":"reference/models/classification/rexnet/#model-builders","title":"Model builders","text":"<p>The following model builders can be used to instantiate a ReXNet model, with or without pre-trained weights. All the model builders internally rely on the <code>ReXNet</code> base class.</p>"},{"location":"reference/models/classification/rexnet/#holocron.models.classification.ReXNet","title":"ReXNet","text":"<pre><code>ReXNet(width_mult: float = 1.0, depth_mult: float = 1.0, num_classes: int = 1000, in_channels: int = 3, in_planes: int = 16, final_planes: int = 180, use_se: bool = True, se_ratio: int = 12, dropout_ratio: float = 0.2, bn_momentum: float = 0.9, act_layer: Module | None = None, norm_layer: Callable[[int], Module] | None = None, drop_layer: Callable[..., Module] | None = None)\n</code></pre> <p>               Bases: <code>Sequential</code></p> <p>Mostly adapted from https://github.com/clovaai/rexnet/blob/master/rexnetv1.py</p> Source code in <code>holocron/models/classification/rexnet.py</code> <pre><code>def __init__(\n    self,\n    width_mult: float = 1.0,\n    depth_mult: float = 1.0,\n    num_classes: int = 1000,\n    in_channels: int = 3,\n    in_planes: int = 16,\n    final_planes: int = 180,\n    use_se: bool = True,\n    se_ratio: int = 12,\n    dropout_ratio: float = 0.2,\n    bn_momentum: float = 0.9,\n    act_layer: nn.Module | None = None,\n    norm_layer: Callable[[int], nn.Module] | None = None,\n    drop_layer: Callable[..., nn.Module] | None = None,\n) -&gt; None:\n    \"\"\"Mostly adapted from https://github.com/clovaai/rexnet/blob/master/rexnetv1.py\"\"\"\n    super().__init__()\n\n    if act_layer is None:\n        act_layer = nn.SiLU(inplace=True)\n    if norm_layer is None:\n        norm_layer = nn.BatchNorm2d\n\n    num_blocks = [1, 2, 2, 3, 3, 5]\n    strides = [1, 2, 2, 2, 1, 2]\n    num_blocks = [ceil(element * depth_mult) for element in num_blocks]\n    strides = functools.reduce(\n        operator.iadd, [[element] + [1] * (num_blocks[idx] - 1) for idx, element in enumerate(strides)], []\n    )\n    depth = sum(num_blocks)\n\n    stem_channel = 32 / width_mult if width_mult &lt; 1.0 else 32\n    inplanes = in_planes / width_mult if width_mult &lt; 1.0 else in_planes\n\n    # The following channel configuration is a simple instance to make each layer become an expand layer\n    chans = [round(width_mult * stem_channel)]\n    chans.extend([round(width_mult * (inplanes + idx * final_planes / depth)) for idx in range(depth)])\n\n    ses = [False] * (num_blocks[0] + num_blocks[1]) + [use_se] * sum(num_blocks[2:])\n\n    layers = conv_sequence(\n        in_channels,\n        chans[0],\n        act_layer,\n        norm_layer,\n        drop_layer,\n        kernel_size=3,\n        stride=2,\n        padding=1,\n        bias=(norm_layer is None),\n    )\n\n    t = 1\n    for in_c, c, s, se in zip(chans[:-1], chans[1:], strides, ses, strict=True):\n        layers.append(ReXBlock(in_channels=in_c, channels=c, t=t, stride=s, use_se=se, se_ratio=se_ratio))\n        t = 6\n\n    pen_channels = int(width_mult * 1280)\n    layers.extend(\n        conv_sequence(\n            chans[-1],\n            pen_channels,\n            act_layer,\n            norm_layer,\n            drop_layer,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=(norm_layer is None),\n        )\n    )\n\n    super().__init__(\n        OrderedDict([\n            (\"features\", nn.Sequential(*layers)),\n            (\"pool\", GlobalAvgPool2d(flatten=True)),\n            (\"head\", nn.Sequential(nn.Dropout(dropout_ratio), nn.Linear(pen_channels, num_classes))),\n        ])\n    )\n\n    # Init all layers\n    init.init_module(self, nonlinearity=\"relu\")\n</code></pre>"},{"location":"reference/models/classification/rexnet/#holocron.models.classification.rexnet1_0x","title":"rexnet1_0x","text":"<pre><code>rexnet1_0x(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; ReXNet\n</code></pre> <p>ReXNet-1.0x from \"ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNette</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ReXNet</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ReXNet</code> <p>classification model</p> Source code in <code>holocron/models/classification/rexnet.py</code> <pre><code>def rexnet1_0x(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; ReXNet:\n    \"\"\"ReXNet-1.0x from\n    [\"ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network\"](https://arxiv.org/pdf/2007.00992.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNette\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ReXNet`][holocron.models.classification.rexnet.ReXNet]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.ReXNet1_0x_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(\n        pretrained,\n        checkpoint,\n        ReXNet1_0x_Checkpoint.DEFAULT.value,\n    )\n    return _rexnet(checkpoint, progress, 1, 1, **kwargs)\n</code></pre>"},{"location":"reference/models/classification/rexnet/#holocron.models.ReXNet1_0x_Checkpoint","title":"ReXNet1_0x_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/rexnet/#holocron.models.ReXNet1_0x_Checkpoint.IMAGENET1K","title":"IMAGENET1K  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENET1K = _checkpoint(arch='rexnet1_0x', url='https://github.com/frgfm/Holocron/releases/download/v0.1.2/rexnet1_0x_224-ab7b9733.pth', dataset=IMAGENET1K, acc1=0.7786, acc5=0.9387, sha256='ab7b973341a59832099f6ee2a41eb51121b287ad4adaae8b2cd8dd92ef058f01', size=14351299, num_params=4796186)\n</code></pre>"},{"location":"reference/models/classification/rexnet/#holocron.models.ReXNet1_0x_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='rexnet1_0x', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/rexnet1_0x_224-7c19fd53.pth', acc1=0.9439, acc5=0.9962, sha256='7c19fd53a5433927e9b4b22fa9cb0833eb1e4c3254b4079b6818fce650a77943', size=14351299, num_params=3527996, commit='d4a59999179b42fc0d3058ac6b76cc41f49dd56e', train_args='./imagenette2-320/ --arch rexnet1_0x --batch-size 64 --mixup-alpha 0.2 --amp --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/rexnet/#holocron.models.ReXNet1_0x_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENET1K\n</code></pre>"},{"location":"reference/models/classification/rexnet/#holocron.models.classification.rexnet1_3x","title":"rexnet1_3x","text":"<pre><code>rexnet1_3x(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; ReXNet\n</code></pre> <p>ReXNet-1.3x from \"ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ReXNet</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ReXNet</code> <p>classification model</p> Source code in <code>holocron/models/classification/rexnet.py</code> <pre><code>def rexnet1_3x(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; ReXNet:\n    \"\"\"ReXNet-1.3x from\n    [\"ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network\"](https://arxiv.org/pdf/2007.00992.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ReXNet`][holocron.models.classification.rexnet.ReXNet]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.ReXNet1_3x_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(\n        pretrained,\n        checkpoint,\n        ReXNet1_3x_Checkpoint.DEFAULT.value,\n    )\n    return _rexnet(checkpoint, progress, 1.3, 1, **kwargs)\n</code></pre>"},{"location":"reference/models/classification/rexnet/#holocron.models.ReXNet1_3x_Checkpoint","title":"ReXNet1_3x_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/rexnet/#holocron.models.ReXNet1_3x_Checkpoint.IMAGENET1K","title":"IMAGENET1K  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENET1K = _checkpoint(arch='rexnet1_3x', url='https://github.com/frgfm/Holocron/releases/download/v0.1.2/rexnet1_3x_224-95479104.pth', dataset=IMAGENET1K, acc1=0.795, acc5=0.9468, sha256='95479104024ce294abbdd528df62bd1a23e67a9db2956e1d6cdb9a9759dc1c69', size=14351299, num_params=7556198)\n</code></pre>"},{"location":"reference/models/classification/rexnet/#holocron.models.ReXNet1_3x_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='rexnet1_3x', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/rexnet1_3x_224-cf85ae91.pth', acc1=0.9488, acc5=0.9939, sha256='cf85ae919cbc9484f9fa150106451f68d2e84c73f1927a1b80aeeaa243ccd65b', size=23920480, num_params=5907848, commit='d4a59999179b42fc0d3058ac6b76cc41f49dd56e', train_args='./imagenette2-320/ --arch rexnet1_3x --batch-size 64 --mixup-alpha 0.2 --amp --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/rexnet/#holocron.models.ReXNet1_3x_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENET1K\n</code></pre>"},{"location":"reference/models/classification/rexnet/#holocron.models.classification.rexnet1_5x","title":"rexnet1_5x","text":"<pre><code>rexnet1_5x(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; ReXNet\n</code></pre> <p>ReXNet-1.5x from \"ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ReXNet</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ReXNet</code> <p>classification model</p> Source code in <code>holocron/models/classification/rexnet.py</code> <pre><code>def rexnet1_5x(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; ReXNet:\n    \"\"\"ReXNet-1.5x from\n    [\"ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network\"](https://arxiv.org/pdf/2007.00992.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ReXNet`][holocron.models.classification.rexnet.ReXNet]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.ReXNet1_5x_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(\n        pretrained,\n        checkpoint,\n        ReXNet1_5x_Checkpoint.DEFAULT.value,\n    )\n    return _rexnet(checkpoint, progress, 1.5, 1, **kwargs)\n</code></pre>"},{"location":"reference/models/classification/rexnet/#holocron.models.ReXNet1_5x_Checkpoint","title":"ReXNet1_5x_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/rexnet/#holocron.models.ReXNet1_5x_Checkpoint.IMAGENET1K","title":"IMAGENET1K  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENET1K = _checkpoint(arch='rexnet1_5x', url='https://github.com/frgfm/Holocron/releases/download/v0.1.2/rexnet1_5x_224-c42a16ac.pth', dataset=IMAGENET1K, acc1=0.8031, acc5=0.9517, sha256='c42a16ac73470d64852b8317ba9e875c833595a90a086b90490a696db9bb6a96', size=14351299, num_params=9727562)\n</code></pre>"},{"location":"reference/models/classification/rexnet/#holocron.models.ReXNet1_5x_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='rexnet1_5x', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/rexnet1_5x_224-4b9d7a59.pth', acc1=0.9447, acc5=0.9962, sha256='4b9d7a5901da6c2b9386987a6120bc86089d84df7727e43b78a4dfe2fc1c719a', size=31625286, num_params=7825772, commit='d4a59999179b42fc0d3058ac6b76cc41f49dd56e', train_args='./imagenette2-320/ --arch rexnet1_5x --batch-size 64 --mixup-alpha 0.2 --amp --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/rexnet/#holocron.models.ReXNet1_5x_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENET1K\n</code></pre>"},{"location":"reference/models/classification/rexnet/#holocron.models.classification.rexnet2_0x","title":"rexnet2_0x","text":"<pre><code>rexnet2_0x(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; ReXNet\n</code></pre> <p>ReXNet-2.0x from \"ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ReXNet</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ReXNet</code> <p>classification model</p> Source code in <code>holocron/models/classification/rexnet.py</code> <pre><code>def rexnet2_0x(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; ReXNet:\n    \"\"\"ReXNet-2.0x from\n    [\"ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network\"](https://arxiv.org/pdf/2007.00992.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ReXNet`][holocron.models.ReXNet]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.ReXNet2_0x_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(\n        pretrained,\n        checkpoint,\n        ReXNet2_0x_Checkpoint.DEFAULT.value,\n    )\n    return _rexnet(checkpoint, progress, 2, 1, **kwargs)\n</code></pre>"},{"location":"reference/models/classification/rexnet/#holocron.models.ReXNet2_0x_Checkpoint","title":"ReXNet2_0x_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/rexnet/#holocron.models.ReXNet2_0x_Checkpoint.IMAGENET1K","title":"IMAGENET1K  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENET1K = _checkpoint(arch='rexnet2_0x', url='https://github.com/frgfm/Holocron/releases/download/v0.1.2/rexnet2_0x_224-c8802402.pth', dataset=IMAGENET1K, acc1=0.8031, acc5=0.9517, sha256='c8802402442551c77fe3874f84d4d7eb1bd67cce274375db11a869ed074a1089', size=14351299, num_params=16365244)\n</code></pre>"},{"location":"reference/models/classification/rexnet/#holocron.models.ReXNet2_0x_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='rexnet2_0x', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/rexnet2_0x_224-3f00641e.pth', acc1=0.9524, acc5=0.9957, sha256='3f00641e48a6d1d3c9794534eb372467e0730700498933c9e79e60c838671d13', size=55724412, num_params=13829854, commit='d4a59999179b42fc0d3058ac6b76cc41f49dd56e', train_args='./imagenette2-320/ --arch rexnet2_0x --batch-size 32 --grad-acc 2 --mixup-alpha 0.2 --amp --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/rexnet/#holocron.models.ReXNet2_0x_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENET1K\n</code></pre>"},{"location":"reference/models/classification/rexnet/#holocron.models.classification.rexnet2_2x","title":"rexnet2_2x","text":"<pre><code>rexnet2_2x(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; ReXNet\n</code></pre> <p>ReXNet-2.2x from \"ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ReXNet</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ReXNet</code> <p>classification model</p> Source code in <code>holocron/models/classification/rexnet.py</code> <pre><code>def rexnet2_2x(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; ReXNet:\n    \"\"\"ReXNet-2.2x from\n    [\"ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network\"](https://arxiv.org/pdf/2007.00992.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ReXNet`][holocron.models.classification.rexnet.ReXNet]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.ReXNet2_2x_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(\n        pretrained,\n        checkpoint,\n        ReXNet2_2x_Checkpoint.DEFAULT.value,\n    )\n    return _rexnet(checkpoint, progress, 2.2, 1, **kwargs)\n</code></pre>"},{"location":"reference/models/classification/rexnet/#holocron.models.ReXNet2_2x_Checkpoint","title":"ReXNet2_2x_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/rexnet/#holocron.models.ReXNet2_2x_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='rexnet2_2x', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/rexnet2_2x_224-b23b2847.pth', acc1=0.9544, acc5=0.9946, sha256='b23b28475329e413bfb491503460db8f47a838ec8dcdc5d13ade6f40ee5841a6', size=67217933, num_params=16694966, commit='d4a59999179b42fc0d3058ac6b76cc41f49dd56e', train_args='./imagenette2-320/ --arch rexnet2_2x --batch-size 32 --grad-acc 2 --mixup-alpha 0.2 --amp --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/rexnet/#holocron.models.ReXNet2_2x_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENETTE\n</code></pre>"},{"location":"reference/models/classification/sknet/","title":"SKNet","text":"<p>The ResNet model is based on the \"Selective Kernel Networks\" paper.</p>"},{"location":"reference/models/classification/sknet/#architecture-overview","title":"Architecture overview","text":"<p>This paper revisits the concept of having a dynamic receptive field selection in convolutional blocks.</p> <p></p> <p>The key takeaways from the paper are the following:</p> <ul> <li>performs convolutions with multiple kernel sizes</li> <li>implements a cross-channel attention mechanism</li> </ul>"},{"location":"reference/models/classification/sknet/#model-builders","title":"Model builders","text":"<p>The following model builders can be used to instantiate a SKNet model, with or without pre-trained weights. All the model builders internally rely on the <code>ResNet</code> base class.</p>"},{"location":"reference/models/classification/sknet/#holocron.models.classification.sknet50","title":"sknet50","text":"<pre><code>sknet50(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; ResNet\n</code></pre> <p>SKNet-50 from \"Selective Kernel Networks\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ResNet</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ResNet</code> <p>classification model</p> Source code in <code>holocron/models/classification/sknet.py</code> <pre><code>def sknet50(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; ResNet:\n    \"\"\"SKNet-50 from\n    [\"Selective Kernel Networks\"](https://arxiv.org/pdf/1903.06586.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ResNet`][holocron.models.classification.resnet.ResNet]\n\n    Returns:\n        classification model\n\n    ::: holocron.models.SKNet50_Checkpoint\n        options:\n            heading_level: 4\n            show_if_no_docstring: true\n    \"\"\"\n    checkpoint = _handle_legacy_pretrained(\n        pretrained,\n        checkpoint,\n        SKNet50_Checkpoint.DEFAULT.value,\n    )\n    return _sknet(checkpoint, progress, [3, 4, 6, 3], [64, 128, 256, 512], **kwargs)\n</code></pre>"},{"location":"reference/models/classification/sknet/#holocron.models.SKNet50_Checkpoint","title":"SKNet50_Checkpoint","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"reference/models/classification/sknet/#holocron.models.SKNet50_Checkpoint.IMAGENETTE","title":"IMAGENETTE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGENETTE = _checkpoint(arch='sknet50', url='https://github.com/frgfm/Holocron/releases/download/v0.2.1/sknet50_224-e2349031.pth', acc1=0.9437, acc5=0.9954, sha256='e2349031c838a4661cd729dbc7825605c9e0c966bd89bbcc9b39f0e324894d1f', size=141253623, num_params=35224394, commit='6e32c5b578711a2ef3731a8f8c61760ed9f03e58', train_args='./imagenette2-320/ --arch sknet50 --batch-size 64 --mixup-alpha 0.2 --amp --device 0 --epochs 100 --lr 1e-3 --label-smoothing 0.1 --random-erase 0.1 --train-crop-size 176 --val-resize-size 232 --opt adamw --weight-decay 5e-2')\n</code></pre>"},{"location":"reference/models/classification/sknet/#holocron.models.SKNet50_Checkpoint.DEFAULT","title":"DEFAULT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEFAULT = IMAGENETTE\n</code></pre>"},{"location":"reference/models/classification/sknet/#holocron.models.classification.sknet101","title":"sknet101","text":"<pre><code>sknet101(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; ResNet\n</code></pre> <p>SKNet-101 from \"Selective Kernel Networks\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ResNet</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ResNet</code> <p>classification model</p> Source code in <code>holocron/models/classification/sknet.py</code> <pre><code>def sknet101(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; ResNet:\n    \"\"\"SKNet-101 from\n    [\"Selective Kernel Networks\"](https://arxiv.org/pdf/1903.06586.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ResNet`][holocron.models.classification.resnet.ResNet]\n\n    Returns:\n        classification model\n    \"\"\"\n    return _sknet(checkpoint if pretrained else None, progress, [3, 4, 23, 3], [64, 128, 256, 512], **kwargs)\n</code></pre>"},{"location":"reference/models/classification/sknet/#holocron.models.classification.sknet152","title":"sknet152","text":"<pre><code>sknet152(pretrained: bool = False, checkpoint: Checkpoint | None = None, progress: bool = True, **kwargs: Any) -&gt; ResNet\n</code></pre> <p>SKNet-152 from \"Selective Kernel Networks\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>checkpoint</code> <p>If specified, the model's parameters will be set to the checkpoint's values</p> <p> TYPE: <code>Checkpoint | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ResNet</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ResNet</code> <p>classification model</p> Source code in <code>holocron/models/classification/sknet.py</code> <pre><code>def sknet152(\n    pretrained: bool = False,\n    checkpoint: Checkpoint | None = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -&gt; ResNet:\n    \"\"\"SKNet-152 from\n    [\"Selective Kernel Networks\"](https://arxiv.org/pdf/1903.06586.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        checkpoint: If specified, the model's parameters will be set to the checkpoint's values\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ResNet`][holocron.models.classification.resnet.ResNet]\n\n    Returns:\n        classification model\n    \"\"\"\n    return _sknet(checkpoint if pretrained else None, progress, [3, 8, 86, 3], [64, 128, 256, 512], **kwargs)\n</code></pre>"},{"location":"reference/models/classification/tridentnet/","title":"TridentNet","text":"<p>The ResNeXt model is based on the \"Scale-Aware Trident Networks for Object Detection\" paper.</p>"},{"location":"reference/models/classification/tridentnet/#architecture-overview","title":"Architecture overview","text":"<p>This paper replaces the bottleneck block of ResNet architectures by a multi-scale version.</p> <p></p> <p>The key takeaways from the paper are the following:</p> <ul> <li>switch bottleneck to a 3 branch system</li> <li>all parallel branches share the same parameters but using different dilation values</li> </ul>"},{"location":"reference/models/classification/tridentnet/#model-builders","title":"Model builders","text":"<p>The following model builders can be used to instantiate a TridentNet model, with or without pre-trained weights. All the model builders internally rely on the <code>ResNet</code> base class.</p>"},{"location":"reference/models/classification/tridentnet/#holocron.models.classification.tridentnet50","title":"tridentnet50","text":"<pre><code>tridentnet50(pretrained: bool = False, progress: bool = True, **kwargs: Any) -&gt; ResNet\n</code></pre> <p>TridentNet-50 from \"Scale-Aware Trident Networks for Object Detection\"</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pre-trained on ImageNet</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>progress</code> <p>If True, displays a progress bar of the download to stderr</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>keyword args of <code>ResNet</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ResNet</code> <p>classification model</p> Source code in <code>holocron/models/classification/tridentnet.py</code> <pre><code>def tridentnet50(pretrained: bool = False, progress: bool = True, **kwargs: Any) -&gt; ResNet:\n    \"\"\"TridentNet-50 from\n    [\"Scale-Aware Trident Networks for Object Detection\"](https://arxiv.org/pdf/1901.01892.pdf)\n\n    Args:\n        pretrained: If True, returns a model pre-trained on ImageNet\n        progress: If True, displays a progress bar of the download to stderr\n        kwargs: keyword args of [`ResNet`][holocron.models.classification.resnet.ResNet]\n\n    Returns:\n        classification model\n    \"\"\"\n    return _tridentnet(\"tridentnet50\", pretrained, progress, [3, 4, 6, 3], [64, 128, 256, 512], **kwargs)\n</code></pre>"}]}