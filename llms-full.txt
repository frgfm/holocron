# Holocron

> PyTorch implementations of recent Computer Vision tricks (ReXNet, RepVGG, Unet3p, YOLOv4, CIoU loss, AdaBelief, PolyLoss, MobileOne).

# Getting started

# Installation

## Virtual environment

Tip

You will need an environment manager, and I cannot recommend enough [uv](https://docs.astral.sh/uv/getting-started/installation/).

Create a virtual environment with your prefered Python version (3.11 or higher is required to use Holocron):

```bash
$ uv venv --python 3.11
```

```bash
$ uv pip install pylocron
```

```bash
$ uv pip install pylocron @ git+https://github.com/frgfm/holocron.git
```

## System installation

You'll need [Python](https://www.python.org/downloads/) 3.11 or higher, and a package installer like [uv](https://docs.astral.sh/uv/getting-started/installation/) or [pip](https://packaging.python.org/en/latest/tutorials/installing-packages/).

```bash
$ uv pip install --system pylocron
```

```bash
$ uv pip install --system pylocron @ git+https://github.com/frgfm/holocron.git
```

```bash
$ pip install pylocron
```

```bash
$ pip install pylocron @ git+https://github.com/frgfm/holocron.git
```

Info

Holocron is built on top of [PyTorch](https://github.com/pytorch/pytorch) which is a complex dependency. Proper installation depends on your system and available hardware. You can refer to [installation guide of uv](https://docs.astral.sh/uv/guides/integration/pytorch) which is quite detailed.

# Holocron Notebooks

Here are some notebooks compiled for users to better leverage the library capabilities:

| Notebook                                                                                                    | Description                                     |     |
| ----------------------------------------------------------------------------------------------------------- | ----------------------------------------------- | --- |
| [Quicktour](https://github.com/frgfm/notebooks/blob/main/holocron/quicktour.ipynb)                          | A presentation of the main features of Holocron |     |
| [HuggingFace Hub integration](https://github.com/frgfm/notebooks/blob/main/holocron/hf_hub.ipynb)           | Use HuggingFace model hub with Holocron         |     |
| [Image classification](https://github.com/frgfm/notebooks/blob/main/holocron/classification_training.ipynb) | How to train your own image classifier          |     |
# Package Reference

# holocron.models

The models subpackage contains definitions of models for addressing different tasks, including: image classification, pixelwise semantic segmentation, object detection, instance segmentation, person keypoint detection and video classification.

## Classification

Classification models expect a 4D image tensor as an input (N x C x H x W) and returns a 2D output (N x K). The output represents the classification scores for each output classes.

### Supported architectures

- [ResNet](../classification/resnet/)
- [ResNeXt](../classification/resnext/)
- [Res2Net](../classification/res2net/)
- [TridentNet](../classification/tridentnet/)
- [ConvNeXt](../classification/convnext/)
- [PyConvResNet](../classification/pyconv_resnet/)
- [ReXNet](../classification/rexnet/)
- [SKNet](../classification/sknet/)
- [DarkNet](../classification/darknet/)
- [DarkNetV2](../classification/darknetv2/)
- [DarkNetV3](../classification/darknetv3/)
- [DarkNetV4](../classification/darknetv4/)
- [RepVGG](../classification/repvgg/)
- [MobileOne](../classification/mobileone/)

### Available checkpoints

Here is the list of available checkpoints:

| **Checkpoint**                          | **Acc@1** | **Acc@5** | **Params** | **Size (MB)** |
| --------------------------------------- | --------- | --------- | ---------- | ------------- |
| CSPDarknet53_Checkpoint.IMAGENETTE      | 94.50%    | 99.64%    | 26.6M      | 101.8         |
| CSPDarknet53_Mish_Checkpoint.IMAGENETTE | 94.65%    | 99.69%    | 26.6M      | 101.8         |
| ConvNeXt_Atto_Checkpoint.IMAGENETTE     | 87.59%    | 98.32%    | 3.4M       | 12.9          |
| Darknet19_Checkpoint.IMAGENETTE         | 93.86%    | 99.36%    | 19.8M      | 75.7          |
| Darknet53_Checkpoint.IMAGENETTE         | 94.17%    | 99.57%    | 40.6M      | 155.1         |
| MobileOne_S0_Checkpoint.IMAGENETTE      | 88.08%    | 98.83%    | 4.3M       | 16.9          |
| MobileOne_S1_Checkpoint.IMAGENETTE      | 91.26%    | 99.18%    | 3.6M       | 13.9          |
| MobileOne_S2_Checkpoint.IMAGENETTE      | 91.31%    | 99.21%    | 5.9M       | 22.8          |
| MobileOne_S3_Checkpoint.IMAGENETTE      | 91.06%    | 99.31%    | 8.1M       | 31.5          |
| ReXNet1_0x_Checkpoint.IMAGENET1K        | 77.86%    | 93.87%    | 4.8M       | 13.7          |
| ReXNet1_0x_Checkpoint.IMAGENETTE        | 94.39%    | 99.62%    | 3.5M       | 13.7          |
| ReXNet1_3x_Checkpoint.IMAGENET1K        | 79.50%    | 94.68%    | 7.6M       | 13.7          |
| ReXNet1_3x_Checkpoint.IMAGENETTE        | 94.88%    | 99.39%    | 5.9M       | 22.8          |
| ReXNet1_5x_Checkpoint.IMAGENET1K        | 80.31%    | 95.17%    | 9.7M       | 13.7          |
| ReXNet1_5x_Checkpoint.IMAGENETTE        | 94.47%    | 99.62%    | 7.8M       | 30.2          |
| ReXNet2_0x_Checkpoint.IMAGENET1K        | 80.31%    | 95.17%    | 16.4M      | 13.7          |
| ReXNet2_0x_Checkpoint.IMAGENETTE        | 95.24%    | 99.57%    | 13.8M      | 53.1          |
| ReXNet2_2x_Checkpoint.IMAGENETTE        | 95.44%    | 99.46%    | 16.7M      | 64.1          |
| RepVGG_A0_Checkpoint.IMAGENETTE         | 92.92%    | 99.46%    | 24.7M      | 94.6          |
| RepVGG_A1_Checkpoint.IMAGENETTE         | 93.78%    | 99.18%    | 30.1M      | 115.1         |
| RepVGG_A2_Checkpoint.IMAGENETTE         | 93.63%    | 99.39%    | 48.6M      | 185.8         |
| RepVGG_B0_Checkpoint.IMAGENETTE         | 92.69%    | 99.21%    | 31.8M      | 121.8         |
| RepVGG_B1_Checkpoint.IMAGENETTE         | 93.96%    | 99.39%    | 100.8M     | 385.1         |
| RepVGG_B2_Checkpoint.IMAGENETTE         | 94.14%    | 99.57%    | 157.5M     | 601.2         |
| Res2Net50_26w_4s_Checkpoint.IMAGENETTE  | 93.94%    | 99.41%    | 23.7M      | 90.6          |
| ResNeXt50_32x4d_Checkpoint.IMAGENETTE   | 94.55%    | 99.49%    | 23.0M      | 88.1          |
| ResNet18_Checkpoint.IMAGENETTE          | 93.61%    | 99.46%    | 11.2M      | 42.7          |
| ResNet34_Checkpoint.IMAGENETTE          | 93.81%    | 99.49%    | 21.3M      | 81.3          |
| ResNet50D_Checkpoint.IMAGENETTE         | 94.65%    | 99.52%    | 23.5M      | 90.1          |
| ResNet50_Checkpoint.IMAGENETTE          | 93.78%    | 99.54%    | 23.5M      | 90            |
| SKNet50_Checkpoint.IMAGENETTE           | 94.37%    | 99.54%    | 35.2M      | 134.7         |

## Object Detection

Object detection models expect a 4D image tensor as an input (N x C x H x W) and returns a list of dictionaries. Each dictionary has 3 keys: box coordinates, classification probability, classification label.

```python
import holocron.models as models
yolov2 = models.yolov2(num_classes=10)
```

### YOLO family

#### YOLOv1

```python
YOLOv1(layout: list[list[int]], num_classes: int = 20, in_channels: int = 3, stem_channels: int = 64, num_anchors: int = 2, lambda_obj: float = 1, lambda_noobj: float = 0.5, lambda_class: float = 1, lambda_coords: float = 5.0, rpn_nms_thresh: float = 0.7, box_score_thresh: float = 0.05, head_hidden_nodes: int = 512, act_layer: Module | None = None, norm_layer: Callable[[int], Module] | None = None, drop_layer: Callable[..., Module] | None = None, conv_layer: Callable[..., Module] | None = None, backbone_norm_layer: Callable[[int], Module] | None = None)
```

Source code in `holocron/models/detection/yolo.py`

```python
def __init__(
    self,
    layout: list[list[int]],
    num_classes: int = 20,
    in_channels: int = 3,
    stem_channels: int = 64,
    num_anchors: int = 2,
    lambda_obj: float = 1,
    lambda_noobj: float = 0.5,
    lambda_class: float = 1,
    lambda_coords: float = 5.0,
    rpn_nms_thresh: float = 0.7,
    box_score_thresh: float = 0.05,
    head_hidden_nodes: int = 512,  # In the original paper, 4096
    act_layer: nn.Module | None = None,
    norm_layer: Callable[[int], nn.Module] | None = None,
    drop_layer: Callable[..., nn.Module] | None = None,
    conv_layer: Callable[..., nn.Module] | None = None,
    backbone_norm_layer: Callable[[int], nn.Module] | None = None,
) -> None:
    super().__init__(
        num_classes, rpn_nms_thresh, box_score_thresh, lambda_obj, lambda_noobj, lambda_class, lambda_coords
    )

    if act_layer is None:
        act_layer = nn.LeakyReLU(0.1, inplace=True)

    if backbone_norm_layer is None and norm_layer is not None:
        backbone_norm_layer = norm_layer

    self.backbone = DarknetBodyV1(layout, in_channels, stem_channels, act_layer, backbone_norm_layer)

    self.block4 = nn.Sequential(
        *conv_sequence(
            1024,
            1024,
            act_layer,
            norm_layer,
            drop_layer,
            conv_layer,
            kernel_size=3,
            padding=1,
            bias=(norm_layer is None),
        ),
        *conv_sequence(
            1024,
            1024,
            act_layer,
            norm_layer,
            drop_layer,
            conv_layer,
            kernel_size=3,
            padding=1,
            stride=2,
            bias=(norm_layer is None),
        ),
        *conv_sequence(
            1024,
            1024,
            act_layer,
            norm_layer,
            drop_layer,
            conv_layer,
            kernel_size=3,
            padding=1,
            bias=(norm_layer is None),
        ),
        *conv_sequence(
            1024,
            1024,
            act_layer,
            norm_layer,
            drop_layer,
            conv_layer,
            kernel_size=3,
            padding=1,
            bias=(norm_layer is None),
        ),
    )

    self.classifier = nn.Sequential(
        nn.Flatten(),
        nn.Linear(1024 * 7**2, head_hidden_nodes),
        act_layer,
        nn.Dropout(0.5),
        nn.Linear(head_hidden_nodes, 7**2 * (num_anchors * 5 + num_classes)),
    )
    self.num_anchors: int = num_anchors

    init_module(self.block4, "leaky_relu")
    init_module(self.classifier, "leaky_relu")
```

#### yolov1

```python
yolov1(pretrained: bool = False, progress: bool = True, pretrained_backbone: bool = True, **kwargs: Any) -> YOLOv1
```

YOLO model from ["You Only Look Once: Unified, Real-Time Object Detection"](https://pjreddie.com/media/files/papers/yolo_1.pdf).

YOLO's particularity is to make predictions in a grid (same size as last feature map). For each grid cell, the model predicts classification scores and a fixed number of boxes (default: 2). Each box in the cell gets 5 predictions: an objectness score, and 4 coordinates. The 4 coordinates are composed of: the 2-D coordinates of the predicted box center (relative to the cell), and the width and height of the predicted box (relative to the whole image).

For training, YOLO uses a multi-part loss whose components are computed by:

\[ \\mathcal{L}_{coords} = \\sum\\limits_{i=0}^{S^2} \\sum\\limits\_{j=0}^{B} \\mathbb{1}_{ij}^{obj} \\Big\[ (x_{ij} - \\hat{x}_{ij})² + (y_{ij} - \\hat{y}_{ij})² + (\\sqrt{w_{ij}} - \\sqrt{\\hat{w}_{ij}})² + (\\sqrt{h_{ij}} - \\sqrt{\\hat{h}\_{ij}})² \\Big\] \]

where: (S) is size of the output feature map (7 for an input size ((448, 448))), (B) is the number of anchor boxes per grid cell (default: 2), (\\mathbb{1}_{ij}^{obj}) equals to 1 if a GT center falls inside the i-th grid cell and among the anchor boxes of that cell, has the highest IoU with the j-th box else 0, ((x_{ij}, y\_{ij}, w\_{ij}, h\_{ij})) are the coordinates of the ground truth assigned to the j-th anchor box of the i-th grid cell, ((\\hat{x}_{ij}, \\hat{y}_{ij}, \\hat{w}_{ij}, \\hat{h}_{ij})) are the coordinate predictions for the j-th anchor box of the i-th grid cell.

\[ \\mathcal{L}_{objectness} = \\sum\\limits_{i=0}^{S^2} \\sum\\limits\_{j=0}^{B} \\Big\[ \\mathbb{1}_{ij}^{obj} \\Big(C_{ij} - \\hat{C}\_{ij} \\Big)^2

- \\lambda\_{noobj} \\mathbb{1}_{ij}^{noobj} \\Big(C_{ij} - \\hat{C}\_{ij} \\Big)^2 \\Big\] \]

where (\\lambda\_{noobj}) is a positive coefficient (default: 0.5), (\\mathbb{1}_{ij}^{noobj} = 1 - \\mathbb{1}_{ij}^{obj}), (C\_{ij}) equals the Intersection Over Union between the j-th anchor box in the i-th grid cell and its matched ground truth box if that box is matched with a ground truth else 0, and (\\hat{C}\_{ij}) is the objectness score of the j-th anchor box in the i-th grid cell..

\[ \\mathcal{L}_{classification} = \\sum\\limits_{i=0}^{S^2} \\mathbb{1}_{i}^{obj} \\sum\\limits_{c \\in classes} (p_i(c) - \\hat{p}\_i(c))^2 \]

where (\\mathbb{1}\_{i}^{obj}) equals to 1 if a GT center falls inside the i-th grid cell else 0, (p_i(c)) equals 1 if the assigned ground truth to the i-th cell is classified as class (c), and (\\hat{p}\_i(c)) is the predicted probability of class (c) in the i-th cell.

And the full loss is given by:

\[ \\mathcal{L}_{YOLOv1} = \\lambda_{coords} \\cdot \\mathcal{L}_{coords} + \\mathcal{L}_{objectness} + \\mathcal{L}\_{classification} \]

where (\\lambda\_{coords}) is a positive coefficient (default: 5).

| PARAMETER             | DESCRIPTION                                                                                               |
| --------------------- | --------------------------------------------------------------------------------------------------------- |
| `pretrained`          | If True, returns a model pre-trained on ImageNet **TYPE:** `bool` **DEFAULT:** `False`                    |
| `progress`            | If True, displays a progress bar of the download to stderr **TYPE:** `bool` **DEFAULT:** `True`           |
| `pretrained_backbone` | If True, backbone parameters will have been pretrained on Imagenette **TYPE:** `bool` **DEFAULT:** `True` |
| `kwargs`              | keyword args of YOLOv1 **TYPE:** `Any` **DEFAULT:** `{}`                                                  |

| RETURNS  | DESCRIPTION      |
| -------- | ---------------- |
| `YOLOv1` | detection module |

Source code in `holocron/models/detection/yolo.py`

```python
def yolov1(pretrained: bool = False, progress: bool = True, pretrained_backbone: bool = True, **kwargs: Any) -> YOLOv1:
    r"""YOLO model from
    ["You Only Look Once: Unified, Real-Time Object Detection"](https://pjreddie.com/media/files/papers/yolo_1.pdf).

    YOLO's particularity is to make predictions in a grid (same size as last feature map). For each grid cell,
    the model predicts classification scores and a fixed number of boxes (default: 2). Each box in the cell gets
    5 predictions: an objectness score, and 4 coordinates. The 4 coordinates are composed of: the 2-D coordinates of
    the predicted box center (relative to the cell), and the width and height of the predicted box (relative to
    the whole image).

    For training, YOLO uses a multi-part loss whose components are computed by:

    $$
    \mathcal{L}_{coords} = \sum\limits_{i=0}^{S^2} \sum\limits_{j=0}^{B}
    \mathbb{1}_{ij}^{obj} \Big[
    (x_{ij} - \hat{x}_{ij})² + (y_{ij} - \hat{y}_{ij})² +
    (\sqrt{w_{ij}} - \sqrt{\hat{w}_{ij}})² + (\sqrt{h_{ij}} - \sqrt{\hat{h}_{ij}})²
    \Big]
    $$

    where:
    $S$ is size of the output feature map (7 for an input size $(448, 448)$),
    $B$ is the number of anchor boxes per grid cell (default: 2),
    $\mathbb{1}_{ij}^{obj}$ equals to 1 if a GT center falls inside the i-th grid cell and among the
    anchor boxes of that cell, has the highest IoU with the j-th box else 0,
    $(x_{ij}, y_{ij}, w_{ij}, h_{ij})$ are the coordinates of the ground truth assigned to
    the j-th anchor box of the i-th grid cell,
    $(\hat{x}_{ij}, \hat{y}_{ij}, \hat{w}_{ij}, \hat{h}_{ij})$ are the coordinate predictions
    for the j-th anchor box of the i-th grid cell.

    $$
    \mathcal{L}_{objectness} = \sum\limits_{i=0}^{S^2} \sum\limits_{j=0}^{B}
    \Big[ \mathbb{1}_{ij}^{obj} \Big(C_{ij} - \hat{C}_{ij} \Big)^2
    + \lambda_{noobj} \mathbb{1}_{ij}^{noobj} \Big(C_{ij} - \hat{C}_{ij} \Big)^2
    \Big]
    $$

    where $\lambda_{noobj}$ is a positive coefficient (default: 0.5),
    $\mathbb{1}_{ij}^{noobj} = 1 - \mathbb{1}_{ij}^{obj}$,
    $C_{ij}$ equals the Intersection Over Union between the j-th anchor box in the i-th grid cell and its
    matched ground truth box if that box is matched with a ground truth else 0,
    and $\hat{C}_{ij}$ is the objectness score of the j-th anchor box in the i-th grid cell..

    $$
    \mathcal{L}_{classification} = \sum\limits_{i=0}^{S^2}
    \mathbb{1}_{i}^{obj} \sum\limits_{c \in classes}
    (p_i(c) - \hat{p}_i(c))^2
    $$

    where $\mathbb{1}_{i}^{obj}$ equals to 1 if a GT center falls inside the i-th grid cell else 0,
    $p_i(c)$ equals 1 if the assigned ground truth to the i-th cell is classified as class $c$,
    and $\hat{p}_i(c)$ is the predicted probability of class $c$ in the i-th cell.

    And the full loss is given by:

    $$
    \mathcal{L}_{YOLOv1} = \lambda_{coords} \cdot \mathcal{L}_{coords} +
    \mathcal{L}_{objectness} + \mathcal{L}_{classification}
    $$

    where $\lambda_{coords}$ is a positive coefficient (default: 5).

    Args:
        pretrained: If True, returns a model pre-trained on ImageNet
        progress: If True, displays a progress bar of the download to stderr
        pretrained_backbone: If True, backbone parameters will have been pretrained on Imagenette
        kwargs: keyword args of [`YOLOv1`][holocron.models.detection.yolo.YOLOv1]

    Returns:
        detection module
    """
    return _yolo(
        "yolov1",
        pretrained,
        progress,
        pretrained_backbone,
        [[192], [128, 256, 256, 512], [*([256, 512] * 4), 512, 1024], [512, 1024] * 2],
        **kwargs,
    )
```

#### YOLOv2

```python
YOLOv2(layout: list[tuple[int, int]], num_classes: int = 20, in_channels: int = 3, stem_chanels: int = 32, anchors: Tensor | None = None, passthrough_ratio: int = 8, lambda_obj: float = 1, lambda_noobj: float = 0.5, lambda_class: float = 1, lambda_coords: float = 5, rpn_nms_thresh: float = 0.7, box_score_thresh: float = 0.05, act_layer: Module | None = None, norm_layer: Callable[[int], Module] | None = None, drop_layer: Callable[..., Module] | None = None, conv_layer: Callable[..., Module] | None = None, backbone_norm_layer: Callable[[int], Module] | None = None)
```

Source code in `holocron/models/detection/yolov2.py`

```python
def __init__(
    self,
    layout: list[tuple[int, int]],
    num_classes: int = 20,
    in_channels: int = 3,
    stem_chanels: int = 32,
    anchors: Tensor | None = None,
    passthrough_ratio: int = 8,
    lambda_obj: float = 1,
    lambda_noobj: float = 0.5,
    lambda_class: float = 1,
    lambda_coords: float = 5,
    rpn_nms_thresh: float = 0.7,
    box_score_thresh: float = 0.05,
    act_layer: nn.Module | None = None,
    norm_layer: Callable[[int], nn.Module] | None = None,
    drop_layer: Callable[..., nn.Module] | None = None,
    conv_layer: Callable[..., nn.Module] | None = None,
    backbone_norm_layer: Callable[[int], nn.Module] | None = None,
) -> None:
    super().__init__(
        num_classes, rpn_nms_thresh, box_score_thresh, lambda_obj, lambda_noobj, lambda_class, lambda_coords
    )

    if act_layer is None:
        act_layer = nn.LeakyReLU(0.1, inplace=True)
    if norm_layer is None:
        norm_layer = nn.BatchNorm2d
    if backbone_norm_layer is None:
        backbone_norm_layer = norm_layer

    # Priors computed using K-means
    if anchors is None:
        # cf. https://github.com/pjreddie/darknet/blob/master/cfg/yolov2-voc.cfg#L242
        anchors = (
            torch.tensor([
                [1.3221, 1.73145],
                [3.19275, 4.00944],
                [5.05587, 8.09892],
                [9.47112, 4.84053],
                [11.2364, 10.0071],
            ])
            / 13
        )

    self.backbone = DarknetBodyV2(
        layout, in_channels, stem_chanels, True, act_layer, backbone_norm_layer, drop_layer, conv_layer
    )

    self.block5 = nn.Sequential(
        *conv_sequence(
            layout[-1][0],
            layout[-1][0],
            act_layer,
            norm_layer,
            drop_layer,
            conv_layer,
            kernel_size=3,
            padding=1,
            bias=(norm_layer is None),
        ),
        *conv_sequence(
            layout[-1][0],
            layout[-1][0],
            act_layer,
            norm_layer,
            drop_layer,
            conv_layer,
            kernel_size=3,
            padding=1,
            bias=(norm_layer is None),
        ),
    )

    self.passthrough_layer = nn.Sequential(
        *conv_sequence(
            layout[-2][0],
            layout[-2][0] // passthrough_ratio,
            act_layer,
            norm_layer,
            drop_layer,
            conv_layer,
            kernel_size=1,
            bias=(norm_layer is None),
        ),
        ConcatDownsample2d(scale_factor=2),
    )

    self.block6 = nn.Sequential(
        *conv_sequence(
            layout[-1][0] + layout[-2][0] // passthrough_ratio * 2**2,
            layout[-1][0],
            act_layer,
            norm_layer,
            drop_layer,
            conv_layer,
            kernel_size=3,
            padding=1,
            bias=(norm_layer is None),
        )
    )

    # Each box has P_objectness, 4 coords, and score for each class
    self.head = nn.Conv2d(layout[-1][0], anchors.shape[0] * (5 + num_classes), 1)

    # Register losses
    self.register_buffer("anchors", anchors)

    init_module(self.block5, "leaky_relu")
    init_module(self.passthrough_layer, "leaky_relu")
    init_module(self.block6, "leaky_relu")
    # Initialize the head like a linear (default Conv2D init is the same as Linear)
    if self.head.bias is not None:
        self.head.bias.data.zero_()
```

#### yolov2

```python
yolov2(pretrained: bool = False, progress: bool = True, pretrained_backbone: bool = True, **kwargs: Any) -> YOLOv2
```

YOLOv2 model from ["YOLO9000: Better, Faster, Stronger"](https://pjreddie.com/media/files/papers/YOLO9000.pdf).

YOLOv2 improves upon YOLO by raising the number of boxes predicted by grid cell (default: 5), introducing bounding box priors and predicting class scores for each anchor box in the grid cell.

For training, YOLOv2 uses the same multi-part loss as YOLO apart from its classification loss:

\[ \\mathcal{L}_{classification} = \\sum\\limits_{i=0}^{S^2} \\sum\\limits\_{j=0}^{B} \\mathbb{1}_{ij}^{obj} \\sum\\limits_{c \\in classes} (p\_{ij}(c) - \\hat{p}\_{ij}(c))^2 \]

where:

- (S) is size of the output feature map (13 for an input size ((416, 416))),
- (B) is the number of anchor boxes per grid cell (default: 5),
- (\\mathbb{1}\_{ij}^{obj}) equals to 1 if a GT center falls inside the i-th grid cell and among the anchor boxes of that cell, has the highest IoU with the j-th box else 0,
- (p\_{ij}(c)) equals 1 if the assigned ground truth to the j-th anchor box of the i-th cell is classified as class (c),
- (\\hat{p}\_{ij}(c)) is the predicted probability of class (c) for the j-th anchor box in the i-th cell.

| PARAMETER             | DESCRIPTION                                                                                               |
| --------------------- | --------------------------------------------------------------------------------------------------------- |
| `pretrained`          | If True, returns a model pre-trained on ImageNet **TYPE:** `bool` **DEFAULT:** `False`                    |
| `progress`            | If True, displays a progress bar of the download to stderr **TYPE:** `bool` **DEFAULT:** `True`           |
| `pretrained_backbone` | If True, backbone parameters will have been pretrained on Imagenette **TYPE:** `bool` **DEFAULT:** `True` |
| `kwargs`              | keyword args of YOLOv2 **TYPE:** `Any` **DEFAULT:** `{}`                                                  |

| RETURNS  | DESCRIPTION      |
| -------- | ---------------- |
| `YOLOv2` | detection module |

Source code in `holocron/models/detection/yolov2.py`

```python
def yolov2(pretrained: bool = False, progress: bool = True, pretrained_backbone: bool = True, **kwargs: Any) -> YOLOv2:
    r"""YOLOv2 model from
    ["YOLO9000: Better, Faster, Stronger"](https://pjreddie.com/media/files/papers/YOLO9000.pdf).

    YOLOv2 improves upon YOLO by raising the number of boxes predicted by grid cell (default: 5), introducing
    bounding box priors and predicting class scores for each anchor box in the grid cell.

    For training, YOLOv2 uses the same multi-part loss as YOLO apart from its classification loss:

    $$
    \mathcal{L}_{classification} = \sum\limits_{i=0}^{S^2}  \sum\limits_{j=0}^{B}
    \mathbb{1}_{ij}^{obj} \sum\limits_{c \in classes}
    (p_{ij}(c) - \hat{p}_{ij}(c))^2
    $$

    where:
    - $S$ is size of the output feature map (13 for an input size $(416, 416)$),
    - $B$ is the number of anchor boxes per grid cell (default: 5),
    - $\mathbb{1}_{ij}^{obj}$ equals to 1 if a GT center falls inside the i-th grid cell and among the
    anchor boxes of that cell, has the highest IoU with the j-th box else 0,
    - $p_{ij}(c)$ equals 1 if the assigned ground truth to the j-th anchor box of the i-th cell is classified
    as class $c$,
    - $\hat{p}_{ij}(c)$ is the predicted probability of class $c$ for the j-th anchor box
    in the i-th cell.

    Args:
        pretrained: If True, returns a model pre-trained on ImageNet
        progress: If True, displays a progress bar of the download to stderr
        pretrained_backbone: If True, backbone parameters will have been pretrained on Imagenette
        kwargs: keyword args of [`YOLOv2`][holocron.models.detection.yolov2.YOLOv2]

    Returns:
        detection module
    """
    if pretrained_backbone:
        kwargs["backbone_norm_layer"] = FrozenBatchNorm2d

    return _yolo(
        "yolov2",
        pretrained,
        progress,
        pretrained_backbone,
        [(64, 0), (128, 1), (256, 1), (512, 2), (1024, 2)],
        **kwargs,
    )
```

#### YOLOv4

```python
YOLOv4(layout: list[tuple[int, int]], num_classes: int = 80, in_channels: int = 3, stem_channels: int = 32, anchors: Tensor | None = None, act_layer: Module | None = None, norm_layer: Callable[[int], Module] | None = None, drop_layer: Callable[..., Module] | None = None, conv_layer: Callable[..., Module] | None = None, backbone_norm_layer: Callable[[int], Module] | None = None)
```

Source code in `holocron/models/detection/yolov4.py`

```python
def __init__(
    self,
    layout: list[tuple[int, int]],
    num_classes: int = 80,
    in_channels: int = 3,
    stem_channels: int = 32,
    anchors: Tensor | None = None,
    act_layer: nn.Module | None = None,
    norm_layer: Callable[[int], nn.Module] | None = None,
    drop_layer: Callable[..., nn.Module] | None = None,
    conv_layer: Callable[..., nn.Module] | None = None,
    backbone_norm_layer: Callable[[int], nn.Module] | None = None,
) -> None:
    super().__init__()

    if act_layer is None:
        act_layer = nn.Mish(inplace=True)
    if norm_layer is None:
        norm_layer = nn.BatchNorm2d
    if backbone_norm_layer is None:
        backbone_norm_layer = norm_layer
    if drop_layer is None:
        drop_layer = DropBlock2d

    # backbone
    self.backbone = DarknetBodyV4(
        layout, in_channels, stem_channels, 3, act_layer, backbone_norm_layer, drop_layer, conv_layer
    )
    # neck
    self.neck = Neck([1024, 512, 256], act_layer, norm_layer, drop_layer, conv_layer)
    # head
    self.head = Yolov4Head(num_classes, anchors, act_layer, norm_layer, drop_layer, conv_layer)

    init_module(self.neck, "leaky_relu")
    init_module(self.head, "leaky_relu")
```

#### yolov4

```python
yolov4(pretrained: bool = False, progress: bool = True, pretrained_backbone: bool = True, **kwargs: Any) -> YOLOv4
```

YOLOv4 model from ["YOLOv4: Optimal Speed and Accuracy of Object Detection"](https://arxiv.org/pdf/2004.10934.pdf).

The architecture improves upon YOLOv3 by including: the usage of [DropBlock](https://arxiv.org/pdf/1810.12890.pdf) regularization, [Mish](https://arxiv.org/pdf/1908.08681.pdf) activation, [CSP](https://arxiv.org/pdf/2004.10934.pdf) and [SAM](https://arxiv.org/pdf/1807.06521.pdf) in the backbone, [SPP](https://arxiv.org/pdf/1406.4729.pdf) and [PAN](https://arxiv.org/pdf/1803.01534.pdf) in the neck.

For training, YOLOv4 uses the same multi-part loss as YOLOv3 apart from its box coordinate loss:

\[ \\mathcal{L}_{coords} = \\sum\\limits_{i=0}^{S^2} \\sum\\limits\_{j=0}^{B} \\min\\limits\_{k \\in [1, M]} C\_{IoU}(\\hat{loc}\_{ij}, loc^{GT}\_k) \]

where:

- (S) is size of the output feature map (13 for an input size ((416, 416))),
- (B) is the number of anchor boxes per grid cell (default: 3),
- (M) is the number of ground truth boxes,
- (C\_{IoU}) is the complete IoU loss,
- (\\hat{loc}\_{ij}) is the predicted bounding box for grid cell (i) at anchor (j), and (loc^{GT}\_k) is the k-th ground truth bounding box.

| PARAMETER             | DESCRIPTION                                                                                               |
| --------------------- | --------------------------------------------------------------------------------------------------------- |
| `pretrained`          | If True, returns a model pre-trained on ImageNet **TYPE:** `bool` **DEFAULT:** `False`                    |
| `progress`            | If True, displays a progress bar of the download to stderr **TYPE:** `bool` **DEFAULT:** `True`           |
| `pretrained_backbone` | If True, backbone parameters will have been pretrained on Imagenette **TYPE:** `bool` **DEFAULT:** `True` |
| `kwargs`              | keyword args of YOLOv4 **TYPE:** `Any` **DEFAULT:** `{}`                                                  |

| RETURNS  | DESCRIPTION      |
| -------- | ---------------- |
| `YOLOv4` | detection module |

Source code in `holocron/models/detection/yolov4.py`

```python
def yolov4(pretrained: bool = False, progress: bool = True, pretrained_backbone: bool = True, **kwargs: Any) -> YOLOv4:
    r"""YOLOv4 model from
    ["YOLOv4: Optimal Speed and Accuracy of Object Detection"](https://arxiv.org/pdf/2004.10934.pdf).

    The architecture improves upon YOLOv3 by including: the usage of [DropBlock](https://arxiv.org/pdf/1810.12890.pdf) regularization, [Mish](https://arxiv.org/pdf/1908.08681.pdf) activation, [CSP](https://arxiv.org/pdf/2004.10934.pdf) and [SAM](https://arxiv.org/pdf/1807.06521.pdf) in the
    backbone, [SPP](https://arxiv.org/pdf/1406.4729.pdf) and [PAN](https://arxiv.org/pdf/1803.01534.pdf) in the
    neck.

    For training, YOLOv4 uses the same multi-part loss as YOLOv3 apart from its box coordinate loss:

    $$
        \mathcal{L}_{coords} = \sum\limits_{i=0}^{S^2}  \sum\limits_{j=0}^{B}
        \min\limits_{k \in [1, M]} C_{IoU}(\hat{loc}_{ij}, loc^{GT}_k)
    $$

    where:
    - $S$ is size of the output feature map (13 for an input size $(416, 416)$),
    - $B$ is the number of anchor boxes per grid cell (default: 3),
    - $M$ is the number of ground truth boxes,
    - $C_{IoU}$ is the complete IoU loss,
    - $\hat{loc}_{ij}$ is the predicted bounding box for grid cell $i$ at anchor $j$,
    and $loc^{GT}_k$ is the k-th ground truth bounding box.

    Args:
        pretrained: If True, returns a model pre-trained on ImageNet
        progress: If True, displays a progress bar of the download to stderr
        pretrained_backbone: If True, backbone parameters will have been pretrained on Imagenette
        kwargs: keyword args of [`YOLOv4`][holocron.models.detection.yolov4.YOLOv4]

    Returns:
        detection module
    """
    if pretrained_backbone:
        kwargs["backbone_norm_layer"] = FrozenBatchNorm2d

    return _yolo(
        "yolov4",
        pretrained,
        progress,
        pretrained_backbone,
        [(64, 1), (128, 2), (256, 8), (512, 8), (1024, 4)],
        **kwargs,
    )
```

## Semantic Segmentation

Semantic segmentation models expect a 4D image tensor as an input (N x C x H x W) and returns a classification score tensor of size (N x K x Ho x Wo).

```python
import holocron.models as models
unet = models.unet(num_classes=10)
```

### U-Net family

#### UNet

```python
UNet(layout: list[int], in_channels: int = 3, num_classes: int = 10, act_layer: Module | None = None, norm_layer: Callable[[int], Module] | None = None, drop_layer: Callable[..., Module] | None = None, conv_layer: Callable[..., Module] | None = None, same_padding: bool = True, bilinear_upsampling: bool = True)
```

Implements a U-Net architecture

| PARAMETER             | DESCRIPTION                                                                                            |
| --------------------- | ------------------------------------------------------------------------------------------------------ |
| `layout`              | number of channels after each contracting block **TYPE:** `list[int]`                                  |
| `in_channels`         | number of channels in the input tensor **TYPE:** `int` **DEFAULT:** `3`                                |
| `num_classes`         | number of output classes **TYPE:** `int` **DEFAULT:** `10`                                             |
| `act_layer`           | activation layer **TYPE:** \`Module                                                                    |
| `norm_layer`          | normalization layer **TYPE:** \`Callable\[[int], Module\]                                              |
| `drop_layer`          | dropout layer **TYPE:** \`Callable[..., Module]                                                        |
| `conv_layer`          | convolutional layer **TYPE:** \`Callable[..., Module]                                                  |
| `same_padding`        | enforces same padding in convolutions **TYPE:** `bool` **DEFAULT:** `True`                             |
| `bilinear_upsampling` | replaces transposed conv by bilinear interpolation for upsampling **TYPE:** `bool` **DEFAULT:** `True` |

Source code in `holocron/models/segmentation/unet.py`

```python
def __init__(
    self,
    layout: list[int],
    in_channels: int = 3,
    num_classes: int = 10,
    act_layer: nn.Module | None = None,
    norm_layer: Callable[[int], nn.Module] | None = None,
    drop_layer: Callable[..., nn.Module] | None = None,
    conv_layer: Callable[..., nn.Module] | None = None,
    same_padding: bool = True,
    bilinear_upsampling: bool = True,
) -> None:
    super().__init__()

    if act_layer is None:
        act_layer = nn.ReLU(inplace=True)

    # Contracting path
    self.encoder = nn.ModuleList([])
    layout_ = [in_channels, *layout]
    pool = False
    for in_chan, out_chan in pairwise(layout_):
        self.encoder.append(
            down_path(in_chan, out_chan, pool, int(same_padding), act_layer, norm_layer, drop_layer, conv_layer)
        )
        pool = True

    self.bridge = nn.Sequential(
        nn.MaxPool2d((2, 2)),
        *conv_sequence(
            layout[-1], 2 * layout[-1], act_layer, norm_layer, drop_layer, conv_layer, kernel_size=3, padding=1
        ),
        *conv_sequence(
            2 * layout[-1], layout[-1], act_layer, norm_layer, drop_layer, conv_layer, kernel_size=3, padding=1
        ),
    )

    # Expansive path
    self.decoder = nn.ModuleList([])
    layout_ = [chan // 2 if bilinear_upsampling else chan for chan in layout[::-1][:-1]] + [layout[0]]
    for in_chan, out_chan in zip([2 * layout[-1], *layout[::-1][:-1]], layout_, strict=True):
        self.decoder.append(
            UpPath(
                in_chan,
                out_chan,
                bilinear_upsampling,
                int(same_padding),
                act_layer,
                norm_layer,
                drop_layer,
                conv_layer,
            )
        )

    # Classifier
    self.classifier = nn.Conv2d(layout[0], num_classes, 1)

    init_module(self, "relu")
```

#### unet

```python
unet(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> UNet
```

U-Net from ["U-Net: Convolutional Networks for Biomedical Image Segmentation"](https://arxiv.org/pdf/1505.04597.pdf)

| PARAMETER    | DESCRIPTION                                                                                     |
| ------------ | ----------------------------------------------------------------------------------------------- |
| `pretrained` | If True, returns a model pre-trained on PASCAL VOC2012 **TYPE:** `bool` **DEFAULT:** `False`    |
| `progress`   | If True, displays a progress bar of the download to stderr **TYPE:** `bool` **DEFAULT:** `True` |
| `kwargs`     | keyword args of UNet **TYPE:** `Any` **DEFAULT:** `{}`                                          |

| RETURNS | DESCRIPTION                 |
| ------- | --------------------------- |
| `UNet`  | semantic segmentation model |

Source code in `holocron/models/segmentation/unet.py`

```python
def unet(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> UNet:
    """U-Net from
    ["U-Net: Convolutional Networks for Biomedical Image Segmentation"](https://arxiv.org/pdf/1505.04597.pdf)

    ![UNet explanation](https://github.com/frgfm/Holocron/releases/download/v0.1.3/unet.png)

    Args:
        pretrained: If True, returns a model pre-trained on PASCAL VOC2012
        progress: If True, displays a progress bar of the download to stderr
        kwargs: keyword args of [`UNet`][holocron.models.segmentation.unet.UNet]

    Returns:
        semantic segmentation model
    """
    return _unet("unet", pretrained, progress, **kwargs)
```

#### DynamicUNet

```python
DynamicUNet(encoder: IntermediateLayerGetter, num_classes: int = 10, act_layer: Module | None = None, norm_layer: Callable[[int], Module] | None = None, drop_layer: Callable[..., Module] | None = None, conv_layer: Callable[..., Module] | None = None, same_padding: bool = True, input_shape: tuple[int, int, int] | None = None, final_upsampling: bool = False)
```

Implements a dymanic U-Net architecture

| PARAMETER          | DESCRIPTION                                                                                                      |
| ------------------ | ---------------------------------------------------------------------------------------------------------------- |
| `encoder`          | feature extractor used for encoding **TYPE:** `IntermediateLayerGetter`                                          |
| `num_classes`      | number of output classes **TYPE:** `int` **DEFAULT:** `10`                                                       |
| `act_layer`        | activation layer **TYPE:** \`Module                                                                              |
| `norm_layer`       | normalization layer **TYPE:** \`Callable\[[int], Module\]                                                        |
| `drop_layer`       | dropout layer **TYPE:** \`Callable[..., Module]                                                                  |
| `conv_layer`       | convolutional layer **TYPE:** \`Callable[..., Module]                                                            |
| `same_padding`     | enforces same padding in convolutions **TYPE:** `bool` **DEFAULT:** `True`                                       |
| `input_shape`      | shape of the input tensor **TYPE:** \`tuple[int, int, int]                                                       |
| `final_upsampling` | if True, replaces transposed conv by bilinear interpolation for upsampling **TYPE:** `bool` **DEFAULT:** `False` |

Source code in `holocron/models/segmentation/unet.py`

```python
def __init__(
    self,
    encoder: IntermediateLayerGetter,
    num_classes: int = 10,
    act_layer: nn.Module | None = None,
    norm_layer: Callable[[int], nn.Module] | None = None,
    drop_layer: Callable[..., nn.Module] | None = None,
    conv_layer: Callable[..., nn.Module] | None = None,
    same_padding: bool = True,
    input_shape: tuple[int, int, int] | None = None,
    final_upsampling: bool = False,
) -> None:
    super().__init__()

    if act_layer is None:
        act_layer = nn.ReLU(inplace=True)

    self.encoder = encoder
    # Determine all feature map shapes
    training_mode = self.encoder.training
    self.encoder.eval()
    input_shape = (3, 256, 256) if input_shape is None else input_shape
    with torch.no_grad():
        shapes = [v.shape[1:] for v in self.encoder(torch.zeros(1, *input_shape)).values()]
    chans = [s[0] for s in shapes]
    if training_mode:
        self.encoder.train()

    # Middle layers
    self.bridge = nn.Sequential(
        nn.BatchNorm2d(chans[-1]) if norm_layer is None else norm_layer(chans[-1]),
        act_layer,
        *conv_sequence(
            chans[-1], 2 * chans[-1], act_layer, norm_layer, drop_layer, conv_layer, kernel_size=3, padding=1
        ),
        *conv_sequence(
            2 * chans[-1], chans[-1], act_layer, norm_layer, drop_layer, conv_layer, kernel_size=3, padding=1
        ),
    )

    # Expansive path
    self.decoder = nn.ModuleList([])
    layout = [*chans[::-1][1:], chans[0]]
    for up_chan, out_chan in zip(chans[::-1], layout, strict=True):
        self.decoder.append(
            UBlock(up_chan, up_chan, out_chan, int(same_padding), act_layer, norm_layer, drop_layer, conv_layer)
        )

    # Final upsampling if sizes don't match
    self.upsample: nn.Sequential | None = None
    if final_upsampling:
        self.upsample = nn.Sequential(
            *conv_sequence(chans[0], chans[0] * 2**2, act_layer, norm_layer, drop_layer, conv_layer, kernel_size=1),
            nn.PixelShuffle(upscale_factor=2),
        )

    # Classifier
    self.classifier = nn.Conv2d(chans[0], num_classes, 1)

    init_module(self, "relu")
```

#### unet2

```python
unet2(pretrained: bool = False, progress: bool = True, in_channels: int = 3, **kwargs: Any) -> DynamicUNet
```

Modified version of U-Net from ["U-Net: Convolutional Networks for Biomedical Image Segmentation"](https://arxiv.org/pdf/1505.04597.pdf) that includes a more advanced upscaling block inspired by [fastai](https://docs.fast.ai/vision.models.unet.html#DynamicUnet)

| PARAMETER     | DESCRIPTION                                                                                     |
| ------------- | ----------------------------------------------------------------------------------------------- |
| `pretrained`  | If True, returns a model pre-trained on PASCAL VOC2012 **TYPE:** `bool` **DEFAULT:** `False`    |
| `progress`    | If True, displays a progress bar of the download to stderr **TYPE:** `bool` **DEFAULT:** `True` |
| `in_channels` | number of input channels **TYPE:** `int` **DEFAULT:** `3`                                       |
| `kwargs`      | keyword args of DynamicUNet **TYPE:** `Any` **DEFAULT:** `{}`                                   |

| RETURNS       | DESCRIPTION                 |
| ------------- | --------------------------- |
| `DynamicUNet` | semantic segmentation model |

Source code in `holocron/models/segmentation/unet.py`

```python
def unet2(pretrained: bool = False, progress: bool = True, in_channels: int = 3, **kwargs: Any) -> DynamicUNet:
    """Modified version of U-Net from
    ["U-Net: Convolutional Networks for Biomedical Image Segmentation"](https://arxiv.org/pdf/1505.04597.pdf)
    that includes a more advanced upscaling block inspired by [fastai](https://docs.fast.ai/vision.models.unet.html#DynamicUnet)

    ![UNet architecture](https://github.com/frgfm/Holocron/releases/download/v0.1.3/unet.png)

    Args:
        pretrained: If True, returns a model pre-trained on PASCAL VOC2012
        progress: If True, displays a progress bar of the download to stderr
        in_channels: number of input channels
        kwargs: keyword args of [`DynamicUNet`][holocron.models.segmentation.unet.DynamicUNet]

    Returns:
        semantic segmentation model
    """
    backbone = UNetBackbone(default_cfgs["unet2"]["encoder_layout"], in_channels=in_channels).features

    return _dynamic_unet("unet2", backbone, pretrained, progress, **kwargs)  # ty: ignore[invalid-argument-type]
```

#### unet_tvvgg11

```python
unet_tvvgg11(pretrained: bool = False, pretrained_backbone: bool = True, progress: bool = True, **kwargs: Any) -> DynamicUNet
```

U-Net from ["U-Net: Convolutional Networks for Biomedical Image Segmentation"](https://arxiv.org/pdf/1505.04597.pdf) with a VGG-11 backbone used as encoder, and more advanced upscaling blocks inspired by [fastai](https://docs.fast.ai/vision.models.unet.html#DynamicUnet)

| PARAMETER             | DESCRIPTION                                                                                             |
| --------------------- | ------------------------------------------------------------------------------------------------------- |
| `pretrained`          | If True, returns a model pre-trained on PASCAL VOC2012 **TYPE:** `bool` **DEFAULT:** `False`            |
| `pretrained_backbone` | If True, the encoder will load pretrained parameters from ImageNet **TYPE:** `bool` **DEFAULT:** `True` |
| `progress`            | If True, displays a progress bar of the download to stderr **TYPE:** `bool` **DEFAULT:** `True`         |
| `kwargs`              | keyword args of DynamicUNet **TYPE:** `Any` **DEFAULT:** `{}`                                           |

| RETURNS       | DESCRIPTION                 |
| ------------- | --------------------------- |
| `DynamicUNet` | semantic segmentation model |

Source code in `holocron/models/segmentation/unet.py`

```python
def unet_tvvgg11(
    pretrained: bool = False, pretrained_backbone: bool = True, progress: bool = True, **kwargs: Any
) -> DynamicUNet:
    """U-Net from
    ["U-Net: Convolutional Networks for Biomedical Image Segmentation"](https://arxiv.org/pdf/1505.04597.pdf)
    with a VGG-11 backbone used as encoder, and more advanced upscaling blocks inspired by [fastai](https://docs.fast.ai/vision.models.unet.html#DynamicUnet)

    Args:
        pretrained: If True, returns a model pre-trained on PASCAL VOC2012
        pretrained_backbone: If True, the encoder will load pretrained parameters from ImageNet
        progress: If True, displays a progress bar of the download to stderr
        kwargs: keyword args of [`DynamicUNet`][holocron.models.segmentation.unet.DynamicUNet]

    Returns:
        semantic segmentation model
    """
    weights = get_model_weights("vgg11").DEFAULT if pretrained_backbone and not pretrained else None  # ty: ignore[unresolved-attribute]
    backbone: nn.Module = get_model("vgg11", weights=weights).features  # ty: ignore[invalid-assignment]

    return _dynamic_unet("unet_vgg11", backbone, pretrained, progress, **kwargs)
```

#### unet_tvresnet34

```python
unet_tvresnet34(pretrained: bool = False, pretrained_backbone: bool = True, progress: bool = True, **kwargs: Any) -> DynamicUNet
```

U-Net from ["U-Net: Convolutional Networks for Biomedical Image Segmentation"](https://arxiv.org/pdf/1505.04597.pdf) with a ResNet-34 backbone used as encoder, and more advanced upscaling blocks inspired by [fastai](https://docs.fast.ai/vision.models.unet.html#DynamicUnet)

| PARAMETER             | DESCRIPTION                                                                                             |
| --------------------- | ------------------------------------------------------------------------------------------------------- |
| `pretrained`          | If True, returns a model pre-trained on PASCAL VOC2012 **TYPE:** `bool` **DEFAULT:** `False`            |
| `pretrained_backbone` | If True, the encoder will load pretrained parameters from ImageNet **TYPE:** `bool` **DEFAULT:** `True` |
| `progress`            | If True, displays a progress bar of the download to stderr **TYPE:** `bool` **DEFAULT:** `True`         |
| `kwargs`              | keyword args of DynamicUNet **TYPE:** `Any` **DEFAULT:** `{}`                                           |

| RETURNS       | DESCRIPTION                 |
| ------------- | --------------------------- |
| `DynamicUNet` | semantic segmentation model |

Source code in `holocron/models/segmentation/unet.py`

```python
def unet_tvresnet34(
    pretrained: bool = False, pretrained_backbone: bool = True, progress: bool = True, **kwargs: Any
) -> DynamicUNet:
    """U-Net from
    ["U-Net: Convolutional Networks for Biomedical Image Segmentation"](https://arxiv.org/pdf/1505.04597.pdf)
    with a ResNet-34 backbone used as encoder, and more advanced upscaling blocks inspired by [fastai](https://docs.fast.ai/vision.models.unet.html#DynamicUnet)

    Args:
        pretrained: If True, returns a model pre-trained on PASCAL VOC2012
        pretrained_backbone: If True, the encoder will load pretrained parameters from ImageNet
        progress: If True, displays a progress bar of the download to stderr
        kwargs: keyword args of [`DynamicUNet`][holocron.models.segmentation.unet.DynamicUNet]

    Returns:
        semantic segmentation model
    """
    weights = get_model_weights("resnet34").DEFAULT if pretrained_backbone and not pretrained else None  # ty: ignore[unresolved-attribute]
    backbone = get_model("resnet34", weights=weights)
    kwargs["final_upsampling"] = kwargs.get("final_upsampling", True)

    return _dynamic_unet("unet_tvresnet34", backbone, pretrained, progress, **kwargs)
```

#### unet_rexnet13

```python
unet_rexnet13(pretrained: bool = False, pretrained_backbone: bool = True, progress: bool = True, in_channels: int = 3, **kwargs: Any) -> DynamicUNet
```

U-Net from ["U-Net: Convolutional Networks for Biomedical Image Segmentation"](https://arxiv.org/pdf/1505.04597.pdf) with a ReXNet-1.3x backbone used as encoder, and more advanced upscaling blocks inspired by [fastai](https://docs.fast.ai/vision.models.unet.html#DynamicUnet).

| PARAMETER             | DESCRIPTION                                                                                             |
| --------------------- | ------------------------------------------------------------------------------------------------------- |
| `pretrained`          | If True, returns a model pre-trained on PASCAL VOC2012 **TYPE:** `bool` **DEFAULT:** `False`            |
| `pretrained_backbone` | If True, the encoder will load pretrained parameters from ImageNet **TYPE:** `bool` **DEFAULT:** `True` |
| `progress`            | If True, displays a progress bar of the download to stderr **TYPE:** `bool` **DEFAULT:** `True`         |
| `in_channels`         | the number of input channels **TYPE:** `int` **DEFAULT:** `3`                                           |
| `kwargs`              | keyword args of DynamicUNet **TYPE:** `Any` **DEFAULT:** `{}`                                           |

| RETURNS       | DESCRIPTION                 |
| ------------- | --------------------------- |
| `DynamicUNet` | semantic segmentation model |

Source code in `holocron/models/segmentation/unet.py`

```python
def unet_rexnet13(
    pretrained: bool = False,
    pretrained_backbone: bool = True,
    progress: bool = True,
    in_channels: int = 3,
    **kwargs: Any,
) -> DynamicUNet:
    """U-Net from
    ["U-Net: Convolutional Networks for Biomedical Image Segmentation"](https://arxiv.org/pdf/1505.04597.pdf)
    with a ReXNet-1.3x backbone used as encoder, and more advanced upscaling blocks inspired by [fastai](https://docs.fast.ai/vision.models.unet.html#DynamicUnet).

    Args:
        pretrained: If True, returns a model pre-trained on PASCAL VOC2012
        pretrained_backbone: If True, the encoder will load pretrained parameters from ImageNet
        progress: If True, displays a progress bar of the download to stderr
        in_channels: the number of input channels
        kwargs: keyword args of [`DynamicUNet`][holocron.models.segmentation.unet.DynamicUNet]

    Returns:
        semantic segmentation model
    """
    backbone = rexnet1_3x(pretrained=pretrained_backbone and not pretrained, in_channels=in_channels).features
    kwargs["final_upsampling"] = kwargs.get("final_upsampling", True)
    kwargs["act_layer"] = kwargs.get("act_layer", nn.SiLU(inplace=True))
    # hotfix of https://github.com/pytorch/vision/issues/3802
    backbone[21] = nn.SiLU(inplace=True)  # ty: ignore[possibly-missing-implicit-call]

    return _dynamic_unet("unet_rexnet13", backbone, pretrained, progress, **kwargs)  # ty: ignore[invalid-argument-type]
```

#### UNetp

```python
UNetp(layout: list[int], in_channels: int = 3, num_classes: int = 10, act_layer: Module | None = None, norm_layer: Callable[[int], Module] | None = None, drop_layer: Callable[..., Module] | None = None, conv_layer: Callable[..., Module] | None = None)
```

Implements a UNet+ architecture

| PARAMETER     | DESCRIPTION                                                             |
| ------------- | ----------------------------------------------------------------------- |
| `layout`      | number of channels after each contracting block **TYPE:** `list[int]`   |
| `in_channels` | number of channels in the input tensor **TYPE:** `int` **DEFAULT:** `3` |
| `num_classes` | number of output classes **TYPE:** `int` **DEFAULT:** `10`              |
| `act_layer`   | activation layer **TYPE:** \`Module                                     |
| `norm_layer`  | normalization layer **TYPE:** \`Callable\[[int], Module\]               |
| `drop_layer`  | dropout layer **TYPE:** \`Callable[..., Module]                         |
| `conv_layer`  | convolutional layer **TYPE:** \`Callable[..., Module]                   |

Source code in `holocron/models/segmentation/unetpp.py`

```python
def __init__(
    self,
    layout: list[int],
    in_channels: int = 3,
    num_classes: int = 10,
    act_layer: nn.Module | None = None,
    norm_layer: Callable[[int], nn.Module] | None = None,
    drop_layer: Callable[..., nn.Module] | None = None,
    conv_layer: Callable[..., nn.Module] | None = None,
) -> None:
    super().__init__()

    if act_layer is None:
        act_layer = nn.ReLU(inplace=True)

    # Contracting path
    self.encoder = nn.ModuleList([])
    layout_ = [in_channels, *layout]
    pool = False
    for in_chan, out_chan in pairwise(layout_):
        self.encoder.append(down_path(in_chan, out_chan, pool, 1, act_layer, norm_layer, drop_layer, conv_layer))
        pool = True

    self.bridge = nn.Sequential(
        nn.MaxPool2d((2, 2)),
        *conv_sequence(
            layout[-1], 2 * layout[-1], act_layer, norm_layer, drop_layer, conv_layer, kernel_size=3, padding=1
        ),
        *conv_sequence(
            2 * layout[-1], layout[-1], act_layer, norm_layer, drop_layer, conv_layer, kernel_size=3, padding=1
        ),
    )

    # Expansive path
    self.decoder = nn.ModuleList([])
    layout_ = [layout[-1], *layout[1:][::-1]]
    for left_chan, up_chan, num_cells in zip(layout[::-1], layout_, range(1, len(layout) + 1), strict=True):
        self.decoder.append(
            nn.ModuleList([
                UpPath(left_chan + up_chan, left_chan, True, 1, act_layer, norm_layer, drop_layer, conv_layer)
                for _ in range(num_cells)
            ])
        )

    # Classifier
    self.classifier = nn.Conv2d(layout[0], num_classes, 1)

    init_module(self, "relu")
```

#### unetp

```python
unetp(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> UNetp
```

UNet+ from ["UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation"](https://arxiv.org/pdf/1912.05074.pdf)

| PARAMETER    | DESCRIPTION                                                                                     |
| ------------ | ----------------------------------------------------------------------------------------------- |
| `pretrained` | If True, returns a model pre-trained on PASCAL VOC2012 **TYPE:** `bool` **DEFAULT:** `False`    |
| `progress`   | If True, displays a progress bar of the download to stderr **TYPE:** `bool` **DEFAULT:** `True` |
| `kwargs`     | keyword args of UNetp **TYPE:** `Any` **DEFAULT:** `{}`                                         |

| RETURNS | DESCRIPTION                 |
| ------- | --------------------------- |
| `UNetp` | semantic segmentation model |

Source code in `holocron/models/segmentation/unetpp.py`

```python
def unetp(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> UNetp:
    """UNet+ from ["UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation"](https://arxiv.org/pdf/1912.05074.pdf)

    ![UNet+ architecture](https://github.com/frgfm/Holocron/releases/download/v0.1.3/unetp.png)

    Args:
        pretrained: If True, returns a model pre-trained on PASCAL VOC2012
        progress: If True, displays a progress bar of the download to stderr
        kwargs: keyword args of [`UNetp`][holocron.models.segmentation.unetpp.UNetp]

    Returns:
        semantic segmentation model
    """
    return _unet("unetp", pretrained, progress, **kwargs)  # type: ignore[return-value]
```

#### UNetpp

```python
UNetpp(layout: list[int], in_channels: int = 3, num_classes: int = 10, act_layer: Module | None = None, norm_layer: Callable[[int], Module] | None = None, drop_layer: Callable[..., Module] | None = None, conv_layer: Callable[..., Module] | None = None)
```

Implements a UNet++ architecture

| PARAMETER     | DESCRIPTION                                                             |
| ------------- | ----------------------------------------------------------------------- |
| `layout`      | number of channels after each contracting block **TYPE:** `list[int]`   |
| `in_channels` | number of channels in the input tensor **TYPE:** `int` **DEFAULT:** `3` |
| `num_classes` | number of output classes **TYPE:** `int` **DEFAULT:** `10`              |
| `act_layer`   | activation layer **TYPE:** \`Module                                     |
| `norm_layer`  | normalization layer **TYPE:** \`Callable\[[int], Module\]               |
| `drop_layer`  | dropout layer **TYPE:** \`Callable[..., Module]                         |
| `conv_layer`  | convolutional layer **TYPE:** \`Callable[..., Module]                   |

Source code in `holocron/models/segmentation/unetpp.py`

```python
def __init__(
    self,
    layout: list[int],
    in_channels: int = 3,
    num_classes: int = 10,
    act_layer: nn.Module | None = None,
    norm_layer: Callable[[int], nn.Module] | None = None,
    drop_layer: Callable[..., nn.Module] | None = None,
    conv_layer: Callable[..., nn.Module] | None = None,
) -> None:
    super().__init__()

    if act_layer is None:
        act_layer = nn.ReLU(inplace=True)

    # Contracting path
    self.encoder = nn.ModuleList([])
    layout_ = [in_channels, *layout]
    pool = False
    for in_chan, out_chan in pairwise(layout_):
        self.encoder.append(down_path(in_chan, out_chan, pool, 1, act_layer, norm_layer, drop_layer, conv_layer))
        pool = True

    self.bridge = nn.Sequential(
        nn.MaxPool2d((2, 2)),
        *conv_sequence(
            layout[-1], 2 * layout[-1], act_layer, norm_layer, drop_layer, conv_layer, kernel_size=3, padding=1
        ),
        *conv_sequence(
            2 * layout[-1], layout[-1], act_layer, norm_layer, drop_layer, conv_layer, kernel_size=3, padding=1
        ),
    )

    # Expansive path
    self.decoder = nn.ModuleList([])
    layout_ = [layout[-1], *layout[1:][::-1]]
    for left_chan, up_chan, num_cells in zip(layout[::-1], layout_, range(1, len(layout) + 1), strict=True):
        self.decoder.append(
            nn.ModuleList([
                UpPath(
                    up_chan + (idx + 1) * left_chan,
                    left_chan,
                    True,
                    1,
                    act_layer,
                    norm_layer,
                    drop_layer,
                    conv_layer,
                )
                for idx in range(num_cells)
            ])
        )

    # Classifier
    self.classifier = nn.Conv2d(layout[0], num_classes, 1)

    init_module(self, "relu")
```

#### unetpp

```python
unetpp(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> UNetpp
```

UNet++ from ["UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation"](https://arxiv.org/pdf/1912.05074.pdf)

| PARAMETER    | DESCRIPTION                                                                                     |
| ------------ | ----------------------------------------------------------------------------------------------- |
| `pretrained` | If True, returns a model pre-trained on PASCAL VOC2012 **TYPE:** `bool` **DEFAULT:** `False`    |
| `progress`   | If True, displays a progress bar of the download to stderr **TYPE:** `bool` **DEFAULT:** `True` |
| `kwargs`     | keyword args of UNetpp **TYPE:** `Any` **DEFAULT:** `{}`                                        |

| RETURNS  | DESCRIPTION                 |
| -------- | --------------------------- |
| `UNetpp` | semantic segmentation model |

Source code in `holocron/models/segmentation/unetpp.py`

```python
def unetpp(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> UNetpp:
    """UNet++ from ["UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation"](https://arxiv.org/pdf/1912.05074.pdf)

    ![UNet++ architecture](https://github.com/frgfm/Holocron/releases/download/v0.1.3/unetpp.png)

    Args:
        pretrained: If True, returns a model pre-trained on PASCAL VOC2012
        progress: If True, displays a progress bar of the download to stderr
        kwargs: keyword args of [`UNetpp`][holocron.models.segmentation.unetpp.UNetpp]

    Returns:
        semantic segmentation model
    """
    return _unet("unetpp", pretrained, progress, **kwargs)  # type: ignore[return-value]
```

#### UNet3p

```python
UNet3p(layout: list[int], in_channels: int = 3, num_classes: int = 10, act_layer: Module | None = None, norm_layer: Callable[[int], Module] | None = None, drop_layer: Callable[..., Module] | None = None, conv_layer: Callable[..., Module] | None = None)
```

Implements a UNet3+ architecture

| PARAMETER     | DESCRIPTION                                                             |
| ------------- | ----------------------------------------------------------------------- |
| `layout`      | number of channels after each contracting block **TYPE:** `list[int]`   |
| `in_channels` | number of channels in the input tensor **TYPE:** `int` **DEFAULT:** `3` |
| `num_classes` | number of output classes **TYPE:** `int` **DEFAULT:** `10`              |
| `act_layer`   | activation layer **TYPE:** \`Module                                     |
| `norm_layer`  | normalization layer **TYPE:** \`Callable\[[int], Module\]               |
| `drop_layer`  | dropout layer **TYPE:** \`Callable[..., Module]                         |
| `conv_layer`  | convolutional layer **TYPE:** \`Callable[..., Module]                   |

Source code in `holocron/models/segmentation/unet3p.py`

```python
def __init__(
    self,
    layout: list[int],
    in_channels: int = 3,
    num_classes: int = 10,
    act_layer: nn.Module | None = None,
    norm_layer: Callable[[int], nn.Module] | None = None,
    drop_layer: Callable[..., nn.Module] | None = None,
    conv_layer: Callable[..., nn.Module] | None = None,
) -> None:
    super().__init__()

    if act_layer is None:
        act_layer = nn.ReLU(inplace=True)
    if norm_layer is None:
        norm_layer = nn.BatchNorm2d

    # Contracting path
    self.encoder = nn.ModuleList([])
    layout_ = [in_channels, *layout]
    pool = False
    for in_chan, out_chan in pairwise(layout_):
        self.encoder.append(down_path(in_chan, out_chan, pool, 1, act_layer, norm_layer, drop_layer, conv_layer))
        pool = True

    # Expansive path
    self.decoder = nn.ModuleList([])
    for row in range(len(layout) - 1):
        self.decoder.append(
            FSAggreg(
                layout[:row],
                layout[row],
                [len(layout) * layout[0]] * (len(layout) - 2 - row) + layout[-1:],
                act_layer,
                norm_layer,
                drop_layer,
                conv_layer,
            )
        )

    # Classifier
    self.classifier = nn.Conv2d(len(layout) * layout[0], num_classes, 1)

    init_module(self, "relu")
```

#### unet3p

```python
unet3p(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> UNet3p
```

UNet3+ from ["UNet 3+: A Full-Scale Connected UNet For Medical Image Segmentation"](https://arxiv.org/pdf/2004.08790.pdf)

| PARAMETER    | DESCRIPTION                                                                                     |
| ------------ | ----------------------------------------------------------------------------------------------- |
| `pretrained` | If True, returns a model pre-trained on PASCAL VOC2012 **TYPE:** `bool` **DEFAULT:** `False`    |
| `progress`   | If True, displays a progress bar of the download to stderr **TYPE:** `bool` **DEFAULT:** `True` |
| `kwargs`     | keyword args of UNet3p **TYPE:** `Any` **DEFAULT:** `{}`                                        |

| RETURNS  | DESCRIPTION                 |
| -------- | --------------------------- |
| `UNet3p` | semantic segmentation model |

Source code in `holocron/models/segmentation/unet3p.py`

```python
def unet3p(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> UNet3p:
    """UNet3+ from
    ["UNet 3+: A Full-Scale Connected UNet For Medical Image Segmentation"](https://arxiv.org/pdf/2004.08790.pdf)

    ![UNet 3+ architecture](https://github.com/frgfm/Holocron/releases/download/v0.1.3/unet3p.png)

    Args:
        pretrained: If True, returns a model pre-trained on PASCAL VOC2012
        progress: If True, displays a progress bar of the download to stderr
        kwargs: keyword args of [`UNet3p`][holocron.models.segmentation.unet3p.UNet3p]

    Returns:
        semantic segmentation model
    """
    return _unet("unet3p", pretrained, progress, **kwargs)  # type: ignore[return-value]
```

# holocron.nn

An addition to the `torch.nn` module of Pytorch to extend the range of neural networks building blocks.

## Non-linear activations

### HardMish

```python
HardMish(inplace: bool = False)
```

Bases: `_Activation`

Implements the Hard Mish activation module from ["H-Mish"](https://github.com/digantamisra98/H-Mish).

This activation is computed as follows:

[ f(x) = \\frac{x}{2} \\cdot \\min(2, \\max(0, x + 2)) ]

| PARAMETER | DESCRIPTION                                                                     |
| --------- | ------------------------------------------------------------------------------- |
| `inplace` | should the operation be performed inplace **TYPE:** `bool` **DEFAULT:** `False` |

Source code in `holocron/nn/modules/activation.py`

```python
def __init__(self, inplace: bool = False) -> None:
    super().__init__()
    self.inplace: bool = inplace
```

### NLReLU

```python
NLReLU(beta: float = 1.0, inplace: bool = False)
```

Bases: `_Activation`

Implements the Natural-Logarithm ReLU activation module from ["Natural-Logarithm-Rectified Activation Function in Convolutional Neural Networks"](https://arxiv.org/pdf/1908.03682.pdf).

This activation is computed as follows:

[ f(x) = ln(1 + \\beta \\cdot max(0, x)) ]

| PARAMETER | DESCRIPTION                                                                     |
| --------- | ------------------------------------------------------------------------------- |
| `beta`    | beta used for NReLU **TYPE:** `float` **DEFAULT:** `1.0`                        |
| `inplace` | should the operation be performed inplace **TYPE:** `bool` **DEFAULT:** `False` |

Source code in `holocron/nn/modules/activation.py`

```python
def __init__(self, beta: float = 1.0, inplace: bool = False) -> None:
    super().__init__(inplace)
    self.beta: float = beta
```

### FReLU

```python
FReLU(in_channels: int, kernel_size: int = 3)
```

Bases: `Module`

Implements the Funnel activation module from ["Funnel Activation for Visual Recognition"](https://arxiv.org/pdf/2007.11824.pdf).

This activation is computed as follows:

[ f(x) = max(\\mathbb{T}(x), x) ]

where the (\\mathbb{T}) is the spatial contextual feature extraction. It is a convolution filter of size `kernel_size`, same padding and groups equal to the number of input channels, followed by a batch normalization.

| PARAMETER     | DESCRIPTION                                                     |
| ------------- | --------------------------------------------------------------- |
| `in_channels` | number of input channels **TYPE:** `int`                        |
| `kernel_size` | size of the convolution filter **TYPE:** `int` **DEFAULT:** `3` |

Source code in `holocron/nn/modules/activation.py`

```python
def __init__(self, in_channels: int, kernel_size: int = 3) -> None:
    super().__init__()
    self.conv = nn.Conv2d(in_channels, in_channels, kernel_size, padding=kernel_size // 2, groups=in_channels)
    self.bn = nn.BatchNorm2d(in_channels)
```

## Loss functions

### Loss

```python
Loss(weight: float | list[float] | Tensor | None = None, ignore_index: int = -100, reduction: str = 'mean')
```

Bases: `Module`

Base loss class.

| PARAMETER      | DESCRIPTION                                                                                                  |
| -------------- | ------------------------------------------------------------------------------------------------------------ |
| `weight`       | class weight for loss computation **TYPE:** \`float                                                          |
| `ignore_index` | specifies target value that is ignored and do not contribute to gradient **TYPE:** `int` **DEFAULT:** `-100` |
| `reduction`    | type of reduction to apply to the final loss **TYPE:** `str` **DEFAULT:** `'mean'`                           |

| RAISES                | DESCRIPTION                              |
| --------------------- | ---------------------------------------- |
| `NotImplementedError` | if the reduction method is not supported |

Source code in `holocron/nn/modules/loss.py`

```python
def __init__(
    self,
    weight: float | list[float] | Tensor | None = None,
    ignore_index: int = -100,
    reduction: str = "mean",
) -> None:
    super().__init__()
    # Cast class weights if possible
    self.weight: Tensor | None
    if isinstance(weight, (float, int)):
        self.register_buffer("weight", torch.Tensor([weight, 1 - weight]))
    elif isinstance(weight, list):
        self.register_buffer("weight", torch.Tensor(weight))
    elif isinstance(weight, Tensor):
        self.register_buffer("weight", weight)
    else:
        self.weight: Tensor | None = None
    self.ignore_index: int = ignore_index
    # Set the reduction method
    if reduction not in {"none", "mean", "sum"}:
        raise NotImplementedError("argument reduction received an incorrect input")
    self.reduction: str = reduction
```

### FocalLoss

```python
FocalLoss(gamma: float = 2.0, **kwargs: Any)
```

Bases: `Loss`

Implementation of Focal Loss as described in ["Focal Loss for Dense Object Detection"](https://arxiv.org/pdf/1708.02002.pdf).

While the weighted cross-entropy is described by:

[ CE(p_t) = -\\alpha_t log(p_t) ]

where (\\alpha_t) is the loss weight of class (t), and (p_t) is the predicted probability of class (t).

the focal loss introduces a modulating factor

[ FL(p_t) = -\\alpha_t (1 - p_t)^\\gamma log(p_t) ]

where (\\gamma) is a positive focusing parameter.

| PARAMETER  | DESCRIPTION                                                               |
| ---------- | ------------------------------------------------------------------------- |
| `gamma`    | exponent parameter of the focal loss **TYPE:** `float` **DEFAULT:** `2.0` |
| `**kwargs` | keyword args of Loss **TYPE:** `Any` **DEFAULT:** `{}`                    |

Source code in `holocron/nn/modules/loss.py`

```python
def __init__(self, gamma: float = 2.0, **kwargs: Any) -> None:
    super().__init__(**kwargs)
    self.gamma: float = gamma
```

### MultiLabelCrossEntropy

```python
MultiLabelCrossEntropy(*args: Any, **kwargs: Any)
```

Bases: `Loss`

Implementation of the cross-entropy loss for multi-label targets

| PARAMETER  | DESCRIPTION                                            |
| ---------- | ------------------------------------------------------ |
| `*args`    | args of Loss **TYPE:** `Any` **DEFAULT:** `()`         |
| `**kwargs` | keyword args of Loss **TYPE:** `Any` **DEFAULT:** `{}` |

Source code in `holocron/nn/modules/loss.py`

```python
def __init__(self, *args: Any, **kwargs: Any) -> None:
    super().__init__(*args, **kwargs)
```

### ComplementCrossEntropy

```python
ComplementCrossEntropy(gamma: float = -1, **kwargs: Any)
```

Bases: `Loss`

Implements the complement cross entropy loss from ["Imbalanced Image Classification with Complement Cross Entropy"](https://arxiv.org/pdf/2009.02189.pdf)

| PARAMETER  | DESCRIPTION                                            |
| ---------- | ------------------------------------------------------ |
| `gamma`    | smoothing factor **TYPE:** `float` **DEFAULT:** `-1`   |
| `**kwargs` | keyword args of Loss **TYPE:** `Any` **DEFAULT:** `{}` |

Source code in `holocron/nn/modules/loss.py`

```python
def __init__(self, gamma: float = -1, **kwargs: Any) -> None:
    super().__init__(**kwargs)
    self.gamma: float = gamma
```

### MutualChannelLoss

```python
MutualChannelLoss(weight: float | list[float] | Tensor | None = None, ignore_index: int = -100, reduction: str = 'mean', xi: int = 2, alpha: float = 1)
```

Bases: `Loss`

Implements the mutual channel loss from ["The Devil is in the Channels: Mutual-Channel Loss for Fine-Grained Image Classification"](https://arxiv.org/pdf/2002.04264.pdf).

| PARAMETER      | DESCRIPTION                                                                                                  |
| -------------- | ------------------------------------------------------------------------------------------------------------ |
| `weight`       | class weight for loss computation **TYPE:** \`float                                                          |
| `ignore_index` | specifies target value that is ignored and do not contribute to gradient **TYPE:** `int` **DEFAULT:** `-100` |
| `reduction`    | type of reduction to apply to the final loss **TYPE:** `str` **DEFAULT:** `'mean'`                           |
| `xi`           | num of features per class **TYPE:** `int` **DEFAULT:** `2`                                                   |
| `alpha`        | diversity factor **TYPE:** `float` **DEFAULT:** `1`                                                          |

Source code in `holocron/nn/modules/loss.py`

```python
def __init__(
    self,
    weight: float | list[float] | Tensor | None = None,
    ignore_index: int = -100,
    reduction: str = "mean",
    xi: int = 2,
    alpha: float = 1,
) -> None:
    super().__init__(weight, ignore_index, reduction)
    self.xi: int = xi
    self.alpha: float = alpha
```

### DiceLoss

```python
DiceLoss(weight: float | list[float] | Tensor | None = None, gamma: float = 1.0, eps: float = 1e-08)
```

Bases: `Loss`

Implements the dice loss from ["V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation"](https://arxiv.org/pdf/1606.04797.pdf).

| PARAMETER | DESCRIPTION                                                                        |
| --------- | ---------------------------------------------------------------------------------- |
| `weight`  | class weight for loss computation **TYPE:** \`float                                |
| `gamma`   | recall/precision control param **TYPE:** `float` **DEFAULT:** `1.0`                |
| `eps`     | small value added to avoid division by zero **TYPE:** `float` **DEFAULT:** `1e-08` |

Source code in `holocron/nn/modules/loss.py`

```python
def __init__(
    self,
    weight: float | list[float] | Tensor | None = None,
    gamma: float = 1.0,
    eps: float = 1e-8,
) -> None:
    super().__init__(weight)
    self.gamma: float = gamma
    self.eps: float = eps
```

### PolyLoss

```python
PolyLoss(*args: Any, eps: float = 2.0, **kwargs: Any)
```

Bases: `Loss`

Implements the Poly1 loss from ["PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions"](https://arxiv.org/pdf/2204.12511.pdf).

| PARAMETER  | DESCRIPTION                                                   |
| ---------- | ------------------------------------------------------------- |
| `*args`    | args of Loss **TYPE:** `Any` **DEFAULT:** `()`                |
| `eps`      | epsilon 1 from the paper **TYPE:** `float` **DEFAULT:** `2.0` |
| `**kwargs` | keyword args of Loss **TYPE:** `Any` **DEFAULT:** `{}`        |

Source code in `holocron/nn/modules/loss.py`

```python
def __init__(
    self,
    *args: Any,
    eps: float = 2.0,
    **kwargs: Any,
) -> None:
    super().__init__(*args, **kwargs)
    self.eps: float = eps
```

## Loss wrappers

### ClassBalancedWrapper

```python
ClassBalancedWrapper(criterion: Module, num_samples: Tensor, beta: float = 0.99)
```

Bases: `Module`

Implementation of the class-balanced loss as described in ["Class-Balanced Loss Based on Effective Number of Samples"](https://arxiv.org/pdf/1901.05555.pdf).

Given a loss function (\\mathcal{L}), the class-balanced loss is described by:

[ CB(p, y) = \\frac{1 - \\beta}{1 - \\beta^{n_y}} \\mathcal{L}(p, y) ]

where (p) is the predicted probability for class (y), (n_y) is the number of training samples for class (y), and (\\beta) is exponential factor.

| PARAMETER     | DESCRIPTION                                                |
| ------------- | ---------------------------------------------------------- |
| `criterion`   | loss module **TYPE:** `Module`                             |
| `num_samples` | number of samples for each class **TYPE:** `Tensor`        |
| `beta`        | rebalancing exponent **TYPE:** `float` **DEFAULT:** `0.99` |

Source code in `holocron/nn/modules/loss.py`

```python
def __init__(self, criterion: nn.Module, num_samples: Tensor, beta: float = 0.99) -> None:
    super().__init__()
    self.criterion = criterion
    self.beta: float = beta
    cb_weights = (1 - beta) / (1 - beta**num_samples)
    if self.criterion.weight is None:
        self.criterion.weight: Tensor | None = cb_weights
    else:
        self.criterion.weight *= cb_weights.to(device=self.criterion.weight.device)  # ty: ignore[invalid-argument-type,possibly-missing-attribute]
```

## Convolution layers

### NormConv2d

```python
NormConv2d(in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = True, padding_mode: Literal['zeros', 'reflect', 'replicate', 'circular'] = 'zeros', eps: float = 1e-14)
```

Bases: `_NormConvNd`

Implements the normalized convolution module from ["Normalized Convolutional Neural Network"](https://arxiv.org/pdf/2005.05274v2.pdf).

In the simplest case, the output value of the layer with input size ((N, C\_{in}, H, W)) and output ((N, C\_{out}, H\_{out}, W\_{out})) can be precisely described as:

[ out(N_i, C\_{out_j}) = bias(C\_{out_j}) + \\sum\_{k = 0}^{C\_{in} - 1} weight(C\_{out_j}, k) \\star \\frac{input(N_i, k) - \\mu(N_i, k)}{\\sqrt{\\sigma^2(N_i, k) + \\epsilon}} ]

where: (\\star) is the valid 2D cross-correlation operator, (\\mu(N_i, k)) and (\\sigma²(N_i, k)) are the mean and variance of (input(N_i, k)) over all slices, (N) is a batch size, (C) denotes a number of channels, (H) is a height of input planes in pixels, and (W) is width in pixels.

| PARAMETER      | DESCRIPTION                                                                                                               |
| -------------- | ------------------------------------------------------------------------------------------------------------------------- |
| `in_channels`  | Number of channels in the input image **TYPE:** `int`                                                                     |
| `out_channels` | Number of channels produced by the convolution **TYPE:** `int`                                                            |
| `kernel_size`  | Size of the convolving kernel **TYPE:** `int`                                                                             |
| `stride`       | Stride of the convolution. **TYPE:** `int` **DEFAULT:** `1`                                                               |
| `padding`      | Zero-padding added to both sides of the input. **TYPE:** `int` **DEFAULT:** `0`                                           |
| `dilation`     | Spacing between kernel elements. **TYPE:** `int` **DEFAULT:** `1`                                                         |
| `groups`       | Number of blocked connections from input channels to output channels. **TYPE:** `int` **DEFAULT:** `1`                    |
| `bias`         | If True, adds a learnable bias to the output. **TYPE:** `bool` **DEFAULT:** `True`                                        |
| `padding_mode` | padding mode for the convolution. **TYPE:** `Literal['zeros', 'reflect', 'replicate', 'circular']` **DEFAULT:** `'zeros'` |
| `eps`          | a value added to the denominator for numerical stability. **TYPE:** `float` **DEFAULT:** `1e-14`                          |

Source code in `holocron/nn/modules/conv.py`

```python
def __init__(
    self,
    in_channels: int,
    out_channels: int,
    kernel_size: int,
    stride: int = 1,
    padding: int = 0,
    dilation: int = 1,
    groups: int = 1,
    bias: bool = True,
    padding_mode: Literal["zeros", "reflect", "replicate", "circular"] = "zeros",
    eps: float = 1e-14,
) -> None:
    kernel_size = _pair(kernel_size)
    stride = _pair(stride)
    padding = _pair(padding)
    dilation = _pair(dilation)
    super().__init__(
        in_channels,
        out_channels,
        kernel_size,
        stride,
        padding,
        dilation,
        False,
        _pair(0),
        groups,
        bias,
        padding_mode,
        False,
        eps,
    )
```

### Add2d

```python
Add2d(in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = True, padding_mode: Literal['zeros', 'reflect', 'replicate', 'circular'] = 'zeros', normalize_slices: bool = False, eps: float = 1e-14)
```

Bases: `_NormConvNd`

Implements the adder module from ["AdderNet: Do We Really Need Multiplications in Deep Learning?"](https://arxiv.org/pdf/1912.13200.pdf).

In the simplest case, the output value of the layer at position ((m, n)) in channel (c) with filter F of spatial size ((d, d)), intput size ((C\_{in}, H, W)) and output ((C\_{out}, H, W)) can be precisely described as:

[ out(m, n, c) = - \\sum\\limits\_{i=0}^d \\sum\\limits\_{j=0}^d \\sum\\limits\_{k=0}^{C\_{in}} |X(m + i, n + j, k) - F(i, j, k, c)| ]

where (C) denotes a number of channels, (H) is a height of input planes in pixels, and (W) is width in pixels.

| PARAMETER          | DESCRIPTION                                                                                                               |
| ------------------ | ------------------------------------------------------------------------------------------------------------------------- |
| `in_channels`      | Number of channels in the input image **TYPE:** `int`                                                                     |
| `out_channels`     | Number of channels produced by the convolution **TYPE:** `int`                                                            |
| `kernel_size`      | Size of the convolving kernel **TYPE:** `int`                                                                             |
| `stride`           | Stride of the convolution. **TYPE:** `int` **DEFAULT:** `1`                                                               |
| `padding`          | Zero-padding added to both sides of the input. **TYPE:** `int` **DEFAULT:** `0`                                           |
| `dilation`         | Spacing between kernel elements. **TYPE:** `int` **DEFAULT:** `1`                                                         |
| `groups`           | Number of blocked connections from input channels to output channels. **TYPE:** `int` **DEFAULT:** `1`                    |
| `bias`             | If True, adds a learnable bias to the output. **TYPE:** `bool` **DEFAULT:** `True`                                        |
| `padding_mode`     | padding mode for the convolution. **TYPE:** `Literal['zeros', 'reflect', 'replicate', 'circular']` **DEFAULT:** `'zeros'` |
| `normalize_slices` | whether slices should be normalized before performing cross-correlation. **TYPE:** `bool` **DEFAULT:** `False`            |
| `eps`              | a value added to the denominator for numerical stability. **TYPE:** `float` **DEFAULT:** `1e-14`                          |

Source code in `holocron/nn/modules/conv.py`

```python
def __init__(
    self,
    in_channels: int,
    out_channels: int,
    kernel_size: int,
    stride: int = 1,
    padding: int = 0,
    dilation: int = 1,
    groups: int = 1,
    bias: bool = True,
    padding_mode: Literal["zeros", "reflect", "replicate", "circular"] = "zeros",
    normalize_slices: bool = False,
    eps: float = 1e-14,
) -> None:
    kernel_size = _pair(kernel_size)
    stride = _pair(stride)
    padding = _pair(padding)
    dilation = _pair(dilation)
    super().__init__(
        in_channels,
        out_channels,
        kernel_size,
        stride,
        padding,
        dilation,
        False,
        _pair(0),
        groups,
        bias,
        padding_mode,
        normalize_slices,
        eps,
    )
```

### SlimConv2d

```python
SlimConv2d(in_channels: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1, groups: int = 1, bias: bool = True, padding_mode: Literal['zeros', 'reflect', 'replicate', 'circular'] = 'zeros', r: int = 32, L: int = 2)
```

Bases: `Module`

Implements the convolution module from ["SlimConv: Reducing Channel Redundancy in Convolutional Neural Networks by Weights Flipping"](https://arxiv.org/pdf/2003.07469.pdf).

First, we compute channel-wise weights as follows:

[ z(c) = \\frac{1}{H \\cdot W} \\sum\\limits\_{i=1}^H \\sum\\limits\_{j=1}^W X\_{c,i,j} ]

where (X \\in \\mathbb{R}^{C \\times H \\times W}) is the input tensor, (H) is height in pixels, and (W) is width in pixels.

[ w = \\sigma(F\_{fc2}(\\delta(F\_{fc1}(z)))) ]

where (z \\in \\mathbb{R}^{C}) contains channel-wise statistics, (\\sigma) refers to the sigmoid function, (\\delta) refers to the ReLU function, (F\_{fc1}) is a convolution operation with kernel of size ((1, 1)) with (max(C/r, L)) output channels followed by batch normalization, and (F\_{fc2}) is a plain convolution operation with kernel of size ((1, 1)) with (C) output channels.

We then proceed with reconstructing and transforming both pathways:

[ X\_{top} = X \\odot w X\_{bot} = X \\odot \\check{w} ]

where (\\odot) refers to the element-wise multiplication and (\\check{w}) is the channel-wise reverse-flip of (w).

[ T\_{top} = F\_{top}(X\_{top}^{(1)} + X\_{top}^{(2)}) T\_{bot} = F\_{bot}(X\_{bot}^{(1)} + X\_{bot}^{(2)}) ]

where (X^{(1)}) and (X^{(2)}) are the channel-wise first and second halves of (X), (F\_{top}) is a convolution of kernel size ((3, 3)), and (F\_{bot}) is a convolution of kernel size ((1, 1)) reducing channels by half, followed by a convolution of kernel size ((3, 3)).

Finally we fuse both pathways to yield the output:

[ Y = T\_{top} \\oplus T\_{bot} ]

where (\\oplus) is the channel-wise concatenation.

| PARAMETER      | DESCRIPTION                                                                                                               |
| -------------- | ------------------------------------------------------------------------------------------------------------------------- |
| `in_channels`  | Number of channels in the input image **TYPE:** `int`                                                                     |
| `kernel_size`  | Size of the convolving kernel **TYPE:** `int`                                                                             |
| `stride`       | Stride of the convolution. **TYPE:** `int` **DEFAULT:** `1`                                                               |
| `padding`      | Zero-padding added to both sides of the input. **TYPE:** `int` **DEFAULT:** `0`                                           |
| `dilation`     | Spacing between kernel elements. **TYPE:** `int` **DEFAULT:** `1`                                                         |
| `groups`       | Number of blocked connections from input channels to output channels. **TYPE:** `int` **DEFAULT:** `1`                    |
| `bias`         | If True, adds a learnable bias to the output. **TYPE:** `bool` **DEFAULT:** `True`                                        |
| `padding_mode` | padding mode for the convolution. **TYPE:** `Literal['zeros', 'reflect', 'replicate', 'circular']` **DEFAULT:** `'zeros'` |
| `r`            | squeezing divider. **TYPE:** `int` **DEFAULT:** `32`                                                                      |
| `L`            | minimum squeezed channels. **TYPE:** `int` **DEFAULT:** `2`                                                               |

Source code in `holocron/nn/modules/conv.py`

```python
def __init__(
    self,
    in_channels: int,
    kernel_size: int,
    stride: int = 1,
    padding: int = 0,
    dilation: int = 1,
    groups: int = 1,
    bias: bool = True,
    padding_mode: Literal["zeros", "reflect", "replicate", "circular"] = "zeros",
    r: int = 32,
    L: int = 2,  # noqa: N803
) -> None:
    super().__init__()
    self.fc1 = nn.Conv2d(in_channels, max(in_channels // r, L), 1)
    self.bn = nn.BatchNorm2d(max(in_channels // r, L))
    self.fc2 = nn.Conv2d(max(in_channels // r, L), in_channels, 1)
    self.conv_top = nn.Conv2d(
        in_channels // 2, in_channels // 2, kernel_size, stride, padding, dilation, groups, bias, padding_mode
    )
    self.conv_bot1 = nn.Conv2d(in_channels // 2, in_channels // 4, 1)
    self.conv_bot2 = nn.Conv2d(
        in_channels // 4, in_channels // 4, kernel_size, stride, padding, dilation, groups, bias, padding_mode
    )
```

### PyConv2d

```python
PyConv2d(in_channels: int, out_channels: int, kernel_size: int, num_levels: int = 2, padding: int = 0, groups: list[int] | None = None, **kwargs: Any)
```

Bases: `ModuleList`

Implements the convolution module from ["Pyramidal Convolution: Rethinking Convolutional Neural Networks for Visual Recognition"](https://arxiv.org/pdf/2006.11538.pdf).

| PARAMETER      | DESCRIPTION                                                                                 |
| -------------- | ------------------------------------------------------------------------------------------- |
| `in_channels`  | Number of channels in the input image **TYPE:** `int`                                       |
| `out_channels` | Number of channels produced by the convolution **TYPE:** `int`                              |
| `kernel_size`  | Size of the convolving kernel **TYPE:** `int`                                               |
| `num_levels`   | number of stacks in the pyramid. **TYPE:** `int` **DEFAULT:** `2`                           |
| `padding`      | Zero-padding added to both sides of the input. **TYPE:** `int` **DEFAULT:** `0`             |
| `groups`       | Number of blocked connections from input channels to output channels. **TYPE:** \`list[int] |
| `kwargs`       | keyword args of torch.nn.Conv2d. **TYPE:** `Any` **DEFAULT:** `{}`                          |

Source code in `holocron/nn/modules/conv.py`

```python
def __init__(
    self,
    in_channels: int,
    out_channels: int,
    kernel_size: int,
    num_levels: int = 2,
    padding: int = 0,
    groups: list[int] | None = None,
    **kwargs: Any,
) -> None:
    if num_levels == 1:
        super().__init__([
            nn.Conv2d(
                in_channels,
                out_channels,
                kernel_size,
                padding=padding,
                groups=groups[0] if isinstance(groups, list) else 1,
                **kwargs,
            )
        ])
    else:
        exp2 = int(math.log2(num_levels))
        reminder = num_levels - 2**exp2
        out_chans = [out_channels // 2 ** (exp2 + 1)] * (2 * reminder) + [out_channels // 2**exp2] * (
            num_levels - 2 * reminder
        )

        k_sizes = [kernel_size + 2 * idx for idx in range(num_levels)]
        if groups is None:
            groups = [1] + [
                min(2 ** (2 + idx), out_chan)
                for idx, out_chan in zip(range(num_levels - 1), out_chans[1:], strict=True)
            ]
        elif not isinstance(groups, list) or len(groups) != num_levels:
            raise ValueError("The argument `group` is expected to be a list of integer of size `num_levels`.")
        paddings = [padding + idx for idx in range(num_levels)]

        super().__init__([
            nn.Conv2d(in_channels, out_chan, k_size, padding=padding, groups=group, **kwargs)
            for out_chan, k_size, padding, group in zip(out_chans, k_sizes, paddings, groups, strict=True)
        ])
    self.num_levels: int = num_levels
```

### Involution2d

```python
Involution2d(in_channels: int, kernel_size: int, padding: int = 0, stride: int = 1, groups: int = 1, dilation: int = 1, reduction_ratio: float = 1)
```

Bases: `Module`

Implements the convolution module from ["Involution: Inverting the Inherence of Convolution for Visual Recognition"](https://arxiv.org/pdf/2103.06255.pdf), adapted from the proposed PyTorch implementation in the paper.

| PARAMETER         | DESCRIPTION                                                                                            |
| ----------------- | ------------------------------------------------------------------------------------------------------ |
| `in_channels`     | Number of channels in the input image **TYPE:** `int`                                                  |
| `kernel_size`     | Size of the convolving kernel **TYPE:** `int`                                                          |
| `padding`         | Zero-padding added to both sides of the input. **TYPE:** `int` **DEFAULT:** `0`                        |
| `stride`          | Stride of the convolution. **TYPE:** `int` **DEFAULT:** `1`                                            |
| `groups`          | Number of blocked connections from input channels to output channels. **TYPE:** `int` **DEFAULT:** `1` |
| `dilation`        | Spacing between kernel elements. **TYPE:** `int` **DEFAULT:** `1`                                      |
| `reduction_ratio` | reduction ratio of the channels to generate the kernel **TYPE:** `float` **DEFAULT:** `1`              |

Source code in `holocron/nn/modules/conv.py`

```python
def __init__(
    self,
    in_channels: int,
    kernel_size: int,
    padding: int = 0,
    stride: int = 1,
    groups: int = 1,
    dilation: int = 1,
    reduction_ratio: float = 1,
) -> None:
    super().__init__()

    self.groups: int = groups
    self.k_size: int = kernel_size

    self.pool: nn.AvgPool2d | None = nn.AvgPool2d(stride, stride) if stride > 1 else None
    self.reduce = nn.Conv2d(in_channels, int(in_channels // reduction_ratio), 1)
    self.span = nn.Conv2d(int(in_channels // reduction_ratio), kernel_size**2 * groups, 1)
    self.unfold = nn.Unfold(kernel_size, dilation, padding, stride)
```

## Regularization layers

### DropBlock2d

```python
DropBlock2d(p: float = 0.1, block_size: int = 7, inplace: bool = False)
```

Bases: `Module`

Implements the DropBlock module from ["DropBlock: A regularization method for convolutional networks"](https://arxiv.org/pdf/1810.12890.pdf)

| PARAMETER    | DESCRIPTION                                                                                |
| ------------ | ------------------------------------------------------------------------------------------ |
| `p`          | probability of dropping activation value **TYPE:** `float` **DEFAULT:** `0.1`              |
| `block_size` | size of each block that is expended from the sampled mask **TYPE:** `int` **DEFAULT:** `7` |
| `inplace`    | whether the operation should be done inplace **TYPE:** `bool` **DEFAULT:** `False`         |

Source code in `holocron/nn/modules/dropblock.py`

```python
def __init__(self, p: float = 0.1, block_size: int = 7, inplace: bool = False) -> None:
    super().__init__()
    self.p: float = p
    self.block_size: int = block_size
    self.inplace: bool = inplace
```

## Downsampling

### ConcatDownsample2d

```python
ConcatDownsample2d(scale_factor: int)
```

Bases: `Module`

Implements a loss-less downsampling operation described in ["YOLO9000: Better, Faster, Stronger"](https://pjreddie.com/media/files/papers/YOLO9000.pdf) by stacking adjacent information on the channel dimension.

| PARAMETER      | DESCRIPTION                            |
| -------------- | -------------------------------------- |
| `scale_factor` | spatial scaling factor **TYPE:** `int` |

Source code in `holocron/nn/modules/downsample.py`

```python
def __init__(self, scale_factor: int) -> None:
    super().__init__()
    self.scale_factor: int = scale_factor
```

### GlobalAvgPool2d

```python
GlobalAvgPool2d(flatten: bool = False)
```

Bases: `Module`

Fast implementation of global average pooling from ["TResNet: High Performance GPU-Dedicated Architecture"](https://arxiv.org/pdf/2003.13630.pdf)

| PARAMETER | DESCRIPTION                                                                         |
| --------- | ----------------------------------------------------------------------------------- |
| `flatten` | whether spatial dimensions should be squeezed **TYPE:** `bool` **DEFAULT:** `False` |

Source code in `holocron/nn/modules/downsample.py`

```python
def __init__(self, flatten: bool = False) -> None:
    super().__init__()
    self.flatten: bool = flatten
```

### GlobalMaxPool2d

```python
GlobalMaxPool2d(flatten: bool = False)
```

Bases: `Module`

Fast implementation of global max pooling from ["TResNet: High Performance GPU-Dedicated Architecture"](https://arxiv.org/pdf/2003.13630.pdf)

| PARAMETER | DESCRIPTION                                                                         |
| --------- | ----------------------------------------------------------------------------------- |
| `flatten` | whether spatial dimensions should be squeezed **TYPE:** `bool` **DEFAULT:** `False` |

Source code in `holocron/nn/modules/downsample.py`

```python
def __init__(self, flatten: bool = False) -> None:
    super().__init__()
    self.flatten: bool = flatten
```

### BlurPool2d

```python
BlurPool2d(channels: int, kernel_size: int = 3, stride: int = 2)
```

Bases: `Module`

Ross Wightman's [implementation](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/blur_pool.py) of blur pooling module as described in ["Making Convolutional Networks Shift-Invariant Again"](https://arxiv.org/pdf/1904.11486.pdf).

| PARAMETER     | DESCRIPTION                                                                                               |
| ------------- | --------------------------------------------------------------------------------------------------------- |
| `channels`    | Number of input channels **TYPE:** `int`                                                                  |
| `kernel_size` | binomial filter size for blurring. currently supports 3 (default) and 5. **TYPE:** `int` **DEFAULT:** `3` |
| `stride`      | downsampling filter stride **TYPE:** `int` **DEFAULT:** `2`                                               |

Source code in `holocron/nn/modules/downsample.py`

```python
def __init__(self, channels: int, kernel_size: int = 3, stride: int = 2) -> None:
    super().__init__()
    self.channels: int = channels
    if kernel_size <= 1:
        raise AssertionError
    self.kernel_size: int = kernel_size
    self.stride: int = stride
    pad_size = [get_padding(kernel_size, stride, dilation=1)] * 4
    self.padding = nn.ReflectionPad2d(pad_size)  # type: ignore[arg-type]
    self._coeffs = torch.tensor((np.poly1d((0.5, 0.5)) ** (self.kernel_size - 1)).coeffs)  # for torchscript compat
    self.kernel: dict[str, Tensor] = {}  # lazy init by device for DataParallel compat
```

### SPP

```python
SPP(kernel_sizes: list[int])
```

Bases: `ModuleList`

SPP layer from ["Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition"](https://arxiv.org/pdf/1406.4729.pdf).

| PARAMETER      | DESCRIPTION                                        |
| -------------- | -------------------------------------------------- |
| `kernel_sizes` | kernel sizes of each pooling **TYPE:** `list[int]` |

Source code in `holocron/nn/modules/downsample.py`

```python
def __init__(self, kernel_sizes: list[int]) -> None:
    super().__init__([nn.MaxPool2d(k_size, stride=1, padding=k_size // 2) for k_size in kernel_sizes])
```

### ZPool

```python
ZPool(dim: int = 1)
```

Bases: `Module`

Z-pool layer from ["Rotate to Attend: Convolutional Triplet Attention Module"](https://arxiv.org/pdf/2010.03045.pdf).

| PARAMETER | DESCRIPTION                                               |
| --------- | --------------------------------------------------------- |
| `dim`     | dimension to pool across **TYPE:** `int` **DEFAULT:** `1` |

Source code in `holocron/nn/modules/downsample.py`

```python
def __init__(self, dim: int = 1) -> None:
    super().__init__()
    self.dim: int = dim
```

## Attention

### SAM

```python
SAM(in_channels: int)
```

Bases: `Module`

SAM layer from ["CBAM: Convolutional Block Attention Module"](https://arxiv.org/pdf/1807.06521.pdf) modified in ["YOLOv4: Optimal Speed and Accuracy of Object Detection"](https://arxiv.org/pdf/2004.10934.pdf).

| PARAMETER     | DESCRIPTION                    |
| ------------- | ------------------------------ |
| `in_channels` | input channels **TYPE:** `int` |

Source code in `holocron/nn/modules/attention.py`

```python
def __init__(self, in_channels: int) -> None:
    super().__init__()
    self.conv = nn.Conv2d(in_channels, 1, 1)
```

### LambdaLayer

```python
LambdaLayer(in_channels: int, out_channels: int, dim_k: int, n: int | None = None, r: int | None = None, num_heads: int = 4, dim_u: int = 1)
```

Bases: `Module`

Lambda layer from ["LambdaNetworks: Modeling long-range interactions without attention"](https://openreview.net/pdf?id=xTJEN-ggl1b). The implementation was adapted from [lucidrains](https://github.com/lucidrains/lambda-networks/blob/main/lambda_networks/lambda_networks.py).

| PARAMETER      | DESCRIPTION                                                      |
| -------------- | ---------------------------------------------------------------- |
| `in_channels`  | input channels **TYPE:** `int`                                   |
| `out_channels` | output channels **TYPE:** `int`                                  |
| `dim_k`        | key dimension **TYPE:** `int`                                    |
| `n`            | number of input pixels **TYPE:** \`int                           |
| `r`            | receptive field for relative positional encoding **TYPE:** \`int |
| `num_heads`    | number of attention heads **TYPE:** `int` **DEFAULT:** `4`       |
| `dim_u`        | intra-depth dimension **TYPE:** `int` **DEFAULT:** `1`           |

Source code in `holocron/nn/modules/lambda_layer.py`

```python
def __init__(
    self,
    in_channels: int,
    out_channels: int,
    dim_k: int,
    n: int | None = None,
    r: int | None = None,
    num_heads: int = 4,
    dim_u: int = 1,
) -> None:
    super().__init__()
    self.u: int = dim_u
    self.num_heads: int = num_heads

    if out_channels % num_heads != 0:
        raise AssertionError("values dimension must be divisible by number of heads for multi-head query")
    dim_v = out_channels // num_heads

    # Project input and context to get queries, keys & values
    self.to_q = nn.Conv2d(in_channels, dim_k * num_heads, 1, bias=False)
    self.to_k = nn.Conv2d(in_channels, dim_k * dim_u, 1, bias=False)
    self.to_v = nn.Conv2d(in_channels, dim_v * dim_u, 1, bias=False)

    self.norm_q = nn.BatchNorm2d(dim_k * num_heads)
    self.norm_v = nn.BatchNorm2d(dim_v * dim_u)

    self.local_contexts: bool = r is not None
    if r is not None:
        if r % 2 != 1:
            raise AssertionError("Receptive kernel size should be odd")
        self.padding: int = r // 2
        self.R = nn.Parameter(torch.randn(dim_k, dim_u, 1, r, r))
    else:
        if n is None:
            raise AssertionError("You must specify the total sequence length (h x w)")
        self.pos_emb = nn.Parameter(torch.randn(n, n, dim_k, dim_u))
```

### TripletAttention

```python
TripletAttention()
```

Bases: `Module`

Triplet attention layer from ["Rotate to Attend: Convolutional Triplet Attention Module"](https://arxiv.org/pdf/2010.03045.pdf). This implementation is based on the [one](https://github.com/LandskapeAI/triplet-attention/blob/master/MODELS/triplet_attention.py) from the paper's authors.

Source code in `holocron/nn/modules/attention.py`

```python
def __init__(self) -> None:
    super().__init__()
    self.c_branch = DimAttention(dim=1)
    self.h_branch = DimAttention(dim=2)
    self.w_branch = DimAttention(dim=3)
```

# holocron.ops

`holocron.ops` implements operators that are specific for Computer Vision.

Note

Those operators currently do not support TorchScript.

## Boxes

### box_giou

```python
box_giou(boxes1: Tensor, boxes2: Tensor) -> Tensor
```

Computes the Generalized-IoU as described in ["Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression"](https://arxiv.org/pdf/1902.09630.pdf). This implementation was adapted from https://github.com/facebookresearch/detr/blob/master/util/box_ops.py

The generalized IoU is defined as follows:

[ GIoU = IoU - \\frac{|C - A \\cup B|}{|C|} ]

where (\\IoU) is the Intersection over Union, (A \\cup B) is the area of the boxes' union, and (C) is the area of the smallest enclosing box covering the two boxes.

| PARAMETER | DESCRIPTION                                       |
| --------- | ------------------------------------------------- |
| `boxes1`  | bounding boxes of shape [M, 4] **TYPE:** `Tensor` |
| `boxes2`  | bounding boxes of shape [N, 4] **TYPE:** `Tensor` |

| RETURNS  | DESCRIPTION                     |
| -------- | ------------------------------- |
| `Tensor` | Generalized-IoU of shape [M, N] |

| RAISES           | DESCRIPTION                                     |
| ---------------- | ----------------------------------------------- |
| `AssertionError` | if the boxes are in incorrect coordinate format |

Source code in `holocron/ops/boxes.py`

```python
def box_giou(boxes1: Tensor, boxes2: Tensor) -> Tensor:
    r"""Computes the Generalized-IoU as described in ["Generalized Intersection over Union: A Metric and A Loss
    for Bounding Box Regression"](https://arxiv.org/pdf/1902.09630.pdf). This implementation was adapted
    from https://github.com/facebookresearch/detr/blob/master/util/box_ops.py

    The generalized IoU is defined as follows:

    $$
    GIoU = IoU - \frac{|C - A \cup B|}{|C|}
    $$

    where $\IoU$ is the Intersection over Union,
    $A \cup B$ is the area of the boxes' union,
    and $C$ is the area of the smallest enclosing box covering the two boxes.

    Args:
        boxes1: bounding boxes of shape [M, 4]
        boxes2: bounding boxes of shape [N, 4]

    Returns:
        Generalized-IoU of shape [M, N]

    Raises:
        AssertionError: if the boxes are in incorrect coordinate format
    """
    # degenerate boxes gives inf / nan results
    # so do an early check
    if torch.any(boxes1[:, 2:] < boxes1[:, :2]) or torch.any(boxes2[:, 2:] < boxes2[:, :2]):
        raise AssertionError("Incorrect coordinate format")
    iou, union = _box_iou(boxes1, boxes2)

    lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])
    rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])

    wh = (rb - lt).clamp(min=0)  # [N,M,2]
    area = wh[:, :, 0] * wh[:, :, 1]

    return iou - (area - union) / area
```

### diou_loss

```python
diou_loss(boxes1: Tensor, boxes2: Tensor) -> Tensor
```

Computes the Distance-IoU loss as described in ["Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression"](https://arxiv.org/pdf/1911.08287.pdf).

The loss is defined as follows:

[ \\mathcal{L}\_{DIoU} = 1 - IoU + \\frac{\\rho^2(b, b^{GT})}{c^2} ]

where (\\IoU) is the Intersection over Union, (b) and (b^{GT}) are the centers of the box and the ground truth box respectively, (c) c is the diagonal length of the smallest enclosing box covering the two boxes, and (\\rho(.)) is the Euclidean distance.

| PARAMETER | DESCRIPTION                                       |
| --------- | ------------------------------------------------- |
| `boxes1`  | bounding boxes of shape [M, 4] **TYPE:** `Tensor` |
| `boxes2`  | bounding boxes of shape [N, 4] **TYPE:** `Tensor` |

| RETURNS  | DESCRIPTION                       |
| -------- | --------------------------------- |
| `Tensor` | Distance-IoU loss of shape [M, N] |

Source code in `holocron/ops/boxes.py`

```python
def diou_loss(boxes1: Tensor, boxes2: Tensor) -> Tensor:
    r"""Computes the Distance-IoU loss as described in ["Distance-IoU Loss: Faster and Better Learning for
    Bounding Box Regression"](https://arxiv.org/pdf/1911.08287.pdf).

    The loss is defined as follows:

    $$
    \mathcal{L}_{DIoU} = 1 - IoU + \frac{\rho^2(b, b^{GT})}{c^2}
    $$

    where $\IoU$ is the Intersection over Union,
    $b$ and $b^{GT}$ are the centers of the box and the ground truth box respectively,
    $c$ c is the diagonal length of the smallest enclosing box covering the two boxes,
    and $\rho(.)$ is the Euclidean distance.

    ![Distance-IoU loss](https://github.com/frgfm/Holocron/releases/download/v0.1.3/diou_loss.png)

    Args:
        boxes1: bounding boxes of shape [M, 4]
        boxes2: bounding boxes of shape [N, 4]

    Returns:
        Distance-IoU loss of shape [M, N]
    """
    return 1 - box_iou(boxes1, boxes2) + iou_penalty(boxes1, boxes2)
```

### ciou_loss

```python
ciou_loss(boxes1: Tensor, boxes2: Tensor) -> Tensor
```

Computes the Complete IoU loss as described in ["Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression"](https://arxiv.org/pdf/1911.08287.pdf).

The loss is defined as follows:

[ \\mathcal{L}\_{CIoU} = 1 - IoU + \\frac{\\rho^2(b, b^{GT})}{c^2} + \\alpha v ]

where (\\IoU) is the Intersection over Union, (b) and (b^{GT}) are the centers of the box and the ground truth box respectively, (c) c is the diagonal length of the smallest enclosing box covering the two boxes, (\\rho(.)) is the Euclidean distance, (\\alpha) is a positive trade-off parameter, and (v) is the aspect ratio consistency.

More specifically:

[ v = \\frac{4}{\\pi^2} \\Big(\\arctan{\\frac{w^{GT}}{h^{GT}}} - \\arctan{\\frac{w}{h}}\\Big)^2 ]

and

[ \\alpha = \\frac{v}{(1 - IoU) + v} ]

| PARAMETER | DESCRIPTION                                       |
| --------- | ------------------------------------------------- |
| `boxes1`  | bounding boxes of shape [M, 4] **TYPE:** `Tensor` |
| `boxes2`  | bounding boxes of shape [N, 4] **TYPE:** `Tensor` |

| RETURNS  | DESCRIPTION                       |
| -------- | --------------------------------- |
| `Tensor` | Complete IoU loss of shape [M, N] |

Example

```python
import torch
from holocron.ops.boxes import box_ciou
boxes1 = torch.tensor([[0, 0, 100, 100], [100, 100, 200, 200]], dtype=torch.float32)
boxes2 = torch.tensor([[50, 50, 150, 150]], dtype=torch.float32)
box_ciou(boxes1, boxes2)
```

Source code in `holocron/ops/boxes.py`

````python
def ciou_loss(boxes1: Tensor, boxes2: Tensor) -> Tensor:
    r"""Computes the Complete IoU loss as described in ["Distance-IoU Loss: Faster and Better Learning for
    Bounding Box Regression"](https://arxiv.org/pdf/1911.08287.pdf).

    The loss is defined as follows:

    $$
    \mathcal{L}_{CIoU} = 1 - IoU + \frac{\rho^2(b, b^{GT})}{c^2} + \alpha v
    $$

    where $\IoU$ is the Intersection over Union,
    $b$ and $b^{GT}$ are the centers of the box and the ground truth box respectively,
    $c$ c is the diagonal length of the smallest enclosing box covering the two boxes,
    $\rho(.)$ is the Euclidean distance,
    $\alpha$ is a positive trade-off parameter,
    and $v$ is the aspect ratio consistency.

    More specifically:

    $$
    v = \frac{4}{\pi^2} \Big(\arctan{\frac{w^{GT}}{h^{GT}}} - \arctan{\frac{w}{h}}\Big)^2
    $$

    and

    $$
    \alpha = \frac{v}{(1 - IoU) + v}
    $$

    Args:
        boxes1: bounding boxes of shape [M, 4]
        boxes2: bounding boxes of shape [N, 4]

    Returns:
        Complete IoU loss of shape [M, N]

    Example:
        ```python
        import torch
        from holocron.ops.boxes import box_ciou
        boxes1 = torch.tensor([[0, 0, 100, 100], [100, 100, 200, 200]], dtype=torch.float32)
        boxes2 = torch.tensor([[50, 50, 150, 150]], dtype=torch.float32)
        box_ciou(boxes1, boxes2)
        ```
    """
    iou = box_iou(boxes1, boxes2)
    v = aspect_ratio_consistency(boxes1, boxes2)

    ciou_loss = 1 - iou + iou_penalty(boxes1, boxes2)

    # Check
    filter_ = (v != 0) & (iou != 0)
    ciou_loss[filter_].addcdiv_(v[filter_], 1 - iou[filter_] + v[filter_])

    return ciou_loss
````

# holocron.optim

To use `holocron.optim` you have to construct an optimizer object, that will hold the current state and will update the parameters based on the computed gradients.

## Optimizers

Implementations of recent parameter optimizer for Pytorch modules.

### LARS

```python
LARS(params: Iterable[Parameter], lr: float = 0.001, momentum: float = 0.0, dampening: float = 0.0, weight_decay: float = 0.0, nesterov: bool = False, scale_clip: tuple[float, float] | None = None)
```

Bases: `Optimizer`

Implements the LARS optimizer from ["Large batch training of convolutional networks"](https://arxiv.org/pdf/1708.03888.pdf).

The estimation of global and local learning rates is described as follows, (\\forall t \\geq 1):

[ \\alpha_t \\leftarrow \\alpha (1 - t / T)^2 \\ \\gamma_t \\leftarrow \\frac{\\lVert \\theta_t \\rVert}{\\lVert g_t \\rVert + \\lambda \\lVert \\theta_t \\rVert} ]

where (\\theta_t) is the parameter value at step (t) ((\\theta_0) being the initialization value), (g_t) is the gradient of (\\theta_t), (T) is the total number of steps, (\\alpha) is the learning rate (\\lambda \\geq 0) is the weight decay.

Then we estimate the momentum using:

[ v_t \\leftarrow m v\_{t-1} + \\alpha_t \\gamma_t (g_t + \\lambda \\theta_t) ]

where (m) is the momentum and (v_0 = 0).

And finally the update step is performed using the following rule:

[ \\theta_t \\leftarrow \\theta\_{t-1} - v_t ]

| PARAMETER      | DESCRIPTION                                                                                           |
| -------------- | ----------------------------------------------------------------------------------------------------- |
| `params`       | iterable of parameters to optimize or dicts defining parameter groups **TYPE:** `Iterable[Parameter]` |
| `lr`           | learning rate **TYPE:** `float` **DEFAULT:** `0.001`                                                  |
| `momentum`     | momentum factor **TYPE:** `float` **DEFAULT:** `0.0`                                                  |
| `weight_decay` | weight decay (L2 penalty) **TYPE:** `float` **DEFAULT:** `0.0`                                        |
| `dampening`    | dampening for momentum **TYPE:** `float` **DEFAULT:** `0.0`                                           |
| `nesterov`     | enables Nesterov momentum **TYPE:** `bool` **DEFAULT:** `False`                                       |
| `scale_clip`   | the lower and upper bounds for the weight norm in local LR of LARS **TYPE:** \`tuple[float, float]    |

Source code in `holocron/optim/lars.py`

```python
def __init__(
    self,
    params: Iterable[torch.nn.parameter.Parameter],
    lr: float = 1e-3,
    momentum: float = 0.0,
    dampening: float = 0.0,
    weight_decay: float = 0.0,
    nesterov: bool = False,
    scale_clip: tuple[float, float] | None = None,
) -> None:
    if not isinstance(lr, float) or lr < 0.0:
        raise ValueError(f"Invalid learning rate: {lr}")
    if momentum < 0.0:
        raise ValueError(f"Invalid momentum value: {momentum}")
    if weight_decay < 0.0:
        raise ValueError(f"Invalid weight_decay value: {weight_decay}")

    defaults = {
        "lr": lr,
        "momentum": momentum,
        "dampening": dampening,
        "weight_decay": weight_decay,
        "nesterov": nesterov,
    }
    if nesterov and (momentum <= 0 or dampening != 0):
        raise ValueError("Nesterov momentum requires a momentum and zero dampening")
    super().__init__(params, defaults)
    # LARS arguments
    self.scale_clip = scale_clip
    if self.scale_clip is None:
        self.scale_clip = (0.0, 10.0)
```

### LAMB

```python
LAMB(params: Iterable[Parameter], lr: float = 0.001, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-08, weight_decay: float = 0.0, scale_clip: tuple[float, float] | None = None)
```

Bases: `Optimizer`

Implements the Lamb optimizer from ["Large batch optimization for deep learning: training BERT in 76 minutes"](https://arxiv.org/pdf/1904.00962v3.pdf).

The estimation of momentums is described as follows, (\\forall t \\geq 1):

[ m_t \\leftarrow \\beta_1 m\_{t-1} + (1 - \\beta_1) g_t \\ v_t \\leftarrow \\beta_2 v\_{t-1} + (1 - \\beta_2) g_t^2 ]

where (g_t) is the gradient of (\\theta_t), (\\beta_1, \\beta_2 \\in [0, 1]^2) are the exponential average smoothing coefficients, (m_0 = 0,\\ v_0 = 0).

Then we correct their biases using:

[ \\hat{m_t} \\leftarrow \\frac{m_t}{1 - \\beta_1^t} \\ \\hat{v_t} \\leftarrow \\frac{v_t}{1 - \\beta_2^t} ]

And finally the update step is performed using the following rule:

[ r_t \\leftarrow \\frac{\\hat{m_t}}{\\sqrt{\\hat{v_t}} + \\epsilon} \\ \\theta_t \\leftarrow \\theta\_{t-1} - \\alpha \\phi(\\lVert \\theta_t \\rVert) \\frac{r_t + \\lambda \\theta_t}{\\lVert r_t + \\theta_t \\rVert} ]

where (\\theta_t) is the parameter value at step (t) ((\\theta_0) being the initialization value), (\\phi) is a clipping function, (\\alpha) is the learning rate, (\\lambda \\geq 0) is the weight decay, (\\epsilon > 0).

| PARAMETER      | DESCRIPTION                                                                                             |
| -------------- | ------------------------------------------------------------------------------------------------------- |
| `params`       | iterable of parameters to optimize or dicts defining parameter groups **TYPE:** `Iterable[Parameter]`   |
| `lr`           | learning rate **TYPE:** `float` **DEFAULT:** `0.001`                                                    |
| `betas`        | beta coefficients used for running averages **TYPE:** `tuple[float, float]` **DEFAULT:** `(0.9, 0.999)` |
| `eps`          | term added to the denominator to improve numerical stability **TYPE:** `float` **DEFAULT:** `1e-08`     |
| `weight_decay` | weight decay (L2 penalty) **TYPE:** `float` **DEFAULT:** `0.0`                                          |
| `scale_clip`   | the lower and upper bounds for the weight norm in local LR of LARS **TYPE:** \`tuple[float, float]      |

Source code in `holocron/optim/lamb.py`

```python
def __init__(
    self,
    params: Iterable[torch.nn.Parameter],
    lr: float = 1e-3,
    betas: tuple[float, float] = (0.9, 0.999),
    eps: float = 1e-8,
    weight_decay: float = 0.0,
    scale_clip: tuple[float, float] | None = None,
) -> None:
    if lr < 0.0:
        raise ValueError(f"Invalid learning rate: {lr}")
    if eps < 0.0:
        raise ValueError(f"Invalid epsilon value: {eps}")
    if not 0.0 <= betas[0] < 1.0:
        raise ValueError(f"Invalid beta parameter at index 0: {betas[0]}")
    if not 0.0 <= betas[1] < 1.0:
        raise ValueError(f"Invalid beta parameter at index 1: {betas[1]}")
    defaults = {"lr": lr, "betas": betas, "eps": eps, "weight_decay": weight_decay}
    super().__init__(params, defaults)
    # LARS arguments
    self.scale_clip = scale_clip
    if self.scale_clip is None:
        self.scale_clip = (0.0, 10.0)
```

### RaLars

```python
RaLars(params: Iterable[Parameter], lr: float = 0.001, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-08, weight_decay: float = 0.0, force_adaptive_momentum: bool = False, scale_clip: tuple[float, float] | None = None)
```

Bases: `Optimizer`

Implements the RAdam optimizer from ["On the variance of the Adaptive Learning Rate and Beyond"](https://arxiv.org/pdf/1908.03265.pdf) with optional Layer-wise adaptive Scaling from ["Large Batch Training of Convolutional Networks"](https://arxiv.org/pdf/1708.03888.pdf)

| PARAMETER                 | DESCRIPTION                                                                                           |
| ------------------------- | ----------------------------------------------------------------------------------------------------- |
| `params`                  | iterable of parameters to optimize or dicts defining parameter groups **TYPE:** `Iterable[Parameter]` |
| `lr`                      | learning rate **TYPE:** `float` **DEFAULT:** `0.001`                                                  |
| `betas`                   | coefficients used for running averages **TYPE:** `tuple[float, float]` **DEFAULT:** `(0.9, 0.999)`    |
| `eps`                     | term added to the denominator to improve numerical stability **TYPE:** `float` **DEFAULT:** `1e-08`   |
| `weight_decay`            | weight decay (L2 penalty) **TYPE:** `float` **DEFAULT:** `0.0`                                        |
| `force_adaptive_momentum` | use adaptive momentum if variance is not tractable **TYPE:** `bool` **DEFAULT:** `False`              |
| `scale_clip`              | the maximal upper bound for the scale factor of LARS **TYPE:** \`tuple[float, float]                  |

Source code in `holocron/optim/ralars.py`

```python
def __init__(
    self,
    params: Iterable[torch.nn.Parameter],
    lr: float = 1e-3,
    betas: tuple[float, float] = (0.9, 0.999),
    eps: float = 1e-8,
    weight_decay: float = 0.0,
    force_adaptive_momentum: bool = False,
    scale_clip: tuple[float, float] | None = None,
) -> None:
    if lr < 0.0:
        raise ValueError(f"Invalid learning rate: {lr}")
    if eps < 0.0:
        raise ValueError(f"Invalid epsilon value: {eps}")
    if not 0.0 <= betas[0] < 1.0:
        raise ValueError(f"Invalid beta parameter at index 0: {betas[0]}")
    if not 0.0 <= betas[1] < 1.0:
        raise ValueError(f"Invalid beta parameter at index 1: {betas[1]}")
    defaults = {"lr": lr, "betas": betas, "eps": eps, "weight_decay": weight_decay}
    super().__init__(params, defaults)
    # RAdam tweaks
    self.force_adaptive_momentum = force_adaptive_momentum
    # LARS arguments
    self.scale_clip = scale_clip
    if self.scale_clip is None:
        self.scale_clip = (0, 10)
```

### TAdam

```python
TAdam(params: Iterable[Parameter], lr: float = 0.001, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-08, weight_decay: float = 0.0, amsgrad: bool = False, dof: float | None = None)
```

Bases: `Optimizer`

Implements the TAdam optimizer from ["TAdam: A Robust Stochastic Gradient Optimizer"](https://arxiv.org/pdf/2003.00179.pdf).

The estimation of momentums is described as follows, (\\forall t \\geq 1):

[ w_t \\leftarrow (\\nu + d) \\Big(\\nu + \\sum\\limits\_{j} \\frac{(g_t^j - m\_{t-1}^j)^2}{v\_{t-1} + \\epsilon} \\Big)^{-1} \\ m_t \\leftarrow \\frac{W\_{t-1}}{W\_{t-1} + w_t} m\_{t-1} + \\frac{w_t}{W\_{t-1} + w_t} g_t \\ v_t \\leftarrow \\beta_2 v\_{t-1} + (1 - \\beta_2) (g_t - g\_{t-1}) ]

where (g_t) is the gradient of (\\theta_t), (\\beta_1, \\beta_2 \\in [0, 1]^2) are the exponential average smoothing coefficients, (m_0 = 0,\\ v_0 = 0,\\ W_0 = \\frac{\\beta_1}{1 - \\beta_1}); (\\nu) is the degrees of freedom and (d) if the number of dimensions of the parameter gradient.

Then we correct their biases using:

[ \\hat{m_t} \\leftarrow \\frac{m_t}{1 - \\beta_1^t} \\ \\hat{v_t} \\leftarrow \\frac{v_t}{1 - \\beta_2^t} ]

And finally the update step is performed using the following rule:

[ \\theta_t \\leftarrow \\theta\_{t-1} - \\alpha \\frac{\\hat{m_t}}{\\sqrt{\\hat{v_t}} + \\epsilon} ]

where (\\theta_t) is the parameter value at step (t) ((\\theta_0) being the initialization value), (\\alpha) is the learning rate, (\\epsilon > 0).

| PARAMETER      | DESCRIPTION                                                                                           |
| -------------- | ----------------------------------------------------------------------------------------------------- |
| `params`       | iterable of parameters to optimize or dicts defining parameter groups **TYPE:** `Iterable[Parameter]` |
| `lr`           | learning rate **TYPE:** `float` **DEFAULT:** `0.001`                                                  |
| `betas`        | coefficients used for running averages **TYPE:** `tuple[float, float]` **DEFAULT:** `(0.9, 0.999)`    |
| `eps`          | term added to the denominator to improve numerical stability **TYPE:** `float` **DEFAULT:** `1e-08`   |
| `weight_decay` | weight decay (L2 penalty) **TYPE:** `float` **DEFAULT:** `0.0`                                        |
| `dof`          | degrees of freedom **TYPE:** \`float                                                                  |

Source code in `holocron/optim/tadam.py`

```python
def __init__(
    self,
    params: Iterable[torch.nn.Parameter],
    lr: float = 1e-3,
    betas: tuple[float, float] = (0.9, 0.999),
    eps: float = 1e-8,
    weight_decay: float = 0.0,
    amsgrad: bool = False,
    dof: float | None = None,
) -> None:
    if lr < 0.0:
        raise ValueError(f"Invalid learning rate: {lr}")
    if eps < 0.0:
        raise ValueError(f"Invalid epsilon value: {eps}")
    if not 0.0 <= betas[0] < 1.0:
        raise ValueError(f"Invalid beta parameter at index 0: {betas[0]}")
    if not 0.0 <= betas[1] < 1.0:
        raise ValueError(f"Invalid beta parameter at index 1: {betas[1]}")
    if not weight_decay >= 0.0:
        raise ValueError(f"Invalid weight_decay value: {weight_decay}")
    defaults = {"lr": lr, "betas": betas, "eps": eps, "weight_decay": weight_decay, "amsgrad": amsgrad, "dof": dof}
    super().__init__(params, defaults)
```

### AdaBelief

Bases: `Adam`

Implements the AdaBelief optimizer from ["AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients"](https://arxiv.org/pdf/2010.07468.pdf).

The estimation of momentums is described as follows, (\\forall t \\geq 1):

[ m_t \\leftarrow \\beta_1 m\_{t-1} + (1 - \\beta_1) g_t \\ s_t \\leftarrow \\beta_2 s\_{t-1} + (1 - \\beta_2) (g_t - m_t)^2 + \\epsilon ]

where (g_t) is the gradient of (\\theta_t), (\\beta_1, \\beta_2 \\in [0, 1]^2) are the exponential average smoothing coefficients, (m_0 = 0,\\ s_0 = 0), (\\epsilon > 0).

Then we correct their biases using:

[ \\hat{m_t} \\leftarrow \\frac{m_t}{1 - \\beta_1^t} \\ \\hat{s_t} \\leftarrow \\frac{s_t}{1 - \\beta_2^t} ]

And finally the update step is performed using the following rule:

[ \\theta_t \\leftarrow \\theta\_{t-1} - \\alpha \\frac{\\hat{m_t}}{\\sqrt{\\hat{s_t}} + \\epsilon} ]

where (\\theta_t) is the parameter value at step (t) ((\\theta_0) being the initialization value), (\\alpha) is the learning rate, (\\epsilon > 0).

| PARAMETER      | DESCRIPTION                                                           |
| -------------- | --------------------------------------------------------------------- |
| `params`       | iterable of parameters to optimize or dicts defining parameter groups |
| `lr`           | learning rate                                                         |
| `betas`        | coefficients used for running averages                                |
| `eps`          | term added to the denominator to improve numerical stability          |
| `weight_decay` | weight decay (L2 penalty)                                             |
| `amsgrad`      | whether to use the AMSGrad variant                                    |

### AdamP

```python
AdamP(params: Iterable[Parameter], lr: float = 0.001, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-08, weight_decay: float = 0.0, amsgrad: bool = False, delta: float = 0.1)
```

Bases: `Adam`

Implements the AdamP optimizer from ["AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights"](https://arxiv.org/pdf/2006.08217.pdf).

The estimation of momentums is described as follows, (\\forall t \\geq 1):

[ m_t \\leftarrow \\beta_1 m\_{t-1} + (1 - \\beta_1) g_t \\ v_t \\leftarrow \\beta_2 v\_{t-1} + (1 - \\beta_2) g_t^2 ]

where (g_t) is the gradient of (\\theta_t), (\\beta_1, \\beta_2 \\in [0, 1]^2) are the exponential average smoothing coefficients, (m_0 = g_0,\\ v_0 = 0).

Then we correct their biases using:

[ \\hat{m_t} \\leftarrow \\frac{m_t}{1 - \\beta_1^t} \\ \\hat{v_t} \\leftarrow \\frac{v_t}{1 - \\beta_2^t} ]

And finally the update step is performed using the following rule:

[ p_t \\leftarrow \\frac{\\hat{m_t}}{\\sqrt{\\hat{n_t} + \\epsilon}} \\ q_t \\leftarrow \\begin{cases} \\prod\_{\\theta_t}(p_t) & if\\ cos(\\theta_t, g_t) < \\delta / \\sqrt{dim(\\theta)}\\ p_t & \\text{otherwise}\\ \\end{cases} \\ \\theta_t \\leftarrow \\theta\_{t-1} - \\alpha q_t ]

where (\\theta_t) is the parameter value at step (t) ((\\theta_0) being the initialization value), (\\prod\_{\\theta_t}(p_t)) is the projection of (p_t) onto the tangent space of (\\theta_t), (cos(\\theta_t, g_t)) is the cosine similarity between (\\theta_t) and (g_t), (\\alpha) is the learning rate, (\\delta > 0), (\\epsilon > 0).

| PARAMETER      | DESCRIPTION                                                                                           |
| -------------- | ----------------------------------------------------------------------------------------------------- |
| `params`       | iterable of parameters to optimize or dicts defining parameter groups **TYPE:** `Iterable[Parameter]` |
| `lr`           | learning rate **TYPE:** `float` **DEFAULT:** `0.001`                                                  |
| `betas`        | coefficients used for running averages **TYPE:** `tuple[float, float]` **DEFAULT:** `(0.9, 0.999)`    |
| `eps`          | term added to the denominator to improve numerical stability **TYPE:** `float` **DEFAULT:** `1e-08`   |
| `weight_decay` | weight decay (L2 penalty) **TYPE:** `float` **DEFAULT:** `0.0`                                        |
| `amsgrad`      | whether to use the AMSGrad variant **TYPE:** `bool` **DEFAULT:** `False`                              |
| `delta`        | delta threshold for projection **TYPE:** `float` **DEFAULT:** `0.1`                                   |

Source code in `holocron/optim/adamp.py`

```python
def __init__(
    self,
    params: Iterable[torch.nn.Parameter],
    lr: float = 1e-3,
    betas: tuple[float, float] = (0.9, 0.999),
    eps: float = 1e-8,
    weight_decay: float = 0.0,
    amsgrad: bool = False,
    delta: float = 0.1,
) -> None:
    super().__init__(params, lr, betas, eps, weight_decay, amsgrad)
    self.delta = delta
```

### Adan

```python
Adan(params: Iterable[Parameter], lr: float = 0.001, betas: tuple[float, float, float] = (0.98, 0.92, 0.99), eps: float = 1e-08, weight_decay: float = 0.0, amsgrad: bool = False)
```

Bases: `Adam`

Implements the Adan optimizer from ["Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models"](https://arxiv.org/pdf/2208.06677.pdf).

The estimation of momentums is described as follows, (\\forall t \\geq 1):

\[ m_t \\leftarrow \\beta_1 m\_{t-1} + (1 - \\beta_1) g_t \\ v_t \\leftarrow \\beta_2 v\_{t-1} + (1 - \\beta_2) (g_t - g\_{t-1}) \\ n_t \\leftarrow \\beta_3 n\_{t-1} + (1 - \\beta_3) [g_t + \\beta_2 (g_t - g\_{t - 1})]^2 \]

where (g_t) is the gradient of (\\theta_t), (\\beta_1, \\beta_2, \\beta_3 \\in [0, 1]^3) are the exponential average smoothing coefficients, (m_0 = g_0,\\ v_0 = 0,\\ n_0 = g_0^2).

Then we correct their biases using:

[ \\hat{m_t} \\leftarrow \\frac{m_t}{1 - \\beta_1^t} \\ \\hat{v_t} \\leftarrow \\frac{v_t}{1 - \\beta_2^t} \\ \\hat{n_t} \\leftarrow \\frac{n_t}{1 - \\beta_3^t} ]

And finally the update step is performed using the following rule:

[ p_t \\leftarrow \\frac{\\hat{m_t} + (1 - \\beta_2) \\hat{v_t}}{\\sqrt{\\hat{n_t} + \\epsilon}} \\ \\theta_t \\leftarrow \\frac{\\theta\_{t-1} - \\alpha p_t}{1 + \\lambda \\alpha} ]

where (\\theta_t) is the parameter value at step (t) ((\\theta_0) being the initialization value), (\\alpha) is the learning rate, (\\lambda \\geq 0) is the weight decay, (\\epsilon > 0).

| PARAMETER      | DESCRIPTION                                                                                                     |
| -------------- | --------------------------------------------------------------------------------------------------------------- |
| `params`       | iterable of parameters to optimize or dicts defining parameter groups **TYPE:** `Iterable[Parameter]`           |
| `lr`           | learning rate **TYPE:** `float` **DEFAULT:** `0.001`                                                            |
| `betas`        | coefficients used for running averages **TYPE:** `tuple[float, float, float]` **DEFAULT:** `(0.98, 0.92, 0.99)` |
| `eps`          | term added to the denominator to improve numerical stability **TYPE:** `float` **DEFAULT:** `1e-08`             |
| `weight_decay` | weight decay (L2 penalty) **TYPE:** `float` **DEFAULT:** `0.0`                                                  |
| `amsgrad`      | whether to use the AMSGrad variant **TYPE:** `bool` **DEFAULT:** `False`                                        |

Source code in `holocron/optim/adan.py`

```python
def __init__(
    self,
    params: Iterable[torch.nn.Parameter],
    lr: float = 1e-3,
    betas: tuple[float, float, float] = (0.98, 0.92, 0.99),
    eps: float = 1e-8,
    weight_decay: float = 0.0,
    amsgrad: bool = False,
) -> None:
    super().__init__(params, lr, betas, eps, weight_decay, amsgrad)  # type: ignore[arg-type]
```

### AdEMAMix

```python
AdEMAMix(params: Iterable[Parameter], lr: float = 0.001, betas: tuple[float, float, float] = (0.9, 0.999, 0.9999), alpha: float = 5.0, eps: float = 1e-08, weight_decay: float = 0.0)
```

Bases: `Optimizer`

Implements the AdEMAMix optimizer from ["The AdEMAMix Optimizer: Better, Faster, Older"](https://arxiv.org/pdf/2409.03137).

The estimation of momentums is described as follows, (\\forall t \\geq 1):

[ m\_{1,t} \\leftarrow \\beta_1 m\_{1, t-1} + (1 - \\beta_1) g_t \\ m\_{2,t} \\leftarrow \\beta_3 m\_{2, t-1} + (1 - \\beta_3) g_t \\ s_t \\leftarrow \\beta_2 s\_{t-1} + (1 - \\beta_2) (g_t - m_t)^2 + \\epsilon ]

where (g_t) is the gradient of (\\theta_t), (\\beta_1, \\beta_2, \\beta_3 \\in [0, 1]^3) are the exponential average smoothing coefficients, (m\_{1,0} = 0,\\ m\_{2,0} = 0,\\ s_0 = 0), (\\epsilon > 0).

Then we correct their biases using:

[ \\hat{m\_{1,t}} \\leftarrow \\frac{m\_{1,t}}{1 - \\beta_1^t} \\ \\hat{s_t} \\leftarrow \\frac{s_t}{1 - \\beta_2^t} ]

And finally the update step is performed using the following rule:

[ \\theta_t \\leftarrow \\theta\_{t-1} - \\eta \\frac{\\hat{m\_{1,t}} + \\alpha m\_{2,t}}{\\sqrt{\\hat{s_t}} + \\epsilon} ]

where (\\theta_t) is the parameter value at step (t) ((\\theta_0) being the initialization value), (\\eta) is the learning rate, (\\alpha > 0) (\\epsilon > 0).

| PARAMETER      | DESCRIPTION                                                                                                       |
| -------------- | ----------------------------------------------------------------------------------------------------------------- |
| `params`       | iterable of parameters to optimize or dicts defining parameter groups **TYPE:** `Iterable[Parameter]`             |
| `lr`           | learning rate **TYPE:** `float` **DEFAULT:** `0.001`                                                              |
| `betas`        | coefficients used for running averages **TYPE:** `tuple[float, float, float]` **DEFAULT:** `(0.9, 0.999, 0.9999)` |
| `alpha`        | the exponential decay rate of the second moment estimates **TYPE:** `float` **DEFAULT:** `5.0`                    |
| `eps`          | term added to the denominator to improve numerical stability **TYPE:** `float` **DEFAULT:** `1e-08`               |
| `weight_decay` | weight decay (L2 penalty) **TYPE:** `float` **DEFAULT:** `0.0`                                                    |

Source code in `holocron/optim/ademamix.py`

```python
def __init__(
    self,
    params: Iterable[torch.nn.Parameter],
    lr: float = 1e-3,
    betas: tuple[float, float, float] = (0.9, 0.999, 0.9999),
    alpha: float = 5.0,
    eps: float = 1e-8,
    weight_decay: float = 0.0,
) -> None:
    if lr < 0.0:
        raise ValueError(f"Invalid learning rate: {lr}")
    if eps < 0.0:
        raise ValueError(f"Invalid epsilon value: {eps}")
    for idx, beta in enumerate(betas):
        if not 0.0 <= beta < 1.0:
            raise ValueError(f"Invalid beta parameter at index {idx}: {beta}")
    defaults = {"lr": lr, "betas": betas, "alpha": alpha, "eps": eps, "weight_decay": weight_decay}
    super().__init__(params, defaults)
```

## Optimizer wrappers

`holocron.optim` also implements optimizer wrappers.

A base optimizer should always be passed to the wrapper; e.g., you should write your code this way:

```python
optimizer = ...
optimizer = wrapper(optimizer)
```

### Lookahead

```python
Lookahead(base_optimizer: Optimizer, sync_rate: float = 0.5, sync_period: int = 6)
```

Bases: `Optimizer`

Implements the Lookahead optimizer wrapper from ["Lookahead Optimizer: k steps forward, 1 step back"](https://arxiv.org/pdf/1907.08610.pdf). <https://arxiv.org/pdf/1907.08610.pdf>\`\_.

Example

```python
from torch.optim import AdamW
from holocron.optim.wrapper import Lookahead
model = ...
opt = AdamW(model.parameters(), lr=3e-4)
opt_wrapper = Lookahead(opt)
```

| PARAMETER        | DESCRIPTION                                                                                             |
| ---------------- | ------------------------------------------------------------------------------------------------------- |
| `base_optimizer` | base parameter optimizer **TYPE:** `Optimizer`                                                          |
| `sync_rate`      | rate of weight synchronization **TYPE:** `float` **DEFAULT:** `0.5`                                     |
| `sync_period`    | number of step performed on fast weights before weight synchronization **TYPE:** `int` **DEFAULT:** `6` |

Source code in `holocron/optim/wrapper.py`

```python
def __init__(
    self,
    base_optimizer: torch.optim.Optimizer,
    sync_rate: float = 0.5,
    sync_period: int = 6,
) -> None:
    if sync_rate < 0 or sync_rate > 1:
        raise ValueError(f"expected positive float lower than 1 as sync_rate, received: {sync_rate}")
    if not isinstance(sync_period, int) or sync_period < 1:
        raise ValueError(f"expected positive integer as sync_period, received: {sync_period}")
    # Optimizer attributes
    self.defaults = {"sync_rate": sync_rate, "sync_period": sync_period}
    self.state = defaultdict(dict)
    # Base optimizer attributes
    self.base_optimizer = base_optimizer
    # Wrapper attributes
    self.fast_steps = 0
    self.param_groups = []
    for group in self.base_optimizer.param_groups:
        self._add_param_group(group)
```

### Scout

```python
Scout(base_optimizer: Optimizer, sync_rate: float = 0.5, sync_period: int = 6)
```

Bases: `Optimizer`

Implements a new optimizer wrapper based on ["Lookahead Optimizer: k steps forward, 1 step back"](https://arxiv.org/pdf/1907.08610.pdf).

Example

```python
from torch.optim import AdamW
from holocron.optim.wrapper import Scout
model = ...
opt = AdamW(model.parameters(), lr=3e-4)
opt_wrapper = Scout(opt)
```

| PARAMETER        | DESCRIPTION                                                                                             |
| ---------------- | ------------------------------------------------------------------------------------------------------- |
| `base_optimizer` | base parameter optimizer **TYPE:** `Optimizer`                                                          |
| `sync_rate`      | rate of weight synchronization **TYPE:** `float` **DEFAULT:** `0.5`                                     |
| `sync_period`    | number of step performed on fast weights before weight synchronization **TYPE:** `int` **DEFAULT:** `6` |

Source code in `holocron/optim/wrapper.py`

```python
def __init__(
    self,
    base_optimizer: torch.optim.Optimizer,
    sync_rate: float = 0.5,
    sync_period: int = 6,
) -> None:
    if sync_rate < 0 or sync_rate > 1:
        raise ValueError(f"expected positive float lower than 1 as sync_rate, received: {sync_rate}")
    if not isinstance(sync_period, int) or sync_period < 1:
        raise ValueError(f"expected positive integer as sync_period, received: {sync_period}")
    # Optimizer attributes
    self.defaults = {"sync_rate": sync_rate, "sync_period": sync_period}
    self.state = defaultdict(dict)
    # Base optimizer attributes
    self.base_optimizer = base_optimizer
    # Wrapper attributes
    self.fast_steps = 0
    self.param_groups = []
    for group in self.base_optimizer.param_groups:
        self._add_param_group(group)
    # Buffer for scouting
    self.buffer = [p.data.unsqueeze(0) for group in self.param_groups for p in group["params"]]
```

# holocron.trainer

`holocron.trainer` provides some basic objects for training purposes.

### Trainer

```python
Trainer(model: Module, train_loader: DataLoader, val_loader: DataLoader, criterion: Module, optimizer: Optimizer, gpu: int | None = None, output_file: str = './checkpoint.pth', amp: bool = False, skip_nan_loss: bool = False, nan_tolerance: int = 5, gradient_acc: int = 1, gradient_clip: float | None = None, on_epoch_end: Callable[[dict[str, float]], Any] | None = None)
```

Baseline trainer class.

| PARAMETER       | DESCRIPTION                                                                                                        |
| --------------- | ------------------------------------------------------------------------------------------------------------------ |
| `model`         | model to train **TYPE:** `Module`                                                                                  |
| `train_loader`  | training loader **TYPE:** `DataLoader`                                                                             |
| `val_loader`    | validation loader **TYPE:** `DataLoader`                                                                           |
| `criterion`     | loss criterion **TYPE:** `Module`                                                                                  |
| `optimizer`     | parameter optimizer **TYPE:** `Optimizer`                                                                          |
| `gpu`           | index of the GPU to use **TYPE:** \`int                                                                            |
| `output_file`   | path where checkpoints will be saved **TYPE:** `str` **DEFAULT:** `'./checkpoint.pth'`                             |
| `amp`           | whether to use automatic mixed precision **TYPE:** `bool` **DEFAULT:** `False`                                     |
| `skip_nan_loss` | whether the optimizer step should be skipped when the loss is NaN **TYPE:** `bool` **DEFAULT:** `False`            |
| `nan_tolerance` | number of consecutive batches with NaN loss before stopping the training **TYPE:** `int` **DEFAULT:** `5`          |
| `gradient_acc`  | number of batches to accumulate the gradient of before performing the update step **TYPE:** `int` **DEFAULT:** `1` |
| `gradient_clip` | the gradient clip value **TYPE:** \`float                                                                          |
| `on_epoch_end`  | callback triggered at the end of an epoch **TYPE:** \`Callable\[\[dict[str, float]\], Any\]                        |

Source code in `holocron/trainer/core.py`

```python
def __init__(
    self,
    model: nn.Module,
    train_loader: DataLoader,
    val_loader: DataLoader,
    criterion: nn.Module,
    optimizer: torch.optim.Optimizer,
    gpu: int | None = None,
    output_file: str = "./checkpoint.pth",
    amp: bool = False,
    skip_nan_loss: bool = False,
    nan_tolerance: int = 5,
    gradient_acc: int = 1,
    gradient_clip: float | None = None,
    on_epoch_end: Callable[[dict[str, float]], Any] | None = None,
) -> None:
    self.model = model
    self.train_loader = train_loader
    self.val_loader = val_loader
    self.criterion = criterion
    self.optimizer = optimizer
    self.amp = amp
    self.scaler: GradScaler
    self.on_epoch_end = on_epoch_end
    self.skip_nan_loss = skip_nan_loss
    self.nan_tolerance = nan_tolerance
    self.gradient_acc = gradient_acc
    self.grad_clip = gradient_clip

    # Output file
    self.output_file = output_file

    # Initialize
    self.step = 0
    self.start_epoch = 0
    self.epoch = 0
    self._grad_count = 0
    self.min_loss = math.inf
    self.gpu = gpu
    self._params: tuple[ParamSeq, ParamSeq] = ([], [])
    self.lr_recorder: list[float] = []
    self.loss_recorder: list[float] = []
    self.set_device(gpu)
    self._reset_opt(self.optimizer.defaults["lr"])
```

#### set_device

```python
set_device(gpu: int | None = None) -> None
```

Move tensor objects to the target GPU

| PARAMETER | DESCRIPTION                                    |
| --------- | ---------------------------------------------- |
| `gpu`     | index of the target GPU device **TYPE:** \`int |

| RAISES           | DESCRIPTION                      |
| ---------------- | -------------------------------- |
| `AssertionError` | if PyTorch cannot access the GPU |
| `ValueError`     | if the device index is invalid   |

Source code in `holocron/trainer/core.py`

```python
def set_device(self, gpu: int | None = None) -> None:
    """Move tensor objects to the target GPU

    Args:
        gpu: index of the target GPU device

    Raises:
        AssertionError: if PyTorch cannot access the GPU
        ValueError: if the device index is invalid
    """
    if isinstance(gpu, int):
        if not torch.cuda.is_available():
            raise AssertionError("PyTorch cannot access your GPU. Please investigate!")
        if gpu >= torch.cuda.device_count():
            raise ValueError("Invalid device index")
        torch.cuda.set_device(gpu)
        self.model = self.model.cuda()
        if isinstance(self.criterion, torch.nn.Module):
            self.criterion = self.criterion.cuda()
```

#### to_cuda

```python
to_cuda(x: Tensor, target: Tensor | list[dict[str, Tensor]]) -> tuple[Tensor, Tensor | list[dict[str, Tensor]]]
```

Move input and target to GPU

| PARAMETER | DESCRIPTION                                                     |
| --------- | --------------------------------------------------------------- |
| `x`       | input tensor **TYPE:** `Tensor`                                 |
| `target`  | target tensor or list of target dictionaries **TYPE:** \`Tensor |

| RETURNS                 | DESCRIPTION                   |
| ----------------------- | ----------------------------- |
| \`tuple\[Tensor, Tensor | list\[dict[str, Tensor]\]\]\` |

| RAISES       | DESCRIPTION                    |
| ------------ | ------------------------------ |
| `ValueError` | if the device index is invalid |

Source code in `holocron/trainer/core.py`

```python
def to_cuda(
    self, x: Tensor, target: Tensor | list[dict[str, Tensor]]
) -> tuple[Tensor, Tensor | list[dict[str, Tensor]]]:
    """Move input and target to GPU

    Args:
        x: input tensor
        target: target tensor or list of target dictionaries

    Returns:
        tuple of input and target tensors

    Raises:
        ValueError: if the device index is invalid
    """
    if isinstance(self.gpu, int):
        if self.gpu >= torch.cuda.device_count():
            raise ValueError("Invalid device index")
        return self._to_cuda(x, target)  # type: ignore[arg-type]
    return x, target
```

#### save

```python
save(output_file: str) -> None
```

Save a trainer checkpoint

| PARAMETER     | DESCRIPTION                           |
| ------------- | ------------------------------------- |
| `output_file` | destination file path **TYPE:** `str` |

Source code in `holocron/trainer/core.py`

```python
def save(self, output_file: str) -> None:
    """Save a trainer checkpoint

    Args:
        output_file: destination file path
    """
    torch.save(
        {
            "epoch": self.epoch,
            "step": self.step,
            "min_loss": self.min_loss,
            "model": self.model.state_dict(),
        },
        output_file,
        _use_new_zipfile_serialization=False,
    )
```

#### load

```python
load(state: dict[str, Any]) -> None
```

Resume from a trainer state

| PARAMETER | DESCRIPTION                                      |
| --------- | ------------------------------------------------ |
| `state`   | checkpoint dictionary **TYPE:** `dict[str, Any]` |

Source code in `holocron/trainer/core.py`

```python
def load(self, state: dict[str, Any]) -> None:
    """Resume from a trainer state

    Args:
        state: checkpoint dictionary
    """
    self.start_epoch = state["epoch"]
    self.epoch = self.start_epoch
    self.step = state["step"]
    self.min_loss = state["min_loss"]
    self.model.load_state_dict(state["model"])
```

#### fit_n_epochs

```python
fit_n_epochs(num_epochs: int, lr: float, freeze_until: str | None = None, sched_type: str = 'onecycle', norm_weight_decay: float | None = None, **kwargs: Any) -> None
```

Train the model for a given number of epochs.

| PARAMETER           | DESCRIPTION                                                              |
| ------------------- | ------------------------------------------------------------------------ |
| `num_epochs`        | number of epochs to train **TYPE:** `int`                                |
| `lr`                | learning rate to be used by the scheduler **TYPE:** `float`              |
| `freeze_until`      | last layer to freeze **TYPE:** \`str                                     |
| `sched_type`        | type of scheduler to use **TYPE:** `str` **DEFAULT:** `'onecycle'`       |
| `norm_weight_decay` | weight decay to apply to normalization parameters **TYPE:** \`float      |
| `**kwargs`          | keyword args passed to the LRScheduler **TYPE:** `Any` **DEFAULT:** `{}` |

Source code in `holocron/trainer/core.py`

```python
def fit_n_epochs(
    self,
    num_epochs: int,
    lr: float,
    freeze_until: str | None = None,
    sched_type: str = "onecycle",
    norm_weight_decay: float | None = None,
    **kwargs: Any,
) -> None:
    """Train the model for a given number of epochs.

    Args:
        num_epochs: number of epochs to train
        lr: learning rate to be used by the scheduler
        freeze_until: last layer to freeze
        sched_type: type of scheduler to use
        norm_weight_decay: weight decay to apply to normalization parameters
        **kwargs: keyword args passed to the [`LRScheduler`][torch.optim.lr_scheduler.LRScheduler]
    """
    freeze_model(self.model.train(), freeze_until)
    # Update param groups & LR
    self._reset_opt(lr, norm_weight_decay)
    # Scheduler
    self._reset_scheduler(lr, num_epochs, sched_type, **kwargs)

    if self.amp:
        self.scaler = GradScaler("cuda")

    mb = master_bar(range(num_epochs))
    for _ in mb:
        self._fit_epoch(mb)
        eval_metrics = self.evaluate()

        # master bar
        mb.main_bar.comment = f"Epoch {self.epoch}/{self.start_epoch + num_epochs}"
        mb.write(f"Epoch {self.epoch}/{self.start_epoch + num_epochs} - {self._eval_metrics_str(eval_metrics)}")

        if eval_metrics["val_loss"] < self.min_loss:
            print(  # noqa: T201
                f"Validation loss decreased {self.min_loss:.4} --> {eval_metrics['val_loss']:.4}: saving state..."
            )
            self.min_loss = eval_metrics["val_loss"]
            self.save(self.output_file)

        if self.on_epoch_end is not None:
            self.on_epoch_end(eval_metrics)
```

#### find_lr

```python
find_lr(freeze_until: str | None = None, start_lr: float = 1e-07, end_lr: float = 1, norm_weight_decay: float | None = None, num_it: int = 100) -> None
```

Gridsearch the optimal learning rate for the training as described in ["Cyclical Learning Rates for Training Neural Networks"](https://arxiv.org/pdf/1506.01186.pdf).

| PARAMETER           | DESCRIPTION                                                         |
| ------------------- | ------------------------------------------------------------------- |
| `freeze_until`      | last layer to freeze **TYPE:** \`str                                |
| `start_lr`          | initial learning rate **TYPE:** `float` **DEFAULT:** `1e-07`        |
| `end_lr`            | final learning rate **TYPE:** `float` **DEFAULT:** `1`              |
| `norm_weight_decay` | weight decay to apply to normalization parameters **TYPE:** \`float |
| `num_it`            | number of iterations to perform **TYPE:** `int` **DEFAULT:** `100`  |

| RAISES       | DESCRIPTION                                                                 |
| ------------ | --------------------------------------------------------------------------- |
| `ValueError` | if the number of iterations is greater than the number of available batches |

Source code in `holocron/trainer/core.py`

```python
def find_lr(
    self,
    freeze_until: str | None = None,
    start_lr: float = 1e-7,
    end_lr: float = 1,
    norm_weight_decay: float | None = None,
    num_it: int = 100,
) -> None:
    """Gridsearch the optimal learning rate for the training as described in
    ["Cyclical Learning Rates for Training Neural Networks"](https://arxiv.org/pdf/1506.01186.pdf).

    Args:
       freeze_until: last layer to freeze
       start_lr: initial learning rate
       end_lr: final learning rate
       norm_weight_decay: weight decay to apply to normalization parameters
       num_it: number of iterations to perform

    Raises:
        ValueError: if the number of iterations is greater than the number of available batches
    """
    if num_it > len(self.train_loader):
        raise ValueError("the value of `num_it` needs to be lower than the number of available batches")

    freeze_model(self.model.train(), freeze_until)
    # Update param groups & LR
    self._reset_opt(start_lr, norm_weight_decay)
    gamma = (end_lr / start_lr) ** (1 / (num_it - 1))
    scheduler = MultiplicativeLR(self.optimizer, lambda step: gamma)

    self.lr_recorder = [start_lr * gamma**idx for idx in range(num_it)]
    self.loss_recorder = []

    if self.amp:
        self.scaler = GradScaler("cuda")

    for batch_idx, (x, target) in enumerate(self.train_loader):
        x, target = self.to_cuda(x, target)

        # Forward
        batch_loss: Tensor = self._get_loss(x, target)  # type: ignore[assignment]
        self._backprop_step(batch_loss)
        # Update LR
        scheduler.step()

        # Record
        if torch.isnan(batch_loss) or torch.isinf(batch_loss):
            if batch_idx == 0:
                raise ValueError("loss value is NaN or inf.")
            break
        self.loss_recorder.append(batch_loss.item())
        # Stop after the number of iterations
        if batch_idx + 1 == num_it:
            break

    self.lr_recorder = self.lr_recorder[: len(self.loss_recorder)]
```

#### plot_recorder

```python
plot_recorder(beta: float = 0.95, **kwargs: Any) -> None
```

Display the results of the LR grid search

| PARAMETER  | DESCRIPTION                                                              |
| ---------- | ------------------------------------------------------------------------ |
| `beta`     | smoothing factor **TYPE:** `float` **DEFAULT:** `0.95`                   |
| `**kwargs` | keyword args of matplotlib.pyplot.show **TYPE:** `Any` **DEFAULT:** `{}` |

| RAISES           | DESCRIPTION                                                                                                                |
| ---------------- | -------------------------------------------------------------------------------------------------------------------------- |
| `AssertionError` | if the number of learning rate recorder and loss recorder are not the same or if the number of learning rate recorder is 0 |

Source code in `holocron/trainer/core.py`

```python
def plot_recorder(self, beta: float = 0.95, **kwargs: Any) -> None:
    """Display the results of the LR grid search

    Args:
        beta: smoothing factor
        **kwargs: keyword args of [`matplotlib.pyplot.show`][matplotlib.pyplot.show]

    Raises:
        AssertionError: if the number of learning rate recorder and loss recorder are not the same or if the number of learning rate recorder is 0
    """
    if len(self.lr_recorder) != len(self.loss_recorder) or len(self.lr_recorder) == 0:
        raise AssertionError("Please run the `lr_find` method first")

    # Exp moving average of loss
    smoothed_losses = []
    avg_loss = 0.0
    for idx, loss in enumerate(self.loss_recorder):
        avg_loss = beta * avg_loss + (1 - beta) * loss
        smoothed_losses.append(avg_loss / (1 - beta ** (idx + 1)))

    # Properly rescale Y-axis
    data_slice = slice(
        min(len(self.loss_recorder) // 10, 10),
        -min(len(self.loss_recorder) // 20, 5) if len(self.loss_recorder) >= 20 else len(self.loss_recorder),
    )
    vals: np.ndarray = np.array(smoothed_losses[data_slice])
    min_idx = vals.argmin()
    max_val = vals.max() if min_idx is None else vals[: min_idx + 1].max()
    delta = max_val - vals[min_idx]

    plt.plot(self.lr_recorder[data_slice], smoothed_losses[data_slice])
    plt.xscale("log")
    plt.xlabel("Learning Rate")
    plt.ylabel("Training loss")
    plt.ylim(vals[min_idx] - 0.1 * delta, max_val + 0.2 * delta)
    plt.grid(True, linestyle="--", axis="x")
    plt.show(**kwargs)
```

#### check_setup

```python
check_setup(freeze_until: str | None = None, lr: float = 0.0003, norm_weight_decay: float | None = None, num_it: int = 100, **kwargs: Any) -> None
```

Check whether you can overfit one batch

| PARAMETER           | DESCRIPTION                                                                   |
| ------------------- | ----------------------------------------------------------------------------- |
| `freeze_until`      | last layer to freeze **TYPE:** \`str                                          |
| `lr`                | learning rate to be used for training **TYPE:** `float` **DEFAULT:** `0.0003` |
| `norm_weight_decay` | weight decay to apply to normalization parameters **TYPE:** \`float           |
| `num_it`            | number of iterations to perform **TYPE:** `int` **DEFAULT:** `100`            |
| `**kwargs`          | keyword args of matplotlib.pyplot.show **TYPE:** `Any` **DEFAULT:** `{}`      |

| RAISES       | DESCRIPTION                     |
| ------------ | ------------------------------- |
| `ValueError` | if the loss value is NaN or inf |

Source code in `holocron/trainer/core.py`

```python
def check_setup(
    self,
    freeze_until: str | None = None,
    lr: float = 3e-4,
    norm_weight_decay: float | None = None,
    num_it: int = 100,
    **kwargs: Any,
) -> None:
    """Check whether you can overfit one batch

    Args:
        freeze_until: last layer to freeze
        lr: learning rate to be used for training
        norm_weight_decay: weight decay to apply to normalization parameters
        num_it: number of iterations to perform
        **kwargs: keyword args of [`matplotlib.pyplot.show`][matplotlib.pyplot.show]

    Raises:
        ValueError: if the loss value is NaN or inf
    """
    freeze_model(self.model.train(), freeze_until)
    # Update param groups & LR
    self._reset_opt(lr, norm_weight_decay)

    x, target = next(iter(self.train_loader))
    x, target = self.to_cuda(x, target)

    losses = []

    if self.amp:
        self.scaler = GradScaler("cuda")

    for _ in range(num_it):
        # Forward
        batch_loss: Tensor = self._get_loss(x, target)  # type: ignore[assignment]
        # Backprop
        self._backprop_step(batch_loss)

        if torch.isnan(batch_loss) or torch.isinf(batch_loss):
            raise ValueError("loss value is NaN or inf.")

        losses.append(batch_loss.item())

    plt.plot(np.arange(len(losses)), losses)
    plt.xlabel("Optimization steps")
    plt.ylabel("Training loss")
    plt.grid(True, linestyle="--", axis="x")
    plt.show(**kwargs)
```

## Image classification

### ClassificationTrainer

```python
ClassificationTrainer(model: Module, train_loader: DataLoader, val_loader: DataLoader, criterion: Module, optimizer: Optimizer, gpu: int | None = None, output_file: str = './checkpoint.pth', amp: bool = False, skip_nan_loss: bool = False, nan_tolerance: int = 5, gradient_acc: int = 1, gradient_clip: float | None = None, on_epoch_end: Callable[[dict[str, float]], Any] | None = None)
```

Bases: `Trainer`

Image classification trainer class.

| PARAMETER       | DESCRIPTION                                                                                                        |
| --------------- | ------------------------------------------------------------------------------------------------------------------ |
| `model`         | model to train **TYPE:** `Module`                                                                                  |
| `train_loader`  | training loader **TYPE:** `DataLoader`                                                                             |
| `val_loader`    | validation loader **TYPE:** `DataLoader`                                                                           |
| `criterion`     | loss criterion **TYPE:** `Module`                                                                                  |
| `optimizer`     | parameter optimizer **TYPE:** `Optimizer`                                                                          |
| `gpu`           | index of the GPU to use **TYPE:** \`int                                                                            |
| `output_file`   | path where checkpoints will be saved **TYPE:** `str` **DEFAULT:** `'./checkpoint.pth'`                             |
| `amp`           | whether to use automatic mixed precision **TYPE:** `bool` **DEFAULT:** `False`                                     |
| `skip_nan_loss` | whether the optimizer step should be skipped when the loss is NaN **TYPE:** `bool` **DEFAULT:** `False`            |
| `nan_tolerance` | number of consecutive batches with NaN loss before stopping the training **TYPE:** `int` **DEFAULT:** `5`          |
| `gradient_acc`  | number of batches to accumulate the gradient of before performing the update step **TYPE:** `int` **DEFAULT:** `1` |
| `gradient_clip` | the gradient clip value **TYPE:** \`float                                                                          |
| `on_epoch_end`  | callback triggered at the end of an epoch **TYPE:** \`Callable\[\[dict[str, float]\], Any\]                        |

Source code in `holocron/trainer/core.py`

```python
def __init__(
    self,
    model: nn.Module,
    train_loader: DataLoader,
    val_loader: DataLoader,
    criterion: nn.Module,
    optimizer: torch.optim.Optimizer,
    gpu: int | None = None,
    output_file: str = "./checkpoint.pth",
    amp: bool = False,
    skip_nan_loss: bool = False,
    nan_tolerance: int = 5,
    gradient_acc: int = 1,
    gradient_clip: float | None = None,
    on_epoch_end: Callable[[dict[str, float]], Any] | None = None,
) -> None:
    self.model = model
    self.train_loader = train_loader
    self.val_loader = val_loader
    self.criterion = criterion
    self.optimizer = optimizer
    self.amp = amp
    self.scaler: GradScaler
    self.on_epoch_end = on_epoch_end
    self.skip_nan_loss = skip_nan_loss
    self.nan_tolerance = nan_tolerance
    self.gradient_acc = gradient_acc
    self.grad_clip = gradient_clip

    # Output file
    self.output_file = output_file

    # Initialize
    self.step = 0
    self.start_epoch = 0
    self.epoch = 0
    self._grad_count = 0
    self.min_loss = math.inf
    self.gpu = gpu
    self._params: tuple[ParamSeq, ParamSeq] = ([], [])
    self.lr_recorder: list[float] = []
    self.loss_recorder: list[float] = []
    self.set_device(gpu)
    self._reset_opt(self.optimizer.defaults["lr"])
```

#### evaluate

```python
evaluate() -> dict[str, float]
```

Evaluate the model on the validation set

| RETURNS            | DESCRIPTION                                                        |
| ------------------ | ------------------------------------------------------------------ |
| `dict[str, float]` | evaluation metrics (validation loss, top1 accuracy, top5 accuracy) |

Source code in `holocron/trainer/classification.py`

```python
@torch.inference_mode()
def evaluate(self) -> dict[str, float]:
    """Evaluate the model on the validation set

    Returns:
        evaluation metrics (validation loss, top1 accuracy, top5 accuracy)
    """
    self.model.eval()

    val_loss, top1, top5, num_samples, num_valid_batches = 0.0, 0, 0, 0, 0
    for x, target in self.val_loader:
        x, target = self.to_cuda(x, target)

        loss, out = self._get_loss(x, target, return_logits=True)  # ty: ignore[invalid-argument-type]

        # Safeguard for NaN loss
        if not torch.isnan(loss) and not torch.isinf(loss):
            val_loss += loss.item()
            num_valid_batches += 1

        pred = out.topk(5, dim=1)[1] if out.shape[1] >= 5 else out.argmax(dim=1, keepdim=True)
        correct = pred.eq(target.view(-1, 1).expand_as(pred))  # ty: ignore[possibly-missing-attribute]
        top1 += cast(int, correct[:, 0].sum().item())
        if out.shape[1] >= 5:
            top5 += cast(int, correct.any(dim=1).sum().item())

        num_samples += x.shape[0]

    val_loss /= num_valid_batches

    return {"val_loss": val_loss, "acc1": top1 / num_samples, "acc5": top5 / num_samples}
```

#### plot_top_losses

```python
plot_top_losses(mean: tuple[float, float, float], std: tuple[float, float, float], classes: Sequence[str] | None = None, num_samples: int = 12, **kwargs: Any) -> None
```

Plot the top losses

| PARAMETER     | DESCRIPTION                                                              |
| ------------- | ------------------------------------------------------------------------ |
| `mean`        | mean of the dataset **TYPE:** `tuple[float, float, float]`               |
| `std`         | standard deviation of the dataset **TYPE:** `tuple[float, float, float]` |
| `classes`     | list of classes **TYPE:** \`Sequence[str]                                |
| `num_samples` | number of samples to plot **TYPE:** `int` **DEFAULT:** `12`              |
| `**kwargs`    | keyword args of matplotlib.pyplot.show **TYPE:** `Any` **DEFAULT:** `{}` |

| RAISES           | DESCRIPTION                                                               |
| ---------------- | ------------------------------------------------------------------------- |
| `AssertionError` | if the argument 'classes' is not specified for multi-class classification |

Source code in `holocron/trainer/classification.py`

```python
@torch.inference_mode()
def plot_top_losses(
    self,
    mean: tuple[float, float, float],
    std: tuple[float, float, float],
    classes: Sequence[str] | None = None,
    num_samples: int = 12,
    **kwargs: Any,
) -> None:
    """Plot the top losses

    Args:
        mean: mean of the dataset
        std: standard deviation of the dataset
        classes: list of classes
        num_samples: number of samples to plot
        **kwargs: keyword args of [`matplotlib.pyplot.show`][matplotlib.pyplot.show]

    Raises:
        AssertionError: if the argument 'classes' is not specified for multi-class classification
    """
    # Record loss, prob, target, image
    losses = np.zeros(num_samples, dtype=np.float32)
    preds = np.zeros(num_samples, dtype=int)
    probs = np.zeros(num_samples, dtype=np.float32)
    targets = np.zeros(num_samples, dtype=np.float32 if self.is_binary else int)
    images = [None] * num_samples

    # Switch to unreduced loss
    reduction = self.criterion.reduction
    self.criterion.reduction = "none"  # type: ignore[assignment]
    self.model.eval()

    train_iter = iter(self.train_loader)

    for x, target in tqdm(train_iter):
        x, target = self.to_cuda(x, target)

        # Forward
        batch_loss, logits = self._get_loss(x, target, return_logits=True)  # ty: ignore[invalid-argument-type]

        # Binary
        if self.is_binary:
            batch_loss = batch_loss.squeeze(1)
            probs_ = torch.sigmoid(logits.squeeze(1))
        else:
            probs_ = torch.softmax(logits, 1).max(dim=1).values

        if torch.any(batch_loss > losses.min()):
            idcs = np.concatenate((losses, batch_loss.cpu().numpy())).argsort()[-num_samples:]
            kept_idcs = [idx for idx in idcs if idx < num_samples]
            added_idcs = [idx - num_samples for idx in idcs if idx >= num_samples]
            # Update
            losses = np.concatenate((losses[kept_idcs], batch_loss.cpu().numpy()[added_idcs]))
            probs = np.concatenate((probs[kept_idcs], probs_.cpu().numpy()))
            if not self.is_binary:
                preds = np.concatenate((preds[kept_idcs], logits[added_idcs].argmax(dim=1).cpu().numpy()))
            targets = np.concatenate((targets[kept_idcs], target[added_idcs].cpu().numpy()))  # ty: ignore[invalid-argument-type]
            imgs = x[added_idcs].cpu() * torch.tensor(std).view(-1, 1, 1)
            imgs += torch.tensor(mean).view(-1, 1, 1)
            images = [images[idx] for idx in kept_idcs] + [to_pil_image(img) for img in imgs]

    self.criterion.reduction = reduction

    if not self.is_binary and classes is None:
        raise AssertionError("arg 'classes' must be specified for multi-class classification")

    # Final sort
    idcs_ = losses.argsort()[::-1]
    losses, preds, probs, targets = losses[idcs_], preds[idcs_], probs[idcs_], targets[idcs_]
    images = [images[idx] for idx in idcs_]

    # Plot it
    num_cols = 4
    num_rows = math.ceil(num_samples / num_cols)
    _, axes = plt.subplots(num_rows, num_cols, figsize=(20, 5))
    for idx, (img, pred, prob, target, loss) in enumerate(zip(images, preds, probs, targets, losses, strict=True)):
        row = int(idx / num_cols)
        col = idx - num_cols * row
        axes[row][col].imshow(img)
        # Loss, prob, target
        if self.is_binary:
            axes[row][col].title.set_text(f"{loss:.3} / {prob:.2} / {target:.2}")
        # Loss, pred (prob), target
        else:
            axes[row][col].title.set_text(
                f"{loss:.3} / {classes[pred]} ({prob:.1%}) / {classes[target]}"  # type: ignore[index]
            )
        axes[row][col].axis("off")

    plt.show(**kwargs)
```

### BinaryClassificationTrainer

```python
BinaryClassificationTrainer(model: Module, train_loader: DataLoader, val_loader: DataLoader, criterion: Module, optimizer: Optimizer, gpu: int | None = None, output_file: str = './checkpoint.pth', amp: bool = False, skip_nan_loss: bool = False, nan_tolerance: int = 5, gradient_acc: int = 1, gradient_clip: float | None = None, on_epoch_end: Callable[[dict[str, float]], Any] | None = None)
```

Bases: `ClassificationTrainer`

Image binary classification trainer class.

| PARAMETER      | DESCRIPTION                                                                            |
| -------------- | -------------------------------------------------------------------------------------- |
| `model`        | model to train **TYPE:** `Module`                                                      |
| `train_loader` | training loader **TYPE:** `DataLoader`                                                 |
| `val_loader`   | validation loader **TYPE:** `DataLoader`                                               |
| `criterion`    | loss criterion **TYPE:** `Module`                                                      |
| `optimizer`    | parameter optimizer **TYPE:** `Optimizer`                                              |
| `gpu`          | index of the GPU to use **TYPE:** \`int                                                |
| `output_file`  | path where checkpoints will be saved **TYPE:** `str` **DEFAULT:** `'./checkpoint.pth'` |
| `amp`          | whether to use automatic mixed precision **TYPE:** `bool` **DEFAULT:** `False`         |

Source code in `holocron/trainer/core.py`

```python
def __init__(
    self,
    model: nn.Module,
    train_loader: DataLoader,
    val_loader: DataLoader,
    criterion: nn.Module,
    optimizer: torch.optim.Optimizer,
    gpu: int | None = None,
    output_file: str = "./checkpoint.pth",
    amp: bool = False,
    skip_nan_loss: bool = False,
    nan_tolerance: int = 5,
    gradient_acc: int = 1,
    gradient_clip: float | None = None,
    on_epoch_end: Callable[[dict[str, float]], Any] | None = None,
) -> None:
    self.model = model
    self.train_loader = train_loader
    self.val_loader = val_loader
    self.criterion = criterion
    self.optimizer = optimizer
    self.amp = amp
    self.scaler: GradScaler
    self.on_epoch_end = on_epoch_end
    self.skip_nan_loss = skip_nan_loss
    self.nan_tolerance = nan_tolerance
    self.gradient_acc = gradient_acc
    self.grad_clip = gradient_clip

    # Output file
    self.output_file = output_file

    # Initialize
    self.step = 0
    self.start_epoch = 0
    self.epoch = 0
    self._grad_count = 0
    self.min_loss = math.inf
    self.gpu = gpu
    self._params: tuple[ParamSeq, ParamSeq] = ([], [])
    self.lr_recorder: list[float] = []
    self.loss_recorder: list[float] = []
    self.set_device(gpu)
    self._reset_opt(self.optimizer.defaults["lr"])
```

#### evaluate

```python
evaluate() -> dict[str, float]
```

Evaluate the model on the validation set

| RETURNS            | DESCRIPTION                                    |
| ------------------ | ---------------------------------------------- |
| `dict[str, float]` | evaluation metrics (validation loss, accuracy) |

Source code in `holocron/trainer/classification.py`

```python
@torch.inference_mode()
def evaluate(self) -> dict[str, float]:
    """Evaluate the model on the validation set

    Returns:
        evaluation metrics (validation loss, accuracy)
    """
    self.model.eval()

    val_loss, top1, num_samples, num_valid_batches = 0.0, 0.0, 0, 0
    for x, target in self.val_loader:
        x, target = self.to_cuda(x, target)

        loss, out = self._get_loss(x, target, return_logits=True)  # ty: ignore[invalid-argument-type]

        # Safeguard for NaN loss
        if not torch.isnan(loss) and not torch.isinf(loss):
            val_loss += loss.item()
            num_valid_batches += 1

        top1 += torch.sum((target.view_as(out) >= 0.5) == (torch.sigmoid(out) >= 0.5)).item() / out[0].numel()  # ty: ignore[possibly-missing-attribute]

        num_samples += x.shape[0]

    val_loss /= num_valid_batches

    return {"val_loss": val_loss, "acc": top1 / num_samples}
```

## Semantic segmentation

### SegmentationTrainer

```python
SegmentationTrainer(*args: Any, num_classes: int = 10, **kwargs: Any)
```

Bases: `Trainer`

Semantic segmentation trainer class.

| PARAMETER     | DESCRIPTION                                                |
| ------------- | ---------------------------------------------------------- |
| `*args`       | args of Trainer **TYPE:** `Any` **DEFAULT:** `()`          |
| `num_classes` | number of output classes **TYPE:** `int` **DEFAULT:** `10` |
| `**kwargs`    | keyword args of Trainer **TYPE:** `Any` **DEFAULT:** `{}`  |

Source code in `holocron/trainer/segmentation.py`

```python
def __init__(self, *args: Any, num_classes: int = 10, **kwargs: Any) -> None:
    super().__init__(*args, **kwargs)
    self.num_classes = num_classes
```

#### evaluate

```python
evaluate(ignore_index: int = 255) -> dict[str, float]
```

Evaluate the model on the validation set

| PARAMETER      | DESCRIPTION                                                                   |
| -------------- | ----------------------------------------------------------------------------- |
| `ignore_index` | index of the class to ignore in evaluation **TYPE:** `int` **DEFAULT:** `255` |

| RETURNS            | DESCRIPTION                                                     |
| ------------------ | --------------------------------------------------------------- |
| `dict[str, float]` | evaluation metrics (validation loss, global accuracy, mean IoU) |

Source code in `holocron/trainer/segmentation.py`

```python
@torch.inference_mode()
def evaluate(self, ignore_index: int = 255) -> dict[str, float]:
    """Evaluate the model on the validation set

    Args:
        ignore_index: index of the class to ignore in evaluation

    Returns:
        evaluation metrics (validation loss, global accuracy, mean IoU)
    """
    self.model.eval()

    val_loss, mean_iou, num_valid_batches = 0.0, 0.0, 0
    conf_mat = torch.zeros(
        (self.num_classes, self.num_classes), dtype=torch.int64, device=next(self.model.parameters()).device
    )
    for x, target in self.val_loader:
        x, target = self.to_cuda(x, target)

        loss, out = self._get_loss(x, target, return_logits=True)  # ty: ignore[invalid-argument-type]

        # Safeguard for NaN loss
        if not torch.isnan(loss) and not torch.isinf(loss):
            val_loss += loss.item()
            num_valid_batches += 1

        # borrowed from https://github.com/pytorch/vision/blob/master/references/segmentation/train.py
        pred = out.argmax(dim=1).flatten()
        target = target.flatten()  # ty: ignore[possibly-missing-attribute]
        k = (target >= 0) & (target < self.num_classes)
        inds = self.num_classes * target[k].to(torch.int64) + pred[k]
        nc = self.num_classes
        conf_mat += torch.bincount(inds, minlength=nc**2).reshape(nc, nc)

    val_loss /= num_valid_batches
    acc_global = (torch.diag(conf_mat).sum() / conf_mat.sum()).item()
    mean_iou = (torch.diag(conf_mat) / (conf_mat.sum(1) + conf_mat.sum(0) - torch.diag(conf_mat))).mean().item()

    return {"val_loss": val_loss, "acc_global": acc_global, "mean_iou": mean_iou}
```

## Object detection

### DetectionTrainer

```python
DetectionTrainer(model: Module, train_loader: DataLoader, val_loader: DataLoader, criterion: Module, optimizer: Optimizer, gpu: int | None = None, output_file: str = './checkpoint.pth', amp: bool = False, skip_nan_loss: bool = False, nan_tolerance: int = 5, gradient_acc: int = 1, gradient_clip: float | None = None, on_epoch_end: Callable[[dict[str, float]], Any] | None = None)
```

Bases: `Trainer`

Object detection trainer class.

| PARAMETER       | DESCRIPTION                                                                                                        |
| --------------- | ------------------------------------------------------------------------------------------------------------------ |
| `model`         | model to train **TYPE:** `Module`                                                                                  |
| `train_loader`  | training loader **TYPE:** `DataLoader`                                                                             |
| `val_loader`    | validation loader **TYPE:** `DataLoader`                                                                           |
| `criterion`     | loss criterion **TYPE:** `Module`                                                                                  |
| `optimizer`     | parameter optimizer **TYPE:** `Optimizer`                                                                          |
| `gpu`           | index of the GPU to use **TYPE:** \`int                                                                            |
| `output_file`   | path where checkpoints will be saved **TYPE:** `str` **DEFAULT:** `'./checkpoint.pth'`                             |
| `amp`           | whether to use automatic mixed precision **TYPE:** `bool` **DEFAULT:** `False`                                     |
| `skip_nan_loss` | whether the optimizer step should be skipped when the loss is NaN **TYPE:** `bool` **DEFAULT:** `False`            |
| `nan_tolerance` | number of consecutive batches with NaN loss before stopping the training **TYPE:** `int` **DEFAULT:** `5`          |
| `gradient_acc`  | number of batches to accumulate the gradient of before performing the update step **TYPE:** `int` **DEFAULT:** `1` |
| `gradient_clip` | the gradient clip value **TYPE:** \`float                                                                          |
| `on_epoch_end`  | callback triggered at the end of an epoch **TYPE:** \`Callable\[\[dict[str, float]\], Any\]                        |

Source code in `holocron/trainer/core.py`

```python
def __init__(
    self,
    model: nn.Module,
    train_loader: DataLoader,
    val_loader: DataLoader,
    criterion: nn.Module,
    optimizer: torch.optim.Optimizer,
    gpu: int | None = None,
    output_file: str = "./checkpoint.pth",
    amp: bool = False,
    skip_nan_loss: bool = False,
    nan_tolerance: int = 5,
    gradient_acc: int = 1,
    gradient_clip: float | None = None,
    on_epoch_end: Callable[[dict[str, float]], Any] | None = None,
) -> None:
    self.model = model
    self.train_loader = train_loader
    self.val_loader = val_loader
    self.criterion = criterion
    self.optimizer = optimizer
    self.amp = amp
    self.scaler: GradScaler
    self.on_epoch_end = on_epoch_end
    self.skip_nan_loss = skip_nan_loss
    self.nan_tolerance = nan_tolerance
    self.gradient_acc = gradient_acc
    self.grad_clip = gradient_clip

    # Output file
    self.output_file = output_file

    # Initialize
    self.step = 0
    self.start_epoch = 0
    self.epoch = 0
    self._grad_count = 0
    self.min_loss = math.inf
    self.gpu = gpu
    self._params: tuple[ParamSeq, ParamSeq] = ([], [])
    self.lr_recorder: list[float] = []
    self.loss_recorder: list[float] = []
    self.set_device(gpu)
    self._reset_opt(self.optimizer.defaults["lr"])
```

#### evaluate

```python
evaluate(iou_threshold: float = 0.5) -> dict[str, float | None]
```

Evaluate the model on the validation set.

| PARAMETER       | DESCRIPTION                                                            |
| --------------- | ---------------------------------------------------------------------- |
| `iou_threshold` | IoU threshold for pair assignment **TYPE:** `float` **DEFAULT:** `0.5` |

| RETURNS            | DESCRIPTION |
| ------------------ | ----------- |
| \`dict\[str, float | None\]\`    |

Source code in `holocron/trainer/detection.py`

```python
@torch.inference_mode()
def evaluate(self, iou_threshold: float = 0.5) -> dict[str, float | None]:
    """Evaluate the model on the validation set.

    Args:
        iou_threshold: IoU threshold for pair assignment

    Returns:
        evaluation metrics (validation loss, localization error rate, classification error rate, detection error rate)
    """
    self.model.eval()

    loc_assigns = 0
    correct, clf_error, loc_fn, loc_fp, num_samples = 0, 0, 0, 0, 0

    for x, target in self.val_loader:
        x, target = self.to_cuda(x, target)

        if self.amp:
            with torch.amp.autocast("cuda"):
                detections = self.model(x)
        else:
            detections = self.model(x)

        for dets, t in zip(detections, target, strict=True):
            if t["boxes"].shape[0] > 0 and dets["boxes"].shape[0] > 0:
                gt_indices, pred_indices = assign_iou(t["boxes"], dets["boxes"], iou_threshold)
                loc_assigns += len(gt_indices)
                correct_ = (t["labels"][gt_indices] == dets["labels"][pred_indices]).sum().item()
            else:
                gt_indices, pred_indices = [], []
                correct_ = 0
            correct += correct_
            clf_error += len(gt_indices) - correct_
            loc_fn += t["boxes"].shape[0] - len(gt_indices)
            loc_fp += dets["boxes"].shape[0] - len(pred_indices)
        num_samples += sum(t["boxes"].shape[0] for t in target)

    nb_preds = num_samples - loc_fn + loc_fp
    # Localization
    loc_err = 1 - 2 * loc_assigns / (nb_preds + num_samples) if nb_preds + num_samples > 0 else None
    # Classification
    clf_err = 1 - correct / loc_assigns if loc_assigns > 0 else None
    # End-to-end
    det_err = 1 - 2 * correct / (nb_preds + num_samples) if nb_preds + num_samples > 0 else None
    return {"loc_err": loc_err, "clf_err": clf_err, "det_err": det_err, "val_loss": loc_err}
```

## Miscellaneous

### freeze_bn

```python
freeze_bn(mod: Module) -> None
```

Prevents parameter and stats from updating in Batchnorm layers that are frozen

Example

```python
from holocron.models import rexnet1_0x
from holocron.trainer.utils import freeze_bn
model = rexnet1_0x()
freeze_bn(model)
```

| PARAMETER | DESCRIPTION                       |
| --------- | --------------------------------- |
| `mod`     | model to train **TYPE:** `Module` |

Source code in `holocron/trainer/utils.py`

````python
def freeze_bn(mod: nn.Module) -> None:
    """Prevents parameter and stats from updating in Batchnorm layers that are frozen

    Example:
        ```python
        from holocron.models import rexnet1_0x
        from holocron.trainer.utils import freeze_bn
        model = rexnet1_0x()
        freeze_bn(model)
        ```

    Args:
        mod: model to train
    """
    # Loop on modules
    for m in mod.modules():
        if isinstance(m, _BatchNorm) and m.affine and all(not p.requires_grad for p in m.parameters()):
            # Switch back to commented code when https://github.com/pytorch/pytorch/issues/37823 is resolved
            m.track_running_stats = False  # ty: ignore[unresolved-attribute]
            m.eval()
````

### freeze_model

```python
freeze_model(model: Module, last_frozen_layer: str | None = None, frozen_bn_stat_update: bool = False) -> None
```

Freeze a specific range of model layers.

Example

```python
from holocron.models import rexnet1_0x
from holocron.trainer.utils import freeze_model
model = rexnet1_0x()
freeze_model(model)
```

| PARAMETER               | DESCRIPTION                                                                                |
| ----------------------- | ------------------------------------------------------------------------------------------ |
| `model`                 | model to train **TYPE:** `Module`                                                          |
| `last_frozen_layer`     | last layer to freeze. Assumes layers have been registered in forward order **TYPE:** \`str |
| `frozen_bn_stat_update` | force stats update in BN layers that are frozen **TYPE:** `bool` **DEFAULT:** `False`      |

| RAISES       | DESCRIPTION                           |
| ------------ | ------------------------------------- |
| `ValueError` | if the last frozen layer is not found |

Source code in `holocron/trainer/utils.py`

````python
def freeze_model(
    model: nn.Module,
    last_frozen_layer: str | None = None,
    frozen_bn_stat_update: bool = False,
) -> None:
    """Freeze a specific range of model layers.

    Example:
        ```python
        from holocron.models import rexnet1_0x
        from holocron.trainer.utils import freeze_model
        model = rexnet1_0x()
        freeze_model(model)
        ```

    Args:
        model: model to train
        last_frozen_layer: last layer to freeze. Assumes layers have been registered in forward order
        frozen_bn_stat_update: force stats update in BN layers that are frozen

    Raises:
        ValueError: if the last frozen layer is not found
    """
    # Unfreeze everything
    for p in model.parameters():
        p.requires_grad_(True)

    # Loop on parameters
    if isinstance(last_frozen_layer, str):
        layer_reached = False
        for n, p in model.named_parameters():
            if not layer_reached or n.startswith(last_frozen_layer):
                p.requires_grad_(False)
            if n.startswith(last_frozen_layer):
                layer_reached = True
            # Once the last param of the layer is frozen, we break
            elif layer_reached:
                break
        if not layer_reached:
            raise ValueError(f"Unable to locate child module {last_frozen_layer}")

    # Loop on modules
    if not frozen_bn_stat_update:
        freeze_bn(model)
````

# holocron.transforms

`holocron.transforms` provides PIL and PyTorch tensor transformations.

### ResizeMethod

Bases: `StrEnum`

Resize methods Available methods are `squish`, `pad`.

#### SQUISH

```python
SQUISH = 'squish'
```

#### PAD

```python
PAD = 'pad'
```

### Resize

```python
Resize(size: tuple[int, int], mode: ResizeMethod = SQUISH, pad_mode: str = 'constant', **kwargs: Any)
```

Bases: `Resize`

Implements a more flexible resizing scheme.

Example

```python
from holocron.transforms import Resize, ResizeMethod
pil_img = ...
tf = Resize((224, 224), mode=ResizeMethod.PAD)
resized_img = tf(pil_img)
```

| PARAMETER  | DESCRIPTION                                                                                                                                       |
| ---------- | ------------------------------------------------------------------------------------------------------------------------------------------------- |
| `size`     | the desired height and width of the image in pixels **TYPE:** `tuple[int, int]`                                                                   |
| `mode`     | the resizing scheme ("squish" is similar to PyTorch, "pad" will preserve the aspect ratio and pad) **TYPE:** `ResizeMethod` **DEFAULT:** `SQUISH` |
| `pad_mode` | padding mode when mode is "pad" **TYPE:** `str` **DEFAULT:** `'constant'`                                                                         |
| `kwargs`   | the keyword arguments of torchvision.transforms.v2.Resize **TYPE:** `Any` **DEFAULT:** `{}`                                                       |

Source code in `holocron/transforms/interpolation.py`

```python
def __init__(
    self,
    size: tuple[int, int],
    mode: ResizeMethod = ResizeMethod.SQUISH,
    pad_mode: str = "constant",
    **kwargs: Any,
) -> None:
    if not isinstance(mode, ResizeMethod):
        raise TypeError("mode is expected to be a ResizeMethod")
    if not isinstance(size, (tuple, list)) or len(size) != 2 or any(s <= 0 for s in size):
        raise ValueError("size is expected to be a sequence of 2 positive integers")
    super().__init__(size, **kwargs)
    self.mode: ResizeMethod = mode
    self.pad_mode: str = pad_mode
    self.size: tuple[int, int]
```

### RandomZoomOut

```python
RandomZoomOut(size: tuple[int, int], scale: tuple[float, float] = (0.5, 1.0), **kwargs: Any)
```

Bases: `Module`

Implements a size reduction of the orignal image to provide a zoom out effect.

Example

```python
from holocron.transforms import RandomZoomOut
pil_img = ...
tf = RandomZoomOut((224, 224), scale=(0.3, 1.))
resized_img = tf(pil_img)
```

| PARAMETER | DESCRIPTION                                                                                                                     |
| --------- | ------------------------------------------------------------------------------------------------------------------------------- |
| `size`    | the desired height and width of the image in pixels **TYPE:** `tuple[int, int]`                                                 |
| `scale`   | the range of relative area of the projected image to the desired size **TYPE:** `tuple[float, float]` **DEFAULT:** `(0.5, 1.0)` |
| `kwargs`  | the keyword arguments of torchvision.transforms.functional.resize **TYPE:** `Any` **DEFAULT:** `{}`                             |

Source code in `holocron/transforms/interpolation.py`

```python
def __init__(self, size: tuple[int, int], scale: tuple[float, float] = (0.5, 1.0), **kwargs: Any) -> None:
    if not isinstance(size, (tuple, list)) or len(size) != 2 or any(s <= 0 for s in size):
        raise ValueError("size is expected to be a sequence of 2 positive integers")
    if len(scale) != 2 or scale[0] > scale[1]:
        raise ValueError("scale is expected to be a couple of floats, the first one being small than the second")
    super().__init__()
    self.size: tuple[int, int] = size
    self.scale: tuple[float, float] = scale
    self._kwargs: dict[str, Any] = kwargs
```

# holocron.utils

`holocron.utils` provides some utilities for general usage.

## Miscellaneous

### parallel

```python
parallel(func: Callable[[Inp], Out], arr: Sequence[Inp], num_threads: int | None = None, progress: bool = False, **kwargs: Any) -> Iterable[Out]
```

Performs parallel tasks by leveraging multi-threading.

Example

```python
from holocron.utils.misc import parallel
parallel(lambda x: x ** 2, list(range(10)))
```

| PARAMETER     | DESCRIPTION                                                                        |
| ------------- | ---------------------------------------------------------------------------------- |
| `func`        | function to be executed on multiple workers **TYPE:** `Callable[[Inp], Out]`       |
| `arr`         | function argument's values **TYPE:** `Sequence[Inp]`                               |
| `num_threads` | number of workers to be used for multiprocessing **TYPE:** \`int                   |
| `progress`    | whether the progress bar should be displayed **TYPE:** `bool` **DEFAULT:** `False` |
| `kwargs`      | keyword arguments of tqdm.auto.tqdm **TYPE:** `Any` **DEFAULT:** `{}`              |

| RETURNS         | DESCRIPTION                |
| --------------- | -------------------------- |
| `Iterable[Out]` | list of function's results |

Source code in `holocron/utils/misc.py`

````python
def parallel(
    func: Callable[[Inp], Out],
    arr: Sequence[Inp],
    num_threads: int | None = None,
    progress: bool = False,
    **kwargs: Any,
) -> Iterable[Out]:
    """Performs parallel tasks by leveraging multi-threading.

    Example:
        ```python
        from holocron.utils.misc import parallel
        parallel(lambda x: x ** 2, list(range(10)))
        ```

    Args:
        func: function to be executed on multiple workers
        arr: function argument's values
        num_threads: number of workers to be used for multiprocessing
        progress: whether the progress bar should be displayed
        kwargs: keyword arguments of [`tqdm.auto.tqdm`][tqdm.auto.tqdm]

    Returns:
        list of function's results
    """
    num_threads = num_threads if isinstance(num_threads, int) else min(16, mp.cpu_count())
    if num_threads < 2:
        results = list(map(func, tqdm(arr, total=len(arr), **kwargs))) if progress else map(func, arr)
    else:
        with ThreadPool(num_threads) as tp:
            results = list(tqdm(tp.imap(func, arr), total=len(arr), **kwargs)) if progress else tp.map(func, arr)

    return results
````

### find_image_size

```python
find_image_size(dataset: Sequence[tuple[Image, Any]], **kwargs: Any) -> None
```

Computes the best image size target for a given set of images

| PARAMETER  | DESCRIPTION                                                                                        |
| ---------- | -------------------------------------------------------------------------------------------------- |
| `dataset`  | an iterator yielding a PIL.Image.Image and a target object **TYPE:** `Sequence[tuple[Image, Any]]` |
| `**kwargs` | keyword args of matplotlib.pyplot.show **TYPE:** `Any` **DEFAULT:** `{}`                           |

Source code in `holocron/utils/misc.py`

```python
def find_image_size(dataset: Sequence[tuple[Image.Image, Any]], **kwargs: Any) -> None:
    """Computes the best image size target for a given set of images

    Args:
        dataset: an iterator yielding a [`PIL.Image.Image`][PIL.Image.Image] and a target object
        **kwargs: keyword args of [`matplotlib.pyplot.show`][matplotlib.pyplot.show]
    """
    # Record height & width
    shapes_ = parallel(lambda x: x[0].size, dataset, progress=True)

    shapes = np.asarray(shapes_)[:, ::-1]
    ratios = shapes[:, 0] / shapes[:, 1]
    sides = np.sqrt(shapes[:, 0] * shapes[:, 1])

    # Compute median aspect ratio & side
    median_ratio = np.median(ratios)
    median_side = np.median(sides)

    height = round(median_side * sqrt(median_ratio))
    width = round(median_side / sqrt(median_ratio))

    # Double histogram
    fig, axes = plt.subplots(1, 2)
    axes[0].hist(ratios, bins=30, alpha=0.7)
    axes[0].title.set_text(f"Aspect ratio (median: {median_ratio:.2})")
    axes[0].grid(True, linestyle="--", axis="x")
    axes[0].axvline(median_ratio, color="r")
    axes[1].hist(sides, bins=30, alpha=0.7)
    axes[1].title.set_text(f"Side (median: {int(median_side)})")
    axes[1].grid(True, linestyle="--", axis="x")
    axes[1].axvline(median_side, color="r")
    fig.suptitle(f"Median image size: ({height}, {width})")
    plt.show(**kwargs)
```

# holocron.utils.data

## Batch collate

### Mixup

```python
Mixup(num_classes: int, alpha: float = 0.2)
```

Bases: `Module`

Implements a batch collate function with MixUp strategy from ["mixup: Beyond Empirical Risk Minimization"](https://arxiv.org/pdf/1710.09412.pdf).

Example

```python
import torch
from torch.utils.data._utils.collate import default_collate
from holocron.utils.data import Mixup
mix = Mixup(num_classes=10, alpha=0.4)
loader = torch.utils.data.DataLoader(dataset, batch_size, collate_fn=lambda b: mix(*default_collate(b)))
```

| PARAMETER     | DESCRIPTION                                       |
| ------------- | ------------------------------------------------- |
| `num_classes` | number of expected classes **TYPE:** `int`        |
| `alpha`       | mixup factor **TYPE:** `float` **DEFAULT:** `0.2` |

Source code in `holocron/utils/data/collate.py`

```python
def __init__(self, num_classes: int, alpha: float = 0.2) -> None:
    super().__init__()
    self.num_classes: int = num_classes
    if alpha < 0:
        raise ValueError("`alpha` only takes positive values")
    self.alpha: float = alpha
```
# Notes

# Changelog

## v0.2.1 (2022-07-16)

Release note: [v0.2.1](https://github.com/frgfm/Holocron/releases/tag/v0.2.1)

## v0.2.0 (2022-02-05)

Release note: [v0.2.0](https://github.com/frgfm/Holocron/releases/tag/v0.2.0)

## v0.1.3 (2020-10-27)

Release note: [v0.1.3](https://github.com/frgfm/Holocron/releases/tag/v0.1.3)

## v0.1.2 (2020-06-21)

Release note: [v0.1.2](https://github.com/frgfm/Holocron/releases/tag/v0.1.2)

## v0.1.1 (2020-05-12)

Release note: [v0.1.1](https://github.com/frgfm/Holocron/releases/tag/v0.1.1)

## v0.1.0 (2020-05-11)

Release note: [v0.1.0](https://github.com/frgfm/Holocron/releases/tag/v0.1.0)
